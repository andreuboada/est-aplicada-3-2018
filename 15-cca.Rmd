# Correlación Canónica (CCA)

<style>
  .espacio {
    margin-bottom: 1cm;
  }
</style>
  
  <style>
  .espacio3 {
    margin-bottom: 3cm;
  }
</style>

<p class="espacio">
</p>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

## CCA vs PCA

<p class="espacio3">
</p>

```{block2, type = "nota"}
* Anteriormente, estudiamos los métodos analíticos de factores como un enfoque para comprender las fuentes clave de variación _dentro_ de conjuntos de variables.

* Hay situaciones en las que tenemos varios conjuntos de variables, y buscamos una comprensión de las dimensiones clave que se correlacionan _entre_ conjuntos.

* El análisis de correlación canónica es uno de los métodos más antiguos y mejor conocidos para descubrir y explorar dimensiones que están correlacionadas _entre_ conjuntos, pero no están correlacionadas _dentro_ del conjunto.
```

<p class="espacio3">
</p>


El análisis de correlación canónica se concentra en la correlación entre una _combinación lineal_ de variables en un conjunto y la _combinación lineal_ de variables en otro conjunto. La idea es determinar primero el par de combinaciones lineales que tienen la combinación lineal más grande. Después, el par de combinaciones lineales que tienen correlación más grande entre todos los pares que no están correlacionados con el primero, y así sucesivamente.

A los pares de combinaciones lineales se les llama _variables canónicas_ y a sus correlaciones se les llama _correlaciones canónicas_.

Las correlaciones canónicas miden qué tan fuerte es la asociación entre dos conjuntos de variables. 

## Variables y correlaciones canónicas

Nos interesa medir la asociación entre _dos_ grupos de variables:

* El primer grupo (de $p$ variables) está representado por vector aleatorio $X^{(1)}$

* El segundo grupo (de $q$ variables) está representado por otro vector aleatorio $X^{(2)}$

Suponemos en lo que sigue que $p \leq q$.

Para los vectores aleatorios $X^{(1)}$ y $X^{(2)}$ sea

$$
E(X^{(1)}) = \mu^{(1)},\qquad \mbox{Var}(X^{(1)}) = \Sigma_{11}
$$

$$
E(X^{(2)}) = \mu^{(2)},\qquad \mbox{Var}(X^{(2)}) = \Sigma_{22}
$$

$$
\mbox{Cov}\left(X^{(1)},X^{(2)}\right) = \Sigma_{12} = \Sigma_{21}
$$

Ponemos

$$
X = \left(X^{(1)}, X^{(2)}\right)
$$

con vector de media

$$
\mu = (\mu^{(1)},\mu^{(2)})
$$

y matriz de covarianzas

$$
\Sigma = \left[
\begin{array}{c|c}
\Sigma_{11} & \Sigma_{12} \\
\hline
\Sigma_{21} & \Sigma_{22}
\end{array}
\right]
$$

<p class="espacio">
</p>

```{block2, type = "nota"}
Observaciones:

* Las correlaciones entre pares de variables, una de $X^{(1)}$ y la otra de $X^{(2)}$ está contenidas en la matriz $\Sigma_{12}$.

* $\Sigma_{12}$ es de dimensión $pq$. Si $p$ y $q$ son grandes, la interpretación de $\Sigma_{12}$ puede ser muy complicada. En este caso, es usual interpretar más fácilmente combinaciones lineales de las variables de cada grupo.

* La tarea principal del _análisis canónico_ es resumir la asociación entre $X^{(1)}$ y $X^{(2)}$.
```

<p class="espacio">
</p>

### Combinaciones lineaes de factores

Las combinaciones lineales proveen de interpretación a conjuntos de variables. Sea

$$
\begin{eqnarray*}
U &=& a^T X^{(1)} \\
V &=& b^T X^{(2)}
\end{eqnarray*}
$$

para un par de vectores $a$ y $b$. Por lo tanto,

$$
\mbox{Var}(U) = a^T \mbox{Var}(X^{(1)}) a = a^T\Sigma_{11}a
$$

$$
\mbox{Var}(V) = b^T \mbox{Var}(X^{(2)}) b = b^T\Sigma_{22}b
$$

$$
\mbox{Cov}(U,V) = a^T \mbox{Cov}\left(X^{(1)},X^{(2)}\right) b = a^T\Sigma_{12}b.
$$

Buscamos coeficientes de los vectores $a$ y $b$ tales que:

$$
\mbox{Corr}(U,V) = \dfrac{a^T\Sigma_{12}b}{\sqrt{a^T\Sigma_{11}a}\sqrt{b^T\Sigma_{22}b}}.
$$

Sea $k$ entre $1$ y $p\leq q$, entonces al $k$-ésimo par de combinaciones lineales $U_k$ y $V_kl$ con varianzas unitarias y que maximizan la correlación entre las opciones que no estén correlacionadas con las anteriores, se les llama _$k$-ésimo par de variables canónicas_.

<p class="espacio3">
</p>

```{block2, type = "nota"}
Suponemos que $\Sigma_{12}$ es de rango completo, entonces

$$
\max_{u,v}\mbox{Corr}(U,V) = \rho_1^*,
$$

se alcanza en las combinaciones lineales (primer par canónico):

$$
U_{1}=\underbrace{e_{1}^{T}\Sigma_{11}^{-1}}_{a_{1}^{T}}X^{(1)}, \qquad \mbox{ y } \qquad V_{1}=\underbrace{f_{1}^{T}\Sigma_{22}^{-1}}_{b_{1}^{T}}X^{(2)} 
$$

donde $\mbox{Var}\left(X_i^{(1)}\right) = \sigma_{ii}$ para $i=1,2,\ldots,p$.

El $k$-ésimo par de variables canónicas $k=2,3,\ldots,p$,

$$
U_k = e_k^T\Sigma_{11}^{-1/2}X^{(1)},\qquad V_k = f_k^T\Sigma_{22}^{-1/2}X^{(2)}
$$

maximiza 

$$
\mbox{Corr}(U_k,V_k)=\rho_k^*
$$

entre las combinaciones lineales que no están correlacionadas con las variables canónicas precedentes de $1,2,\ldots,k-1$.
```

<p class="espacio3">
</p>

Aquí ${\rho_1^*}^2 \geq {\rho_2^*}^2 \geq \cdots \geq {\rho_p^*}^2$ son los eigenvalores de 

$$
\Sigma_{11}^{-1/2} \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \Sigma_{11}^{-1/2},
$$

y $e_1,e_2,\ldots,e_p$ son los eigenvectores asociados.

De la misma forma ${\rho_1^*}^2,{\rho_2^*}^2,\ldots,{\rho_p^*}^2$ son los eigenvalores asociados a los $p$ eigenvectores de dimensión $q$, $f_1,f_2,\ldots,f_p$, de la matriz 

$$
\Sigma_{22}^{-1/2} \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12} \Sigma_{22}^{-1/2}.
$$

<p class="espacio">
</p>

```{block2, type = "information"}
**Propiedades:** Para $k,\,l = 1,2,\ldots,p$:
  
<p class="espacio">
</p>
  
  
* $\mbox{Var}(U_k) = \mbox{Var}(V_k)$ = 1
  
* $\mbox{Cov}(U_k,U_l) = \mbox{Corr}(U_k,U_l) = 0 \quad k\neq l$
  
* $\mbox{Cov}(V_k,V_l) = \mbox{Corr}(V_k,V_l) = 0 \quad k\neq l$
  
* $\mbox{Cov}(U_k,V_l) = \mbox{Corr}(U_k,V_l) = 0 \quad k\neq l$
```

<br>

Si las variables originales están estandarizadas $Z^{(1)}=\left(Z^{(1)}_1,\ldots,Z^{(1)}_p\right)$ entonces

$$
U_k = a_k^T Z^{(1)} = e_k^T \rho_{11}^{-1} Z^{(1)}
$$
$$
V_k = b_k^T Z^{(2)} = f_k^T \rho_{22}^{-1} Z^{(2)}
$$

Aquí sucede que $\mbox{Cov}(Z^{(1)}) = \rho_{11}$, es decir, la covarianza de $Z^{(1)}$ la representamos como matriz de correlaciones utilizando la letra $\rho$, y de forma similar, $\mbox{Cov}(Z^{(2)}) = \rho_{22}$.

Además se puede ver que:

$$
\mbox{Corr}(U_k,V_k) = \rho_k^*, \quad k=1,2,\ldots,p
$$

donde ${\rho_1^*}^2\geq{\rho_2^*}^2\geq\ldots\geq{\rho_p^*}^2$ son los eigenvalores de la matriz

$$
\rho_{11}^{-1/2}\rho_{12}\rho_{22}^{-1}\rho_{21}\rho_{11}^{-1/2}
$$

<br>

---

Notemos que:

$$
\begin{eqnarray*}
a_k^T(X^{(1)}-\mu^{(1)}) &=& a_{k1}(X_1^{(1)}-\mu_1^{(1)}) + \cdots+a_{kp}(X_p^{(1)}-\mu_p^{(1)})\\
&=& a_{k1}\sqrt{\sigma_{11}}\dfrac{(X_1^{(1)}-\mu_1^{(1)})}{\sqrt{\sigma_{11}}} + \cdots+a_{kp}\sqrt{\sigma_{pp}}\dfrac{(X_p^{(1)}-\mu_p^{(1)})}{\sqrt{\sigma_{pp}}}.
\end{eqnarray*}
$$

Por lo tanto, los coeficientes canónicos de las variables estandarizadas

$$
Z_i ^{(1)} = \dfrac{X_i^{(1)}- \mu_i^{(1)}}{\sqrt{\sigma_{ii}}}
$$

están relacionados simplemente con los coeficientes canónicos de la variable original $X_i^{(1)}$. Más específicamente, si $a_k$ es el vector de coeficientes de la $k$-ésima variable canónica $U_k$, entonces

$$
a_k^TV_{11}^{1/2}
$$

es el vector de coeficientes de la $k$-ésima variable canónica de las variables estandarizadas $Z^{(1)}$. Aquí $V_{11}^{1/2}$ es la matriz diagonal con $i$-ésimo elemento $\sqrt{\sigma_{ii}}$.

<p class="espacio3">
</p>
</div>
<br>

![](figuras/manicule2.jpg) 
<div class="centered">
<p class="espacio">
</p>

Al estandarizar las variables $X^{(1)}$ y $X^{(2)}$ y realizar análisis de correlación canónica sobre $Z^{(1)}$ y $Z^{(2)}$, las correlaciones canónicas

(a) Se multiplican por un factor de $\sqrt{\sigma_{ii}}$.

(b) No cambian.

(c) Deben sumar $1$.

(d) Ninguna de las anteriores.

<p class="espacio3">
</p>
</div>
<br>

---





### Ejemplo simple

$$
X=\begin{pmatrix}
1 & 1 & 3 \\
2 & 3 & 2 \\
1 & 1 & 1 \\
1 & 1 & 2 \\
2 & 2 & 3 \\
3 & 3 & 2 \\
1 & 3 & 2 \\
4 & 3 & 5 \\
5 & 5 & 5 
\end{pmatrix}, \qquad Y= \left(\begin{array}{rrr}
4 & 4 & −1.07846 \\
3 & 3 & 1.214359 \\
2 & 2 & 0.307180 \\
2 & 3 & −0.385641 \\
2 & 1 & −0.078461 \\
1 & 1 & 1.61436 \\
1 & 2 & 0.814359 \\
2 & 1 & -0.0641016 \\
1 & 2 & 1.5359 \\
\end{array}\right)
$$

En este ejemplo, construimos la tercera columna de $Y$ de las columnas de $X$ con los pesos lineales $a$ = $(0.4, 0.6, -\sqrt{0.48})$.

Algunas preguntas:

* ¿Cuál debería ser el primer vector de pesos canónicos para las variables $Y$?

* ¿Cuál debería ser la primera correlación canónica?

Respuestas:

* El propósito del análisis de correlación canónica es (a) buscar y (b) caracterizar la redundancia lineal entre ambos conjuntos de variables.

* En nuestro ejemplo simple, una de las variables en $Y$ se puede reproducir exactamente como una combinación lineal de las tres variables en X.

* El análisis de correlación canónica (si funciona correctamente) simplemente seleccionará
$y_3$ como la primera variable canónica en el conjunto $Y$, con pesos canónicos
$b = (0,0,1)$, y recupera la combinación lineal de las variables del primer
grupo utilizada para generar $y_3$, generando $a = (0.4, 0.6, -\sqrt{0.48})$ como los pesos canónicos para el conjunto $X$.

* La primera correlación canónica será, por supuesto, 1.

```{r}
X <- matrix(c( 1,1,3,2,3,2,1,1,1,
    1,1,2,2,2,3,3,3,2,1,3,2,
    4,3,5,5,5,5),9,3,byrow=T)

Y <- matrix(c( 4,4,-1.07846,
               3,3,1.214359,
               2,2,0.307180,
               2,3,-0.385641,
               2,1,-0.078461,
               1,1,1.61436,
               1,2,0.814359,
               2,1,-0.0641016,
       1,2,1.535900),9,3,byrow=T)
```

Para calcular los pesos completamente, necesitamos las matrices de varianzas y covarianzas para $X$ e $Y$, así como las matrices de covarianza cruzada.

```{r}
S_xy <- cov(X, Y)
S_xx <- var(X)
S_yx <- cov(Y, X)
S_yy <- var(Y)
S_xy
```

Ahora que tenemos estas matrices, es fácil calcular los pesos canónicos y las correlaciones canónicas:

```{r}
A <- eigen(solve(S_xx) %*% S_xy %*% solve(S_yy) %*% S_yx)$vectors
B <- eigen(solve(S_yy) %*% S_yx %*% solve(S_xx) %*% S_xy)$vectors
R <- sqrt(eigen(solve(S_yy) %*% S_yx %*% solve(S_xx) %*% S_xy)$values)
```

```{r}
A
```

```{r}
B
```

```{r}
R
```


## Ejemplo: test psicológico

Supongamos que se desea investigar asociaciones entre mediciones psicológicas y 
medidas de desempeño académico y contamos con datos con 600 observaciones de ocho 
variables. Las variables psicológicas son `locus_of_control`, `self_concept` y `motivation`. 
Las variables académicas son pruebas estandarizadas en lectura (`read`), redacción (`write`),
matemáticas (`math`) y ciencia (`science`). Además, la variable `female` es una variable 
indicadora de que una estudiante es mujer.

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(GGally)
library(CCA)

psychademic %>% head %>% knitr::kable()
```

Las variables psicológicas son:

```{r}
(psych_variables <- attr(psychademic, "psychology"))
```

Las variables académicas son:

```{r}
(academic_variables <- attr(psychademic, "academic"))
```

Veamos las correlaciones entre las variables psicológicas:

```{r message=FALSE, warning=FALSE, out.width='90%', out.height='400cm'}
ggpairs(psychademic, psych_variables, title = "Within Psychological Variables")
```

Y entre las académicas:

```{r message=FALSE, warning=FALSE, out.width='90%', out.height='400cm'}
ggpairs(psychademic, academic_variables, title = "Within Academic Variables")
```

Veamos las correlaciones entre pares de ambos grupos de variables:

```{r message=FALSE, warning=FALSE, out.width='100%', out.height='480cm'}
ggduo(
  psychademic, psych_variables, academic_variables,
  types = list(continuous = "smooth_lm"),
  title = "Between Academic and Psychological Variable Correlation",
  xlab = "Psychological",
  ylab = "Academic"
)
```

También podemos analizar las correlaciones numéricamente:

```{r}
datos <- psychademic %>%
  mutate_at(.funs = as.numeric, .vars = vars(-sex)) %>%
  mutate(sex = ifelse(sex == 'female', 1, 0))
matcor(datos[,psych_variables], datos[,academic_variables])
```

LLevamos a cabo CCA:

```{r}
cc1 <- cc(datos[,psych_variables], datos[,academic_variables])

# correlaciones canónicas
cc1$cor
```

Coeficientes canónicos:

```{r}
cc1[3:4]
```

<p class="espacio">
</p>

```{block2, type = "information"}
**Nota:** 
  
<p class="espacio">
</p>
  
* Las correlaciones anteriores son entre las variables observadas y las variables canónicas, a estas se les conoce como _cargas canónicas_. 

* Las variables canónicas son en realidad un tipo de variable latente.

* En general, el número de dimensiones canónicas es igual al número de variables en el conjunto más pequeño; sin embargo, la cantidad de dimensiones significativas puede ser aún menor. Las dimensiones canónicas, también conocidas como variables canónicas, son variables latentes que son análogas a los factores obtenidos en el análisis de factores. 
```

<br>

En este ejemplo particular, hay tres dimensiones canónicas, de las cuales solo las dos primeras son estadísticamente significativas.

```{r}
ev <- (1 - cc1$cor^2)

n <- dim(datos)[1]
p <- length(datos[,psych_variables])
q <- length(datos[,academic_variables])
k <- min(p, q)
m <- n - 3/2 - (p + q)/2

w <- rev(cumprod(rev(ev)))

d1 <- d2 <- f <- vector("numeric", k)

for (i in 1:k) {
    s <- sqrt((p^2 * q^2 - 4)/(p^2 + q^2 - 5))
    si <- 1/s
    d1[i] <- p * q
    d2[i] <- m * s - p * q/2 + 1
    r <- (1 - w[i]^si)/w[i]^si
    f[i] <- r * d2[i]/d1[i]
    p <- p - 1
    q <- q - 1
}

pv <- pf(f, d1, d2, lower.tail = FALSE)
(dmat <- cbind(WilksL = w, F = f, df1 = d1, df2 = d2, p = pv))
```

Cuando las variables en el modelo tienen desviaciones estándar muy diferentes, los coeficientes estandarizados permiten una comparación más fácil entre las variables. 

Coeficientes canónicos estandarizados para variables psicológicas:

```{r}
s1 <- diag(sqrt(diag(cov(datos[,psych_variables]))))
s1 %*% cc1$xcoef
```

Coeficientes canónicos estandarizados para variables académicas:

```{r}
s2 <- diag(sqrt(diag(cov(datos[,academic_variables]))))
s2 %*% cc1$ycoef
```

Los coeficientes canónicos estandarizados se interpretan de manera análoga a la interpretación de los coeficientes de regresión estandarizados. Por ejemplo, para la variable de lectura, un aumento de la desviación estándar en lectura se refleja en una disminución de 0.45 desviaciones estándar de en la primera variable canónica del conjunto 2 cuando las otras variables en el modelo se mantienen constantes.

## Ejemplo: fenómenos meteorológicos

Ejemplo de (Marc MeNugget)[https://menugget.blogspot.mx/2012/03/canonical-correlation-analysis-for.html].

Veamos ahora el uso del Análisis de Correlación Canónica (CCA) para encontrar patrones entre pares de observaciones en campos climáticos. El método produce resultados similares a los de componentes principales, pero los patrones reflejan la _correlación_ máxima en vez de la _covarianza_ máxima. Además, el resultado del modelo es una combinación de modelos lineales que se pueden usar para un modelo de predicción de un campo climático.

Descargamos los campos de anomalías mensuales de presión superficial del mar (SLP) y temperatura de la superficie del mar (TSM) para la región del Pacífico ecuatorial de la [NOAA](https://www.esrl.noaa.gov/).

```{r message=FALSE, warning=FALSE, results='hide'}
library(curl)

url_pres <- "ftp://ftp.cdc.noaa.gov/Datasets.other/hadslp/slp.mnmean.nc"
if(!file.exists("datos/slp.mnmean.nc")){
  curl_fetch_disk(url_pres, path = "datos/slp.mnmean.nc", 
                  handle = new_handle(CONNECTTIMEOUT = 120))
}

url_temp <- "http://www.esrl.noaa.gov/psd/thredds/fileServer/Datasets/kaplan_sst/sst.mon.anom.nc"
if(!file.exists("datos/sst.mon.anom.nc")){
  curl_fetch_disk(url = url_temp, path = "datos/sst.mon.anom.nc", handle = new_handle(CONNECTTIMEOUT = 120))
}
```

Leemos los datos:

```{r}
library(maps)
library(mapproj)
library(ncdf4)
library(CCP)
library(irlba)
nc <- nc_open(filename = "datos/slp.mnmean.nc")
slp.lon <- ncvar_get(nc, "lon")
slp.lat <- ncvar_get(nc, "lat")
slp.t <- ncvar_get(nc, "time")
slp.raw <- ncvar_get(nc, "slp")
nc_close(nc)
slp.t <- as.Date(slp.t, origin="1800-01-01")
temp <- which(slp.lon > 180)
slp.lon[temp] <- slp.lon[temp] - 360
slp.grd <- expand.grid(slp.lon, slp.lat)
colnames(slp.grd) <- c("lon", "lat")
slp <- matrix(c(slp.raw), nrow=length(slp.t), ncol<-length(slp.lon)*length(slp.lat), byrow=TRUE)
row.names(slp) <- as.character(slp.t)
```

Calculamos anomalías en la presión superficial del mar:

```{r}
anomaly <- function(y, x, level="daily"){ 
 y <- as.matrix(y)
 
 if(level=="monthly"){levs=unique(x$mon)}
 if(level=="daily"){levs=unique(x$yday)}
 
 levs_lookup=vector("list", length(levs))
 names(levs_lookup)<-levs
 for(i in 1:length(levs)){
  if(level=="monthly"){levs_lookup[[i]]<-which(x$mon == names(levs_lookup[i]))}
  if(level=="daily"){levs_lookup[[i]]<-which(x$yday == names(levs_lookup[i]))}
 }
 
 for(j in 1:length(levs)){
  y[levs_lookup[[j]],] <- t(t(as.matrix(y[levs_lookup[[j]],])) - apply(as.matrix(y[levs_lookup[[j]],]), 2, mean, na.rm=TRUE))
 }
 
 y
}
```

```{r}
slp <- anomaly(slp, as.POSIXlt(slp.t), level="monthly")
```

Leemos los datos de temperaturas:

```{r}
nc <- nc_open(filename = "datos/sst.mon.anom.nc")
sst.lon <- ncvar_get(nc, "lon")
sst.lat <- ncvar_get(nc, "lat")
sst.t <- ncvar_get(nc, "time")
sst.raw <- ncvar_get(nc, "sst")
nc_close(nc)
sst.t <- as.Date(sst.t, origin="1800-01-01")
temp <- which(sst.lon > 180)
sst.lon[temp] <- sst.lon[temp] - 360
sst.grd <- expand.grid(sst.lon, sst.lat)
colnames(sst.grd) <- c("lon", "lat")
sst <- matrix(c(sst.raw), nrow=length(sst.t), ncol<-length(sst.lon)*length(sst.lat), byrow=TRUE)
row.names(sst) <- as.character(sst.t)
```

Tomamos un subconjunto de los datos de acuerdo a fechas:

```{r}
t.min <- as.Date("1980-01-01")
t.max1 <- as.Date("1998-12-01")
t.max2 <- as.Date("2018-04-01")
slp.t.incl <- which(slp.t >= t.min & slp.t <= t.max2)
sst.t.incl <- which(sst.t >= t.min & sst.t <= t.max2)
```

Nos concentramos primero en la región del pacífico:

```{r}
lon.lim <- c(-180, -70)
lat.lim <- c(-30, 30)

slp.grd.incl <- which(slp.grd[,1] < -70 & slp.grd[,1] > -180 & slp.grd[,2] < 30 & slp.grd[,2] > -30)
sst.grd.incl <- which(sst.grd[,1] < -70 & sst.grd[,1] > -180 & sst.grd[,2] < 30 & sst.grd[,2] > -30)
```

Centramos cada una de las matrices:

```{r}
X <- slp[slp.t.incl, slp.grd.incl]
Y <- sst[sst.t.incl, sst.grd.incl]
Y[is.na(Y)] <- 0
X_c <- scale(X, center=TRUE, scale=FALSE)
Y_c <- scale(Y, center=TRUE, scale=FALSE)
```

```{r, echo = FALSE}
val2col<-function(z, zlim, col = heat.colors(12), breaks){
 if(!missing(breaks)){
  if(length(breaks) != (length(col)+1)){stop("must have one more break than colour")}
 }
 if(missing(breaks) & !missing(zlim)){
  zlim[2] <- zlim[2]+c(zlim[2]-zlim[1])*(1E-3)#adds a bit to the range in both directions
  zlim[1] <- zlim[1]-c(zlim[2]-zlim[1])*(1E-3)
  breaks <- seq(zlim[1], zlim[2], length.out=(length(col)+1)) 
 }
 if(missing(breaks) & missing(zlim)){
  zlim <- range(z, na.rm=TRUE)
  zlim[2] <- zlim[2]+c(zlim[2]-zlim[1])*(1E-3)#adds a bit to the range in both directions
  zlim[1] <- zlim[1]-c(zlim[2]-zlim[1])*(1E-3)
  breaks <- seq(zlim[1], zlim[2], length.out=(length(col)+1))
 }
 CUT <- cut(z, breaks=breaks)
 colorlevels <- col[match(CUT, levels(CUT))] # assign colors to heights for each point
 return(colorlevels)
}

color.palette <- function(steps, n.steps.between=NULL, ...){
 
 if(is.null(n.steps.between)) n.steps.between <- rep(0, (length(steps)-1))
 if(length(n.steps.between) != length(steps)-1) 
   stop("Must have one less n.steps.between value than steps")
 
 fill.steps <- cumsum(rep(1, length(steps))+c(0,n.steps.between))
 RGB <- matrix(NA, nrow=3, ncol=fill.steps[length(fill.steps)])
 RGB[,fill.steps] <- col2rgb(steps)
 
 for(i in which(n.steps.between>0)){
  col.start=RGB[,fill.steps[i]]
  col.end=RGB[,fill.steps[i+1]]
  for(j in seq(3)){
   vals <- seq(col.start[j], col.end[j], length.out=n.steps.between[i]+2)[2:(2+n.steps.between[i]-1)]  
   RGB[j,(fill.steps[i]+1):(fill.steps[i+1]-1)] <- vals
  }
 }
 
 new.steps <- rgb(RGB[1,], RGB[2,], RGB[3,], maxColorValue = 255)
 pal <- colorRampPalette(new.steps, ...)
 return(pal)
}
```

Analicemos en las medias de ambas variables a través del tiempo:

```{r}
X_m <- matrix(apply(X,1,mean),nrow=nrow(X_c), ncol=1)
rownames(X_m) <- rownames(X)
Y_m <- matrix(apply(Y,1,mean),nrow=nrow(Y_c), ncol=1)
rownames(Y_m) <- rownames(Y)
zran <- range(X_m, Y_m)
zlim <- c(-max(abs(zran)), max(abs(zran)))
pal <- color.palette(c("red", "yellow", "white", "cyan", "blue"), c(10,1,1,10))
colorvalues1 <- val2col(X_m, zlim, col=pal(ncol))
colorvalues2 <- val2col(Y_m, zlim, col=pal(ncol))
```

Creamos polígonos para graficar:

```{r}
#slp
spacing <- 5
slp.poly <- vector(mode="list", dim(slp.grd)[1])
for(i in seq(slp.poly)){
 x=c(slp.grd[i,1]-spacing/2, slp.grd[i,1]+spacing/2, slp.grd[i,1]+spacing/2, slp.grd[i,1]-spacing/2)
 y=c(slp.grd[i,2]-spacing/2, slp.grd[i,2]-spacing/2, slp.grd[i,2]+spacing/2, slp.grd[i,2]+spacing/2)
 slp.poly[[i]] <- data.frame(x=x, y=y)
}
 
#sst
spacing <- 5
sst.poly <- vector(mode="list", dim(sst.grd)[1])
for(i in seq(sst.poly)){
 x=c(sst.grd[i,1]-spacing/2, sst.grd[i,1]+spacing/2, sst.grd[i,1]+spacing/2, sst.grd[i,1]-spacing/2)
 y=c(sst.grd[i,2]-spacing/2, sst.grd[i,2]-spacing/2, sst.grd[i,2]+spacing/2, sst.grd[i,2]+spacing/2)
 sst.poly[[i]] <- data.frame(x=x, y=y)
}
```

Gráfica de anomalías de presión a nivel del mar:

```{r message=FALSE, warning=FALSE}
#mapproj settings
project="fisheye"
orientation=c(mean(lat.lim), mean(lon.lim), 0)
PAR=1
par(mai=c(0.1, 0.1, 0.1, 0.1))
map("world",project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim)
for(i in seq(slp.grd.incl)){
  polygon(mapproject(x=slp.poly[[slp.grd.incl[i]]][,1], y=slp.poly[[slp.grd.incl[i]]][,2]), col=colorvalues1[i], border=colorvalues1[i], lwd=0.3)
}
map("world",project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col="black")
map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col="grey", lwd=1)
box()
```

Mapa de anomalías 

```{r message=FALSE, warning=FALSE}
par(mai=c(0.1, 0.1, 0.1, 0.1))
map("world",project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim, xaxs="i", yaxs="i")
for(i in seq(sst.grd.incl)){
  polygon(mapproject(x=sst.poly[[sst.grd.incl[i]]][,1], y=sst.poly[[sst.grd.incl[i]]][,2]), col=colorvalues2[i], border=colorvalues2[i], lwd=0.3)
}
map("world",project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col="black")
map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col="grey", lwd=1)
box()
```

Ahora hacemos el análisis de correlación canónica. Primero calculamos la covarianza entre $X$ y $Y$:

```{r}
cov4gappy <- function(F1, F2=NULL){
    if(is.null(F2)){
        F1 <- as.matrix(F1)
        F1_val<-replace(F1, which(!is.na(F1)), 1)
        F1_val<-replace(F1_val, which(is.na(F1_val)), 0) 
        n_pairs=(t(F1_val)%*%F1_val)
 
        F1<-replace(F1, which(is.na(F1)), 0)
        cov_mat <- (t(F1)%*%F1)/n_pairs
        cov_mat <- replace(cov_mat, which(is.na(cov_mat)), 0)
    }
 
    if(!is.null(F2)){
        if(dim(F1)[1] == dim(F2)[1]){
            F1 <- as.matrix(F1)
            F2 <- as.matrix(F2)
 
            F1_val<-replace(F1, which(!is.na(F1)), 1)
            F1_val<-replace(F1_val, which(is.na(F1_val)), 0) 
            F2_val<-replace(F2, which(!is.na(F2)), 1)
            F2_val<-replace(F2_val, which(is.na(F2_val)), 0) 
            n_pairs=(t(F1_val)%*%F2_val)
 
            F1<-replace(F1, which(is.na(F1)), 0)
            F2<-replace(F2, which(is.na(F2)), 0)
            cov_mat <- (t(F1)%*%F2)/n_pairs
            cov_mat <- replace(cov_mat, which(is.na(cov_mat)), 0)
 
        } else {
            print("ERROR; matrices columns not of the same lengths")
        }
    }
 
    cov_mat
}

exp.mat<-function(MAT, EXP, tol=NULL){
 MAT <- as.matrix(MAT)
 matdim <- dim(MAT)
 if(is.null(tol)){
  tol=min(1e-7, .Machine$double.eps*max(matdim)*max(MAT))
 }
 if(matdim[1]>=matdim[2]){ 
  svd1 <- svd(MAT)
  keep <- which(svd1$d > tol)
  res <- t(svd1$u[,keep]%*%diag(svd1$d[keep]^EXP, nrow=length(keep))%*%t(svd1$v[,keep]))
 }
 if(matdim[1]<matdim[2]){ 
  svd1 <- svd(t(MAT))
  keep <- which(svd1$d > tol)
  res <- svd1$u[,keep]%*%diag(svd1$d[keep]^EXP, nrow=length(keep))%*%t(svd1$v[,keep])
 }
 return(res)
}
```


```{r}
F1 <- as.matrix(X_c)
F2 <- as.matrix(Y_c)

F1 <- as.matrix(F1)
F1_val<-replace(F1, which(!is.na(F1)), 1)
F1_val<-replace(F1_val, which(is.na(F1_val)), 0) 
n_pairs=(t(F1_val)%*%F1_val)
 
F1<-replace(F1, which(is.na(F1)), 0)
cov_mat1 <- (t(F1)%*%F1)/n_pairs
cov_mat1 <- replace(cov_mat1, which(is.na(cov_mat1)), 0)

F2 <- as.matrix(F2)
F2_val<-replace(F2, which(!is.na(F2)), 1)
F2_val<-replace(F2_val, which(is.na(F2_val)), 0) 
n_pairs=(t(F2_val)%*%F2_val)
 
F2<-replace(F2, which(is.na(F2)), 0)
cov_mat2 <- (t(F2)%*%F2)/n_pairs
cov_mat2 <- replace(cov_mat2, which(is.na(cov_mat2)), 0)

L1 <- irlba(cov_mat1, nu=5, nv=5)
L2 <- irlba(cov_mat2, nu=5, nv=5)

A1_coeff <- replace(F1, which(is.na(F1)), 0)%*%L1$u[,1:5]
A1_norm <- F1_val%*%(L1$u[,1:5]^2)
A1=A1_coeff/A1_norm

A2_coeff <- replace(F2, which(is.na(F2)), 0)%*%L2$u[,1:5]
A2_norm <- F2_val%*%(L2$u[,1:5]^2)
A2=A2_coeff/A2_norm

x <- A1[,1:5] %*% exp.mat(diag(L1$d[1:5]), -0.5)
y <- A2[,1:5] %*% exp.mat(diag(L2$d[1:5]), -0.5)
```

Hacemos descomposición en valores singulares:

```{r}

xdim <- dim(t(x))
ydim <- dim(t(y))
 
mindim=min(xdim[2], ydim[2])
xcols <- 1:xdim[2]
ycols <- (xdim[2]+1):(xdim[2]+ydim[2])
 
Sxx <- cov4gappy(t(x))
Sxy <- cov4gappy(t(x),t(y))
Syy <- cov4gappy(t(y))

svd1 <- svd(exp.mat(Syy, -0.5) %*% t(Sxy) %*% exp.mat(Sxx, -0.5))
A <- exp.mat(Sxx, -0.5) %*% svd1$v
B <- exp.mat(Syy, -0.5) %*% svd1$u
```

Veamos las variables canónicas:

```{r}
V <- t(x) %*% A
W <- t(y) %*% B

rho=NA*1:mindim
for(i in 1:mindim){
  rho[i] <- cor(as.vector(W[,i]), as.vector(V[,i]), use="pairwise")
}
 
# A test of CCA significance
# cca.sig  <- p.asym(rho, xdim[1], xdim[2], ydim[2], tstat = "Pillai")
```

```{r, echo = FALSE}
bp.cca <- function(eofx,eofy, 
n_pcx_incl=eofx$n_sig, n_pcy_incl=eofy$n_sig, 
rowx_incl=1:length(eofx$A[,1]), rowy_incl=1:length(eofy$A[,1]),
y_lag=0){
 
 require(CCP)
 #since these are PCs no centering is needed
 x <- eofx$A[rowx_incl,1:n_pcx_incl] %*% exp.mat(diag(eofx$Lambda[1:n_pcx_incl]), -0.5)
 y <- eofy$A[rowy_incl,1:n_pcy_incl] %*% exp.mat(diag(eofy$Lambda[1:n_pcy_incl]), -0.5)
 
 if(sign(y_lag) != 0){
  if(sign(y_lag)==1){
   x <- x[1:(length(x[,1])-y_lag),]
   y <- y[(1+y_lag):length(y[,1]),]
  }
  if(sign(y_lag)==-1){
   x <- x[(1-y_lag):length(x[,1]),]
   y <- y[1:(length(y[,1])+y_lag),]
  }
 }
 
 xdim <- dim(x)
 ydim <- dim(y)
 
 mindim=min(xdim[2], ydim[2])
 
 if(xdim[1]!=ydim[1])
  stop("unequal number of rows in x and y")
 
 
 xcols <- 1:xdim[2]
 ycols <- (xdim[2]+1):(xdim[2]+ydim[2])
 
 Sxx <- cov4gappy(x)
 Sxy <- cov4gappy(x,F2=y)
 Syy <- cov4gappy(y)
 
 ############################################
 #Canonical vectors ("patterns" or loadings)#
 ############################################ 
 if(xdim[2] >= ydim[2]){
  svd1 <- svd(exp.mat(Sxx, -0.5) %*% Sxy %*% exp.mat(Syy, -0.5))
  A <- exp.mat(Sxx, -0.5) %*% svd1$u
  B <- exp.mat(Syy, -0.5) %*% svd1$v
 } else {
  svd1 <- svd(exp.mat(Syy, -0.5) %*% t(Sxy) %*% exp.mat(Sxx, -0.5))
  A <- exp.mat(Sxx, -0.5) %*% svd1$v
  B <- exp.mat(Syy, -0.5) %*% svd1$u
 }
 
 ####################
 #Canonical variates#
 ####################
 V <- x %*% A
 W <- y %*% B
 
 ########################
 #Canonical correlations#
 ########################
 
 rho=NA*1:mindim
 for(i in 1:mindim){
  rho[i] <- cor(as.vector(W[,i]), as.vector(V[,i]), use="pairwise")
 }
 
 #A test of CCA significance
 cca.sig  <- p.asym(rho, xdim[1], xdim[2], ydim[2], tstat = "Pillai")
 
 #out
 list(
  rho=rho, stat=cca.sig$stat, df1=cca.sig$df1, df2=cca.sig$df2,
  approx=cca.sig$approx,p.value=cca.sig$p.value,
  mindim=mindim, n_pcx_incl=n_pcx_incl, n_pcy_incl=n_pcy_incl,
  rowx_incl=rowx_incl, rowy_incl=rowy_incl,
  A=A, B=B,
  V=V, W=W,
  x_ts=rownames(x), y_ts=rownames(y), y_lag=y_lag,
  x_dim=eofx$F1_dim, y_dim=eofy$F1_dim,
  x_cols_incl=eofx$F1_cols_incl,  y_cols_incl=eofy$F1_cols_incl,
  x_center=eofx$F1_center, x_scale=eofx$F1_scale, 
  y_center=eofy$F1_center, y_scale=eofy$F1_scale
 )
 
}

eof.mca <- function(F1, F2=NULL,
centered=TRUE, scaled=FALSE,
F1_cols_incl=1:length(F1[1,]), 
F2_cols_incl=1:length(F2[1,]),
nu=NULL, nv=NULL, method="svd", F2_lag=NULL
){
 
    if(method=="irlba"){
        print("Only squared variance, not variance, is calculated using irlba method")
    }
 
    ####################
    ###eof vectors######
    ####################
    if(is.null(F2)){ # EOF
        F1 <- scale(F1, center=centered, scale=scaled)
 
        F1_center=attr(F1,"scaled:center")
        F1_scale=attr(F1,"scaled:scale")
        F2_center=NULL
        F2_scale=NULL
 
        F1_ts <- rownames(F1)
        F2_ts <- NULL
 
        F1_dim <- dim(F1)
        F2_dim <- NULL
 
        C <- cov4gappy(F1[,F1_cols_incl])
        F2_cols_incl=NULL
 
        if(method=="svd") {   
            if(is.null(nu)){
                nu=F1_dim[2]
            }
            if(is.null(nv)){
                nv=F1_dim[2]
            }
            L <- svd(C)
        }
 
        if(method=="irlba") {
            if(is.null(nu)){
                nu=5
            }
            if(is.null(nv)){
                nv=5
            }
            L <- irlba(C, nu=nu, nv=nv)
        }
 
        L$v<-NULL
    }
 
    if(!is.null(F2)){ # MCA
        F1 <- scale(F1, center=centered, scale=scaled)
        F2 <- scale(F2, center=centered, scale=scaled)
 
        F1_center=attr(F1,"scaled:center")
        F1_scale=attr(F1,"scaled:scale")
        F2_center=attr(F2,"scaled:center")
        F2_scale=attr(F2,"scaled:scale")
 
        if(!is.null(F2_lag)){
            if(sign(F2_lag)==1){
                F1 <- F1[(1+F2_lag):length(F1[,1]),]
                F2 <- F2[1:(length(F2[,1])-F2_lag),]
            }
            if(sign(F2_lag)==-1){
                F1 <- F1[1:(length(F1[,1])-F2_lag),]
                F2 <- F2[(1+F2_lag):length(F2[,1]),]
            }
        }
 
        F1_ts <- rownames(F1)
        F2_ts <- rownames(F2)
 
        F1_dim <- dim(F1)
        F2_dim <- dim(F2)
 
        C <- cov4gappy(F1[,F1_cols_incl], F2=F2[,F2_cols_incl])
 
        if(method=="svd") {
            if(is.null(nu)){
                nu=min(F1_dim[2], F2_dim[2])
            }
            if(is.null(nv)){
                nv=min(F1_dim[2], F2_dim[2])
            }
            L <- svd(C)
        }
 
        if(method=="irlba") {
            if(is.null(nu)){
                nu=5
            }
            if(is.null(nv)){
                nv=5
            }           
            L <- irlba(C, nu=nu, nv=nv)
        }
 
    }
 
    ###################
    ###eof mca stats###
    ###################
    if(method=="svd"){
        expl_var=L$d/sum(L$d) #explained variance
        sq_cov_frac=L$d^2/sum(L$d^2) #squared covariance fraction
    }
    if(method=="irlba"){
        expl_var=NULL
        if(dim(C)[1] <= dim(C)[2]){
            sq_cov_frac=L$d^2/sum(diag(C%*%t(C)))
        } else {
            sq_cov_frac=L$d^2/sum(diag(t(C)%*%C))
        }
    }
 
    if(length(L$d)>1){
        Lambda_err <- sqrt(2/min(F1_dim[2], F2_dim[2]))*L$d
        upper.lim <- L$d+Lambda_err
        lower.lim <- L$d-Lambda_err
        NORTHok=0*L$d
        for(i in seq(L$d)){
            Lambdas <- L$d
            Lambdas[i] <- NaN
            nearest <- which.min(abs(L$d[i]-Lambdas))
            if(nearest > i){
                if(lower.lim[i] > upper.lim[nearest]) NORTHok[i] <- 1
            }
            if(nearest < i){
                if(upper.lim[i] < lower.lim[nearest]) NORTHok[i] <- 1
            }
        }
        n_sig <- min(which(NORTHok==0))-1
    }
 
    ##########################################################
    ###expansion of eof coefficients "principle components"###
    ##########################################################
 
    A_coeff = NULL
    A_norm = NULL
    A = NULL
    B_coeff = NULL
    B_norm = NULL
    B = NULL
 
    #trim columns of original data
    F1 <- as.matrix(F1[,F1_cols_incl])
 
    #setup for norm
    F1_val<-replace(F1, which(!is.na(F1)), 1)
    F1_val<-replace(F1_val, which(is.na(F1_val)), 0)
 
    #calc of expansion coefficient and scaling norm
    A_coeff <- replace(F1, which(is.na(F1)), 0)%*%L$u[,1:nu]
    A_norm <- F1_val%*%(L$u[,1:nu]^2)
    A=A_coeff/A_norm
 
    if(!is.null(F2)){
        #trim columns of original data then center then scale
        F2 <- F2[,F2_cols_incl]       
 
        #setup for norm
        F2_val<-replace(F2, which(!is.na(F2)), 1)
        F2_val<-replace(F2_val, which(is.na(F2_val)), 0)
 
        #calc of expansion coefficient and scaling norm
        B_coeff <- replace(F2, which(is.na(F2)), 0)%*%L$v[,1:nv]
        B_norm <- F2_val%*%(L$v[,1:nv]^2)
        B=B_coeff/B_norm
    }
 
    ############
    ###result###
    ############
    RESULT <- list(
        u=L$u[,1:nu], v=L$v[,1:nv], 
        Lambda=L$d, Lambda_err=Lambda_err,
        expl_var=expl_var, sq_cov_frac=sq_cov_frac, 
        NORTHok=NORTHok, n_sig=n_sig, nu=nu, nv=nv,
        F1_dim=F1_dim, F2_dim=F2_dim,
        F1_cols_incl=F1_cols_incl,  F2_cols_incl=F2_cols_incl,
        F1_center=F1_center, F1_scale=F1_scale, 
        F2_center=F2_center, F2_scale=F2_scale,
        A=A, B=B,
        F1_ts=F1_ts, F2_ts=F2_ts, F2_lag=F2_lag
    )
 
    RESULT
}
```

Repetimos los mapas de las variables canónicas (primer par):

```{r message=FALSE, warning=FALSE}
eof.slp <- eof.mca(slp[slp.t.incl, slp.grd.incl], centered=TRUE, nu=20)
eof.sst <- eof.mca(sst[sst.t.incl, sst.grd.incl], centered=TRUE, nu=20)
dim(eof.slp$A)
dim(eof.sst$A)

#CCA model will be built on a smaller time subset of the PCs (eof$A)
eof.slp.match <- which(as.Date(rownames(eof.slp$A)) >= t.min & as.Date(rownames(eof.slp$A)) <= t.max1)
eof.sst.match <- which(as.Date(rownames(eof.sst$A)) >= t.min & as.Date(rownames(eof.sst$A)) <= t.max1)
 
#The BPCCA model based on SLP and SST EOFS
bpcca <- bp.cca(eof.slp, eof.sst,
n_pcx_incl=eof.slp$n_sig, n_pcy_incl=eof.sst$n_sig, 
rowx_incl=eof.slp.match, rowy_incl=eof.sst.match)
 
 
###Map of CCA
slp.cca.modes <- eof.slp$u[,1:bpcca$n_pcx_incl] %*% bpcca$A
sst.cca.modes <- eof.sst$u[,1:bpcca$n_pcy_incl] %*% bpcca$B

MODE=1
zran <- range(slp.cca.modes[,MODE], sst.cca.modes[,MODE])
zlim <- c(-max(abs(zran)), max(abs(zran)))
 
heights=c(3,2)
widths=c(4,4,0.5)
pal=color.palette(c("red", "yellow", "white", "cyan", "blue"), c(10,1,1,10))
ncol=100
res=200
colorvalues1 <- val2col(slp.cca.modes[,MODE], zlim, col=pal(ncol)) #color levels for the polygons
colorvalues2 <- val2col(sst.cca.modes[,MODE], zlim, col=pal(ncol)) #color levels for the polygons
 
#mapproj settings
project="fisheye"
orientation=c(mean(lat.lim), mean(lon.lim), 0)
PAR=1
 
par(omi=c(0.5, 0.5, 0.5, 0.5), ps=12)
par(mai=c(0.1, 0.1, 0.1, 0.1))
map("world",project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim)
for(i in seq(slp.grd.incl)){
 polygon(mapproject(x=slp.poly[[slp.grd.incl[i]]][,1], y=slp.poly[[slp.grd.incl[i]]][,2]), col=colorvalues1[i], border=colorvalues1[i], lwd=0.3)
}
map("world",project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col="black")
map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col="grey", lwd=1)

par(mai=c(0.1, 0.1, 0.1, 0.1))
map("world",project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim, xaxs="i", yaxs="i")
for(i in seq(sst.grd.incl)){
 polygon(mapproject(x=sst.poly[[sst.grd.incl[i]]][,1], y=sst.poly[[sst.grd.incl[i]]][,2]), col=colorvalues2[i], border=colorvalues2[i], lwd=0.3)
}
map("world",project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col="black")
map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col="grey", lwd=1)
```
















## Tarea

Como parte de un estudio más amplio de los efectos de la estructura organizacional en la "satisfacción laboral", Dunham\ investigó en qué medida las medidas de satisfacción laboral están relacionadas con las características del trabajo. Utilizando un instrumento de encuesta, Dunham obtuvo medidas de $p = 5$ características del trabajo y $q = 7$ variables de satisfacción laboral para $n = 784$ ejecutivos de la rama corporativa de una gran corporación comercial minorista. ¿Las medidas de satisfacción laboral están asociadas con las características del trabajo? La respuesta puede tener implicaciones para el diseño del trabajo.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
worksat <- read_csv("datos/worksat.csv")
attr(worksat, "spec") <- NULL
worksat %>% str() 
```

1. Calcula la matriz de correlaciones muestrales basada en las 784 observaciones.

2. Encuentra los eigenvalores de $\Sigma_{11}^{-1/2} \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \Sigma_{11}^{-1/2}$ y verifica que estos eigenvalores son los mismos eigenvalores de $\Sigma_{22}^{-1/2} \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12} \Sigma_{22}^{-1/2}$.

3. Calcula todas las correlaciones canónicas y las variables canónicas. 

4. Reporta los coeficientes de las variables canónicas e interpreta el primer par de variables canónicas. Primero analiza qué variables contribuyen más a cada variable canónica. Para proporcionar interpretaciones de $U_1$ y $V_1$, calcula las correlaciones muestrales entre $U_1$ y las variables que la componen y entre $V_1$ y sus variables respectivas. Además, haz una tabla que muestre las correlaciones entre las variables de un conjunto y la primera variable canónica del otro conjunto.

5. Sean $Z^{(l)}$ y $Z^{(2)}$ las variables estandarizadas correspondientes a $X^{(1)}$ y $X^{(2)}$,
respectivamente. ¿Qué proporción de la varianza muestral total de $Z^{(1)}$ se explica por la variable canónica $U_1$? ¿Qué proporción de la varianza muestral total de $Z^{(2)}$ se explica por la variable canónica $V_1$? Discute tus respuestas.

#### Referencias {-}+

* Dunham, R. B. (1977). Reactions to job characteristics: Moderating effects of the organization. Academy of Management Journal, 20(1), 42-65.
