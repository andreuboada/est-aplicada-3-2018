[
["index.html", "Estadística Aplicada III Temario y referencias", " Estadística Aplicada III Andreu Boada de Atela 2018-05-15 Temario y referencias Todas las notas y material del curso estarán en este repositorio. Análisis exploratorio de datos Temas selectos de R Análisis de datos categóricos Regresión logística Regularización y selección de modelos Introducción a modelos lineales generalizados Análisis de discriminante lineal Componentes principales y análisis de factores Análisis de la correlación canónica Introducción a análisis de conglomerados Evaluación Tareas semanales (20%) Examen parcial (30% práctico, 20% teórico) Un examen final (30% práctico) Software: R y Rstudio R Sitio de R (CRAN) Rstudio Interfaz gráfica para trabajar en R. Recursos para aprender R Referencias principales Gelman, A., &amp; Hill, J. (2006). Data Analysis using Regression and Multilevel/hierarchical Models. Cambridge University Press. Christopher, M. B. (2016). Pattern Recognition and Machine Learning. Springer. Agresti, Alan. (2013). Categorical Data Analysis. 3rd ed. Wiley. Johnson, R., y Wichern, D. (2007). Applied Multivariate Statistical Analysis. 6th ed. Pearson Prentice Hall. Otras referencias Friedman, J., Hastie, T., &amp; Tibshirani, R. (2001). The Elements of Statistical Learning. Springer. Anderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis. 3rd ed. Wiley. Field, A., Miles, J., &amp; Field, Z. (2012). Discovering Statistics Using R. Sage publications. Everitt, B., &amp; Hothorn, T. (2011). An Introduction to Applied Multivariate Analysis with R. Springer Tareas Enviar tareas por correo electrónico a: andreuboadadeatela@gmail.com con el asunto “EAPLICADA3-Tarea-[XX]-[clave única 1]-[clave única 2]” donde [XX] es el número de la tarea, [clave única 1] y [clave única 2] son tu clave y la de tu compañero con quien vas a trabajar durante el semestre. "],
["intro.html", "Clase 1 Introducción 1.1 ¿Por qué un análisis multivariado? 1.2 La paradoja de Simpson 1.3 Modelos log-lineales 1.4 Interpretación de parámetros 1.5 Otros ejemplos 1.6 Tarea", " Clase 1 Introducción .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } La investigación científica es un proceso de aprendizaje iterativo. Para explicar un fenómeno físico o social primero se deben especificar los objetivos de una investigación y luego probar los objetivos a través de la recopilación y el análisis de datos pertinentes. A su vez, el análisis de los datos recopilados (experimentalmente o mediante observación) generalmente sugerirá una explicación modificada del fenómeno. A lo largo de este proceso iterativo, generalmente se añaden o se excluyen variables del análisis. Por lo tanto, la complejidad de la mayoría de los fenómenos requieren que el investigador recolecte muchas variables de las observaciones. En este curso veremos una introducción a los modelos estadísticos que son multivariados, es decir, modelos en los cuales los datos corresponden a mediciones en muchas variables. 1.1 ¿Por qué un análisis multivariado? Las razones por las cuales se utilizan modelos multivariados son generalmente (Johnson, Wichern, and others 2014): Factores de “confusión”. Un factor de confusión (o confound variable) es una variable que puede correlacionarse con otra variable de interés. Las correlaciones espurias son un posible tipo de confusión, donde el factor de confusión hace que una variable sin importancia real parezca ser importante. Pero las confusiones pueden ocultar variables reales importantes tan fácilmente como pueden producir falsas. Un ejemplo de esto, conocido como la “paradoja” de Simpson, consiste de datos en los cuales la dirección de una aparente asociación entre un predictor y la variable respuesta se puede revertir al considerar un factor de confusión. 1.2 La paradoja de Simpson La paradoja de Simpson, también conocida como el efecto Yule-Simpson, ocurre cuando existe una asociación entre dos variables en varios grupos pero la dirección de esta asociación se invierte cuando los datos se combinan para formar un solo grupo. En un análisis de los scores de SAT (examen de posicionamiento para la universidad) en Estados Unidos en 1997 se encontró que había una relación negativa entre el salario promedio anual de los maestros y el score total promedio de los alumnos que presentaron el SAT: library(tidyverse) sat &lt;- read_csv(&quot;datos/sat.csv&quot;) ggplot(sat, aes(x = teacher_salary, y = total_score)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + xlab(&quot;Salario anual promedio (en miles) de maestros&quot;) + ylab(&quot;Score total promedio de SAT&quot;) Podemos revisar el resultado de la regresión lineal (haciendo uso del paquete stargazer): library(stargazer) out1 &lt;- lm(formula = total_score ~ teacher_salary, data = sat) stargazer(out1, type = &#39;html&#39;, style = &quot;all&quot;, single.row = T, title = &quot;Regresión lineal del promedio de sueldo de maestros vs SAT promedio&quot;) Regresión lineal del promedio de sueldo de maestros vs SAT promedio Dependent variable: total_score teacher_salary -5.540*** (1.630) t = -3.390 p = 0.002 Constant 1,159.000*** (57.700) t = 20.100 p = 0.000 Observations 50 R2 0.193 Adjusted R2 0.177 Residual Std. Error 67.900 (df = 48) F Statistic 11.500*** (df = 1; 48) (p = 0.002) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Podemos observar que el coeficiente de la variable salario es \\(-5.54\\) y es significartivo según la prueba de hipótesis correspondiente. Desafortunadamente, la asociación entre salario y score de SAT parece ser negativa: a medida que aumenta el salario, se predice que el score SAT promedio disminuye. Afortunadamente para los maestros, una vez que se cuenta la variable de la fracción de alumnos que presentan el SAT, vemos una relación positiva estadísticamente significativa: out2 &lt;- lm(formula = total_score ~ teacher_salary + perc_take_sat, data = sat) stargazer(out2, type = &#39;html&#39;, style = &quot;all&quot;, single.row = T, title = &quot;Incluyendo el factor de confusión&quot;) Incluyendo el factor de confusión Dependent variable: total_score teacher_salary 2.180** (1.030) t = 2.120 p = 0.040 perc_take_sat -2.780*** (0.228) t = -12.200 p = 0.000 Constant 988.000*** (31.900) t = 31.000 p = 0.000 Observations 50 R2 0.806 Adjusted R2 0.797 Residual Std. Error 33.700 (df = 47) F Statistic 97.400*** (df = 2; 47) (p = 0.000) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Podemos ver que el coeficiente de la regresión lineal correspondiente al salario de los maestros se invierte. Gráficamente podemos visualizar este efecto: Vemos que dentro de cada grupo, la pendiente es positiva (o al menos no negativa). También podríamos agregar etiquetas: Otro ejemplo de este fenómeno es cuando la Universidad de California, Berkeley fue demandada por discrimanción hacia mujeres que habían solicitado admisión a un posgrado en 1973. De acuerdo con las estadísticas de admisión, los hombres que presentaron la solicitud tenían mayor probabilidad de ser admitidos que las mujeres, y la diferencia era tan sustancial que uno podría concluir que efectivamente había existido dicha discriminación. Sin embargo, al examinar los datos individualmente por departamento, parecía que no había una diferencia significativa en contra de las mujeres. ucb &lt;- UCBAdmissions %&gt;% as.tibble() ucb %&gt;% head(10) %&gt;% knitr::kable() Admit Gender Dept n Admitted Male A 512 Rejected Male A 313 Admitted Female A 89 Rejected Female A 19 Admitted Male B 353 Rejected Male B 207 Admitted Female B 17 Rejected Female B 8 Admitted Male C 120 Rejected Male C 205 Los datos contienen el número de solicitudes y admisiones por género a seis escuelas de postgrado diferentes. Analicemos una tabla de contingencia entre la variable género y la variable admitido: tab &lt;- ucb %&gt;% group_by(Gender, Admit) %&gt;% summarise(p = sum(n)) %&gt;% spread(Admit, p) tab %&gt;% knitr::kable() Gender Admitted Rejected Female 557 1278 Male 1198 1493 Los solicitantes masculinos tenían una tasa de aceptación del 44.52%, en comparación con solo el 30.35% de las mujeres, condicionando en la variable de género: tab %&gt;% gather(Admit, n, -Gender) %&gt;% mutate(prop = round(prop.table(n) * 100,2)) %&gt;% select(-n) %&gt;% spread(Admit, prop) %&gt;% knitr::kable() Gender Admitted Rejected Female 30.4 69.7 Male 44.5 55.5 Incluso podemos proporcionar una prueba estadística para apoyar la afirmación de que hubo sesgo en las admisiones. En R, se puede realizar una prueba de proporciones a través de la función prop.test(): prop.test(tab %&gt;% ungroup() %&gt;% select(-Gender) %&gt;% as.matrix()) #&gt; #&gt; 2-sample test for equality of proportions with continuity #&gt; correction #&gt; #&gt; data: tab %&gt;% ungroup() %&gt;% select(-Gender) %&gt;% as.matrix() #&gt; X-squared = 90, df = 1, p-value &lt;2e-16 #&gt; alternative hypothesis: two.sided #&gt; 95 percent confidence interval: #&gt; -0.170 -0.113 #&gt; sample estimates: #&gt; prop 1 prop 2 #&gt; 0.304 0.445 A partir de la prueba de hipótesis realizada anteriormente, se puede concluir que hay una diferencias significativa entre la proporción de hombres admitidos y la proporción de mujeres admitidas en los programas de posgrado. Sin embargo, si hacemos el mismo análisis por departamento, veremos que las diferencias ya no son tan significativas: ucb %&gt;% spread(Admit, n) %&gt;% mutate(total = Admitted + Rejected, porc = round(Admitted/total*100,2)) %&gt;% select(Gender,Dept,porc) %&gt;% spread(Gender, porc) %&gt;% knitr::kable() Dept Female Male A 82.41 62.1 B 68.00 63.0 C 34.06 36.9 D 34.93 33.1 E 23.92 27.8 F 7.04 5.9 Condicionando por departamento ahora vemos que las mujeres realmente tienen mayores tasas de admisión en cuatro de los seis departamentos (A, B, D, F). ¿Cómo puede ser esto? En realidad las diferencias tienen que ver con el porcentaje de solicitantes (hombres y mujeres) que son admitidos por departamentos, es decir, hay departamentos más competitivos que otros. Consideremos un modelo log-lineal. Sea \\(p_{ijk}\\) la proporción de la población en la celda \\((i,j,k)\\). Por ejemplo, \\(p_{112}\\) es la proporción de solicitantes que son admitidos, son hombres y se postulan para el Departamento B. tab &lt;- xtabs(n ~ ., ucb) llout &lt;- loglin(tab, list(1:2,c(1,3),2:3),param=TRUE) #&gt; 9 iterations: deviation 0.0492 Los efectos globales de la variable de admitidos son: llout$param$Admit %&gt;% knitr::kable() x Admitted -0.321 Rejected 0.321 Los efectos globales de la variable Departamento, por ejemplo, nos dicen qué departamentos tienden a tener más solicitantes (el A, el C y el D), aunque en realidad esto no sea tan relevante: llout$param$Dept %&gt;% knitr::kable() x A 0.154 B -0.765 C 0.540 D 0.430 E -0.029 F -0.330 Los efectos globales de la variable género también nos dicen que el número de solicitantes hombres es mayor que el número de solicitantes que son mujeres: llout$param$Gender %&gt;% knitr::kable() x Female -0.329 Male 0.329 Analicemos ahora los términos de interacciones entre variables. Como la variable que nos interesa es la de admisión, veamos la interacción de Admit-Gender y Admit-Department: llout$param$Admit.Gender %&gt;% knitr::kable() Female Male Admitted 0.025 -0.025 Rejected -0.025 0.025 llout$param$Admit.Dept %&gt;% knitr::kable() A B C D E F Admitted 0.637 0.615 0.006 -0.01 -0.232 -1.02 Rejected -0.637 -0.615 -0.006 0.01 0.232 1.02 En términos de asociación con la variable de admisión, la relación con la variable de departamento es mucho más fuerte que con la de género, lo que significa que la mayoría de los parámetros estimados son mucho más grandes en el primer caso. En otras palabras, el departamento es la variable más importante, no el género. Más aún, los resultados anteriores también muestran que existe una interacción Admit-Female positiva, es decir, que a las mujeres les va un poco mejor que a los hombres en cuanto a la admisión. 1.3 Modelos log-lineales Denotemos los 3 factores por \\(X^{(s)}\\), \\(s=1,2,3\\). En este caso particular, \\(X^{(1)}\\) es la variable de admisión Admit, y toma valores de \\(1\\) y \\(2\\), admitido y no admitido respectivamente. \\(X^{(2)}\\) la variable género tomaría valores \\(1\\) y \\(2\\) para hombre y mujer respectivamente, mientras que \\(X^{(3)}\\), la variable de departamento, toma valores del \\(1\\) al \\(6\\) para representar los departamentos A al F. En esta notación no estamos usando variables indicadoras. Estamos trabajando con variables estrictamente categóricas, cuyos valores son meramente etiquetas. Sea \\(X_r^{(s)}\\) el valor de \\(X^{(s)}\\) para el \\(i\\)-ésimo solicitante en la muestra, para \\(r=1,2,\\ldots,n\\). Aquí el número de observaciones es el número de solicitantes $n=$4526. Nuestros datos son los conteos en todas las categorías: \\[ N_{ijk} = \\mbox{nú}\\;\\mbox{mero de solicitantes }r\\mbox{ tales que }X_r^{(1)}=i, X_r^{(2)}=j,\\mbox{ y } X_r^{(3)}=k. \\] A esto le llamamos una tabla de contigencias en tres variables. Cada valor \\(N_{ijk}\\) es una celda de la tabla. Sea \\(p_{ijk}\\) la proporción poblacional de un solicitante elegido al azar en la celda \\((i,j,k)\\), es decir, \\[ p_{ijk} = P(X^{(1)}=i \\mbox{ y } X^{(2)}=j \\mbox{ y } X^{(3)}=k) = E(N_{ijk})/n. \\] Como se mencionó, nos interesan las relaciones entre las factores, en forma de independencia, tanto plena como parcial. De hecho, es común que un analista ajuste sucesivamente modelos más refinados a los datos, asumiendo cada uno una estructura de dependencia más compleja que la anterior. Esto se desarrollará en detalle a continuación. Considere primero el modelo que asume la independencia total: \\[ \\begin{eqnarray*} p_{ijk} &amp; = &amp; P(X^{(1)}=i \\mbox{ y } X^{(2)}=j \\mbox{ y } X^{(3)}=k) \\\\ &amp;=&amp; P(X^{(1)}=i) \\cdot P(X^{(2)}=j) \\cdot P(X^{(3)}=k). \\end{eqnarray*} \\] Tomando logaritmo de ambos lados, vemos que la independencia de los tres factores es equivalente a escribir una ecuación de la forma \\[ \\mbox{log}(p_{ijk}) = a_i + b_j + c_k, \\] donde \\(a_i,b_j,c_k\\) son cantidades estimadas. Por ejemplo, \\[ b_j = \\mbox{log}\\left(P(X^{(2)}=j)\\right). \\] El punto es que el modelo es similar a un modelo de regresión lineal sin interacciones. El análogo de que no haya interacción entre las variables aquí está representado por el supuesto de independencia. Por ejemplo, si suponemos que Departamento es independiente de Admisión y Género, pero que Admisión y Género no son independientes entre sí, el modelo incluiría un término de interacción \\(i-j\\): \\[ p_{ijk} = P(X^{(1)}=i, X^{(2)}=j)\\cdot P(X^{(3)} = k), \\] por lo que el modelo sería \\[ \\mbox{log}(p_{ijk}) = a_{ij} + b_k. \\] La mayoría de los modelos formales reescriben esto como \\[ a_{ij} = u + v_i + w_j + r_{ij}, \\] de tal forma que el término de interacción \\(P(X^{(1)}=i \\mbox{ y } X^{(2)}=j)\\) es una suma de un “efecto global” \\(u\\), “efectos principales” \\(v_i\\) y \\(w_j\\) y “efectos de interacción” \\(r_{ij}\\), nuevamente de forma análoga a la regresión lineal. Sin embargo, tenga en cuenta que esto en realidad nos da demasiados parámetros. Para el término de interacción \\(a_{ij}\\) del modelo, tenemos \\(2 \\times 3 = 6\\) probabilidades reales, pero \\(1 + 2 + 2 + 2\\times 2 = 9\\) parámetros (1 para \\(u\\), 2 para \\(v_i\\) y así sucesivamente). Por esta razón, generalmente los modelos tienen restricciones de la forma \\[ \\displaystyle{\\sum_i{v_i}}=0. \\] Es posible enumerar todas las restricciones, aunque en la mayoría de los modelos aún con restricciones el número de parámetros puede ser muy grande. ¿Qué modelo es más apropiado en el ejemplo anterior? \\(p_{ijk} = P(X^{(1)}=i) \\cdot P(X^{(2)}=j, X^{(3)}=k)\\) \\(p_{ijk} = P(X^{(1)}=i, X^{(2)}=j, X^{(3)}=k)\\) \\(p_{ijk} = P(X^{(1)}=i)\\cdot P(X^{(2)}=j)\\cdot P(X^{(3)}=k)\\) \\(p_{ijk} = P(X^{(1)}=i, X^{(3)}=k) \\cdot P(X^{(2)}=j)\\) Otro posible modelo tendría Admitido y Género condicionalmente independientes, dado Departamento, lo que significa que en cualquier género, la proporción de admitidos y su género, no están relacionados. Escribimos el modelo de esta manera \\[ \\begin{eqnarray*} p_{ijk} &amp;=&amp; P(X^{(1)}=i, X^{(2)}=j, X^{(3)}=k) \\\\ &amp;=&amp; P(X^{(1)}=i, X^{(2)}=j|X^{(3)}=k) \\cdot P(X^{(3)}=k) \\\\ &amp;=&amp; P(X^{(1)}=i|X^{(3)}=k) \\cdot P(X^{(2)}=j|X^{(3)}=k) \\cdot P(X^{(3)}=k), \\end{eqnarray*} \\] y el modelo sería de la forma \\[ \\mbox{log}(p_{ijk}) = u + a_i + f_{ik} + b_j + h_{jk} + c_k. \\] ¿Cuántos parámetros tendría este modelo? 30 35 40 45 1.4 Interpretación de parámetros Consideremos los modelos: \\[ \\begin{eqnarray*} \\mbox{(1)} &amp; \\qquad &amp; \\mbox{log}(p_{ijk}) = a_{ij} + b_k, \\\\ \\mbox{(2)} &amp; \\qquad &amp; \\mbox{log}(p_{ijk}) = u + a_i + f_{ik} + b_j + h_{jk} + c_k. \\end{eqnarray*} \\] La independencia que representa el modelo (1) tiene una interpretación muy diferente a las independencias representadas por el modelo (2). 1.4.1 Ejemplo: dos monedas Supongamos que tenemos una gran caja con monedas de dos tipos. Las monedas de tipo 1 tienen probabilidad \\(p\\) de salir águila, y las monedas tipo 2 tienen probabilidad \\(q\\) de salir águila. Una proporción \\(r\\) de las monedas es del tipo 1. Seleccionamos una moneda al azar de la caja, lanzamos esa moneda \\(M\\) veces, y observamos \\(N\\) águilas. ¿Cuál es la distribución de \\(N\\)? \\[ p_N(k) = r \\dbinom{M}{k}p^k (1-p)^{M-k} + (1-r) \\dbinom{M}{k}q^k (1-q)^{M-k}, \\] donde \\(k=0,1,\\ldots,M\\). Es fácil ver por qué a esta distribución se le conoce como modelo de mezcla. Esta función de distribución de probabilidad es una mezcla de dos funciones de distribución de probabilidad binomiales, con proporciones de mezcla \\(r\\) y \\(1-r\\). Si \\(M\\) es una variable aleatoria con soporte (o rango) \\(R\\) y \\(\\left\\{g_t\\right\\}_{t\\in R}\\) es una colección de funciones de densidad, entonces se dice que \\(h\\) es una función de densidad de mezcla si \\[ h = \\displaystyle{\\sum_{k\\in R}p_M(k)g_k}, \\] cuando \\(M\\) es una variable aleatoria discreta, o si \\[ h = \\displaystyle{\\int_{t\\in R}f_M(t)g_t(u)\\;\\;dt}, \\] cuando \\(M\\) es una variable aleatoria continua. En el ejemplo de la moneda: \\(Y = N\\) \\(S = \\{1,2\\}\\) \\(p_M(1) = r, \\quad p_M(2) = 1-r\\) \\(g_1\\) es la fn de densidad de una va \\(\\mbox{Binomial}(M,p)\\) \\(g_2\\) es la fn de densidad de una va \\(\\mbox{Binomial}(M,q)\\) Dada la elección de moneda, el resultado de la moneda 1 es independiente del resultado de la oneda 2. Sin embargo, los resultados de las monedas no son independientes si no se sabe qué moneda fue seleccionada. Este ejemplo es como el del modelo (2) mencionado arriba. Por otro lado, se sabe que la estatura y el peso están asociadas linealmente, pero ambas son independientes de la preferenvia por algún sabor de helado, una situación similar a la del modelo (1). Es importante considerar las interacciones que están consideradas en el modelo, por ejemplo: en el modelo (1) hay una interacción entre los factores \\(i,j\\) pero no hay interacción con el factor \\(k\\), por lo que \\(i\\) y \\(j\\) se modelan como completamente independientes de \\(k\\) y no entre sí, mientras que en el modelo (2) \\(i\\) y \\(j\\) tienen una interacción con \\(k\\) y no entre sí, por lo que \\(i\\) y \\(j\\) no son independientes entre sí, pero sí los son dado \\(k\\). Consideremos ahora el modelo \\[ \\mbox{log}(p_{ijk}) = u + a_i + f_{ik} + b_j + h_{jk} + l_{ij} + c_k, \\] ¿cuál de las siguientes afirmaciones es cierta? \\(i\\) y \\(j\\) son dependientes dado \\(k\\). \\(i\\) y \\(j\\) son independientes dado \\(k\\). \\(i\\) y \\(k\\) son dependientes dado \\(j\\). \\(i\\) y \\(k\\) son independientes dado \\(j\\). Si hubiéramos incluido un término \\(m_{ijk}\\), que ahora haría que el modelo estuviera lleno (o saturado), entonces sería posible que los factores \\(i\\) y \\(j\\) estuvieran altamente relacionados para algunos valores de \\(k\\), y menos relacionados para otros. Claramente, cuantas más variables tengamos, y cuanto mayor sea el orden de las interacciones que incluimos, más difícil será interpretar el modelo. 1.5 Otros ejemplos 1.5.1 Discriminación de residentes hispanos con discapacidades La mayoría de los estados en los Estados Unidos proporcionan servicios y apoyo a personas con discapacidades (por ejemplo, discapacidad intelectual, parálisis cerebral, autismo, etc.) y sus familias. La agencia a través de la cual el estado de California sirve al desarrollo de la población discapacitada es el Departamento de Servicios de Desarrollo de California (DDS). Una de las responsabilidades de DDS es asignar fondos que respalden a más de 250,000 residentes con discapacidades de desarrollo (denominados “consumidores”). Hace algunos años, se hizo una alegación de discriminación presentando un análisis univariado que examinaba los gastos anuales promedio de los consumidores por etnia. El análisis reveló que el gasto anual promedio en consumidores hispanos (Hispanic) era aproximadamente un tercera parte (1/3) del gasto promedio en consumidores blancos no hispanos (White non-Hispanic). Este hallazgo fue el catalizador para una mayor investigación; posteriormente, los legisladores estatales y los gerentes de departamento buscaron servicios de consultoría de un estadístico. El conjunto de datos utilizado en el análisis contiene seis variables: ID: identificador único por consumidor, Categoría de edad: es una variable importante porque, aunque la edad suele ser causa de discriminación legal, en este caso, para la población específica de Hispanos no americanos la edad no se consideraría relacionada con los casos de discriminación. El propósito de estas ayudas es que los que viven con alguna discapacidad puedan vivir igual que los que no tienen ninguna discapacidad. Es lógico, por lo tanto, que mientras las personas tienen mayor edad requieran de mayor ayuda económica. Los seis grupos de edad utilizados en este ámbito son: 0 a 5 años de edad, 6 a 12, 13 a 17, 18 a 21, 22 a 50, y más de 51. Edad: edad del consumidor, Género: se incluye en los datos como una variable a considerar porque el género es otro factor sujeto a discriminación, Gastos: el gasto anual que el gobierno le dedica a un consumidor para apoyar a estos individuos y sus familias. El gasto se dedica a: ayuda a la familia, servicios psicológicos, gastos médicos, transporte y costos relacionados a la vivienda como la renta. Origen étnico: es la variable más importante ya que con respecto a esta variable se presentaron las supuestas alegaciones por discriminación. Los primeros 10 renglones de la tabla se muestran a continuación: dds &lt;- read_csv(&quot;datos/californiaDDSData.csv&quot;) dds %&gt;% head(10) %&gt;% knitr::kable() Id Age Cohort Age Gender Expenditures Ethnicity 10210 13-17 17 Female 2113 White not Hispanic 10409 22-50 37 Male 41924 White not Hispanic 10486 0-5 3 Male 1454 Hispanic 10538 18-21 19 Female 6400 Hispanic 10568 13-17 13 Male 4412 White not Hispanic 10690 13-17 15 Female 4566 Hispanic 10711 13-17 13 Female 3915 White not Hispanic 10778 13-17 17 Male 3873 Black 10820 13-17 14 Female 5021 White not Hispanic 10823 13-17 13 Male 2887 Hispanic Se puede ver que la columna “Age Cohort” tiene la categoría de edad a la cual correponde la observación. También podemos utilizar la función glimpse() del paquete tibble: glimpse(dds) #&gt; Observations: 1,000 #&gt; Variables: 6 #&gt; $ Id &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, ... #&gt; $ `Age Cohort` &lt;chr&gt; &quot;13-17&quot;, &quot;22-50&quot;, &quot;0-5&quot;, &quot;18-21&quot;, &quot;13-17&quot;, &quot;13-17... #&gt; $ Age &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15... #&gt; $ Gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Fema... #&gt; $ Expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, ... #&gt; $ Ethnicity &lt;chr&gt; &quot;White not Hispanic&quot;, &quot;White not Hispanic&quot;, &quot;Hisp... Podemos ver que el conjunto de datos contiene una muestra de exactamente 1000 observaciones que fueron seleccionadas aleatoriamente. Veamos una tabla del gasto promedio por etnicidad: dds %&gt;% group_by(Ethnicity) %&gt;% summarise(Gasto_promedio = round(mean(Expenditures),0)) %&gt;% knitr::kable() Ethnicity Gasto_promedio American Indian 36438 Asian 18392 Black 20885 Hispanic 11066 Multi Race 4457 Native Hawaiian 42782 Other 3316 White not Hispanic 24698 Podemos comparar también con el promedio de todos los consumidores: mean(dds$Expenditures) #&gt; [1] 18066 Es común hacer gráficas de barras para representar medias, aunque en realidad, esto no es lo más recomendable: media_por_etnia &lt;- dds %&gt;% group_by(Ethnicity) %&gt;% summarise(Media_etnia = mean(Expenditures)) media_por_etnia$Etnia &lt;- reorder(media_por_etnia$Ethnicity, -media_por_etnia$Media_etnia, FUN = median) ggplot(media_por_etnia, aes(x = Etnia, y = Media_etnia)) + geom_bar(stat = &#39;identity&#39;) + theme(axis.text.x = element_text(angle=25)) ¿Se puede concluir que existe evidencia de discriminación contra los Hispanos en comparación con los blancos no hispanos? Sí No Con respecto a discriminación de género, usualmente concluiríamos que no hay evidencia de discriminación: dds %&gt;% group_by(Gender) %&gt;% summarise(Gasto_promedio = round(mean(Expenditures),0)) %&gt;% knitr::kable() Gender Gasto_promedio Female 18130 Male 18001 Las necesidades de los consumidores aumentan conforme envejecen, lo que resulta en mayores gastos para personas de mayor edad: dds$Age_Cohort &lt;- ordered(dds$`Age Cohort`, levels=c(&quot;0-5&quot;,&quot;6-12&quot;,&quot;13-17&quot;,&quot;18-21&quot;,&quot;22-50&quot;,&quot;51+&quot;)) dds %&gt;% group_by(Age_Cohort) %&gt;% summarise(Gasto_promedio = round(mean(Expenditures),0)) %&gt;% knitr::kable() Age_Cohort Gasto_promedio 0-5 1415 6-12 2227 13-17 3923 18-21 9889 22-50 40209 51+ 53522 El problema de que haya discrimnación o no se puede analizar más a fondo viendo, por ejemplo, qué porcentaje de consumidores pertenecen a cada etnia, tal como sucedió en el ejemplo de la controversia de Berkeley: dds %&gt;% group_by(Ethnicity) %&gt;% summarise(num_cons = n(), porc_cons = paste0(num_cons/10,&#39;%&#39;)) %&gt;% knitr::kable() Ethnicity num_cons porc_cons American Indian 4 0.4% Asian 129 12.9% Black 59 5.9% Hispanic 376 37.6% Multi Race 26 2.6% Native Hawaiian 3 0.3% Other 2 0.2% White not Hispanic 401 40.1% Podemos observar que los dos grandes grupos pertenecen a las 2 etnias del problema de discriminación que estamos analizando de blancos no hispanos vs hispanos. Examinemos de nuevo las medias y el porcentaje de consumidores de estos dos grupos: dds_blancos_hispanos &lt;- dds %&gt;% filter(Ethnicity %in% c(&quot;Hispanic&quot;,&quot;White not Hispanic&quot;)) dds_blancos_hispanos %&gt;% group_by(Ethnicity) %&gt;% summarise(media_gasto = mean(Expenditures), porc_consum = n()/10) %&gt;% knitr::kable() Ethnicity media_gasto porc_consum Hispanic 11066 37.6 White not Hispanic 24698 40.1 Tiende a haber un consenso general de que hay una diferencia significativa en la cantidad promedio de gastos entre el grupo de blancos no hispanos y el de hispanos. ¿Por qué podría haber diferencias en los promedios? ¿Se puede determinar si realmente existe discriminación? Algunas razones que sugieren normalmente son: Los hispanos tienen más apoyo familiar, y por lo tanto, es menos probable que busquen asistencia financiada por el gobierno, Los hispanos están menos informados sobre cómo buscar ayuda. Ambas razones son difíciles de modelar y podrían apoyar alegaciones de discriminación, en vez de negarlas. Analicemos ahora diferencias para cada grupo de edad entre hispanos y blancos: dds_blancos_hispanos %&gt;% group_by(Ethnicity, Age_Cohort) %&gt;% summarise(media_gasto = mean(Expenditures)) %&gt;% spread(Ethnicity, media_gasto) #&gt; # A tibble: 6 x 3 #&gt; Age_Cohort Hispanic `White not Hispanic` #&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0-5 1393. 1367. #&gt; 2 6-12 2312. 2052. #&gt; 3 13-17 3955. 3904. #&gt; 4 18-21 9960. 10133. #&gt; 5 22-50 40924. 40188. #&gt; 6 51+ 55585 52670. ¿Se puede concluir que el típico hispano recibe menos fondos (es decir, gastos) que el típico blanco? Dado que la cantidad típica de gastos para los hispanos (en todos excepto un grupo de edad) es más alta que la cantidad típica de gastos para los blancos que no son hispanos en cada grupo de edad (excepto en uno), la hipótesis de discriminación sería refutada. Si un consumidor hispano fuera a reclamar discriminación porque es hispano (frente a blancos no hispanos), podría hacerlo con base en el promedio general de gastos para todos los consumidores de su grupo. Podemos entender mejor por qué esta aparente asociación desaparece cuando consideramos la variable de grupo de edad si analizamos el porcentaje de consumidores en cada categoría de edad para los grupos de hispanos y blancos: dds_blancos_hispanos %&gt;% group_by(Ethnicity) %&gt;% mutate(num_etnia = n()) %&gt;% ungroup() %&gt;% group_by(Ethnicity, Age_Cohort) %&gt;% summarise(porc_grupo_edad = round(n()/first(num_etnia)*100,2)) %&gt;% spread(Ethnicity, porc_grupo_edad) #&gt; # A tibble: 6 x 3 #&gt; Age_Cohort Hispanic `White not Hispanic` #&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0-5 11.7 4.99 #&gt; 2 6-12 24.2 11.5 #&gt; 3 13-17 27.4 16.7 #&gt; 4 18-21 20.7 17.2 #&gt; 5 22-50 11.4 33.2 #&gt; 6 51+ 4.52 16.5 Veamos estas medias como un promedio ponderado por grupo de edad: \\[ \\bar{X}_k = \\displaystyle{\\sum_{i=1}^{m}w_{ki}\\bar{X}_{ki}}, \\] donde \\(\\bar{X}_k\\) es la media del \\(k\\)-ésimo grupo étnico, \\(w_{ki}\\) es el porcentaje del \\(k\\)-ésimo grupo étnico en el \\(i\\)-ésimo grupo de edad, y \\(\\bar{X}_{ki}\\) es la media de gasto del \\(k\\)-ésimo grupo étnico en el \\(i\\)-ésimo grupo de edad. Los pesos \\(w_{ki}\\) para la población hispana son más altos para los 4 grupos de edad más jóvenes y más bajas para los 2 grupos de edad más viejos, en comparación con la población blanca no hispana. En otras palabras, la población total de consumidores hispanos es relativamente más joven en comparación con la población de consumidores blancos no hispanos. Dado que los gastos para los consumidores más jóvenes son más bajos, el promedio general de los gastos para los hispanos (frente a los blancos no hispanos) es menor. Factores de causa común. Incluso cuando no hay factores de confusión, un fenómeno puede realmente surgir de múltiples causas. Puede ocurrir que exista una correlación entre dos variables, sin embargo, es posible que esto no te diga nada cuando estos dos factores tienen como causa común a un tercer factor. Además, cuando la causalidad es múltiple, una causa puede ocultar a otra. 1.5.2 Consumo de chocolate y premios Nobel En un artículo reciente se publicó un resultado que demuestra una correlación estadísticamente significativa entre el consumo de chocolate per capita y el número de premios Nobel del país por 10 millones de habitantes. El artículo se puede consultar aquí: Messerli, 2012. En el artículo está publicada esta gráfica: Los datos de consumo de chocolate per cápita provienen de fuentes de datos distintas: confectionerynews.com, theobroma-cacao.de, y caobisco. Por otro lado, los datos del número de premios Nobel por cada 10 millones de habitantes están publicados en Wikipedia. Mientras que Messerli advierte en su artículo que la existencia de una correlación no implica causalidad, esto no impidió que los medios populares publicaran historias con estos titulares: “Eating Chocolate May Help You Win Nobel Prize” - CBS News There’s A Shocking Connection Between Eating More Chocolate And Winning The Nobel Prize - Business Insider “Why Chocolate Makes You Smart (or Peaceful)” - Psychology Today “Study links eating chocolate to winning Nobels” - USA Today Como describimos anteriormente, se tienen datos para varios años del consumo per capita de chocolate (en kg). chocolate_nobel &lt;- read_csv(&quot;datos/chocolate_nobel.csv&quot;) chocolate_nobel %&gt;% sample_n(10) %&gt;% knitr::kable() Country Year Cons_per_capita Nobel_Laureates Population_2017 Laureates_per_10_million US 2004 5.30 335 3.24e+08 10.325 Ireland 2006 7.64 2 4.67e+06 4.281 Switzerland 2011 10.55 21 8.48e+06 24.776 Hungary 2009 3.58 8 9.72e+06 8.229 Austria 2007 8.22 18 8.74e+06 20.606 Germany 2011 11.60 91 8.21e+07 11.082 Netherlands 2012 5.40 19 1.70e+07 11.153 Spain 2007 3.27 2 4.64e+07 0.431 Finland 2009 6.87 3 5.52e+06 5.432 US 2012 5.50 335 3.24e+08 10.325 Podemos ver una gráfica del consumo de chocolate vs el número de premios nobel: ggplot(chocolate_nobel, aes(x=Cons_per_capita, y = Laureates_per_10_million)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) El artículo original era más una nota sarcástica, que un artículo de investigación. Muchos artículos, blogs y medios mostraron que esta aparente correlación no tiene sentido. Estas críticas muestran que el número de Nobel para 10 millones de habitantes también está “correlacionado” con el PIB per cápita, el índice de desarrollo humano, el consumo de todo tipo de bienes de lujo, etc. Factores de interacción. Incluso cuando las variables no están correlacionadas por completo, la importancia de cada una puede depender de la otra. Por ejemplo, las plantas se benefician tanto de la luz como del agua. Pero en ausencia de cualquiera, el otro no es en absoluto beneficioso. Tales interacciones ocurren en una gran cantidad de sistemas. Por lo tanto, la inferencia efectiva sobre una variable generalmente dependerá de la consideración de otras variables. Inferencia causal. A pesar de su importancia central, todavía no existe un enfoque unificado para hacer inferencia causal en las ciencias o en estadística. Incluso hay personas que argumentan que la causa realmente no existe, que es una ilusión psíquica. Por ejemplo, en sistemas dinámicos complejos todo parece causar todo lo demás, por lo que el término “causa” pierde valor intuitivo. Sin embargo, existe un acuerdo general: la inferencia causal siempre depende de supuestos no verificables. Otra forma de decir esto es que siempre nos será posible imaginar alguna forma en la que la inferencia sobre la causa sea incorrecta, sin importar qué tan cuidadosamente se haya realizado el diseño o el análisis. En este curso nuestros análisis jamás van a pretender hacer alguna inferencia sobre la causa de los fenómenos observados; únicamente se harán afirmaciones sobre las asociaciones, interacciones y relaciones entre las variables en los datos observados. La mayoría de nuestros análisis se van a enfocar en datos que fueron obtenidos sin que activamente se controlara o manipulara cualquiera de las variables en las cuales se hicieron las mediciones. Los diseños estadísticos en los cuales se controla alguna de las variables observadas en los datos se estudian en el curso de Diseño de experimentos. Generalmente vamos a suponer que los factores (o variables) observados son aleatorios. Esto quiere decir que nuestro análisis va a estar basado en el supuesto de que los datos provienen de una muestra aleatoria de la población de interés en un momento determinado del tiempo. En muchas ramas de la estadística, contar con datos temporales es muy importante. Por ejemplo, en el curso de Análisis de supervivencia se estudia el uso de modelos estadísticos en aplicaciones en las cuáles se desea estimar la distribución de un período entre dos eventos, como la duración del empleo (tiempo transcurrido entre el contrato y el abandono de la empresa), del tiempo de vida de un paciente, la diferencia en algún beneficio terapéutico sobre la prolongación de la vida para un nuevo tratamiento con respecto al tratamiento tradicional, o el tiempo de falla en un sistema mecánico. Hoy en día, el uso de herramientas computacionales ha adquirido importancia en la ciencia estadística. Esto resultó en el desarrollo de nuevas técnicas computacionales para fines estadísticos, tales como el uso de muestreo para estimar cantidades estadísticas o parámetros (bootstrap), la simulación de variables aleatorias, la simulación de modelos probabilísticos, la simulación de modelos multivariados, y la inferencia de gráficas estadísticas. Estos temas se ven en el curso de Estadística computacional. La técnica de simulación ha sido esencial en los últimos años. Se ha desarrollado una clase de métodos de simulación para poder calcular la distribución posterior, estos se conocen como cadenas de Markov via Monte Carlo (MCMC por sus siglas en inglés). El desarrollo de los métodos MCMC es lo que ha propiciado el desarrollo de la estadística bayesiana en años recientes. En el curso de Estadística bayesiana se estudia la teoría básica que sirve de fundamento para la estadística bayesiana: la teoría de decisión, la probabilidad subjetiva, la utilidad, la inferencia como problema de decisión, y la inferencia paramétrica bayesiana. El curso de Regresión avanzada está diseñado para estudiar inferencia bayesiana y el uso de modelos estadísticos bayesianos en el análisis de aplicaciones reales actuales. El enfoque es en modelos lineales generalizados, modelos dinámicos y modelos jerárquicos o multinivel. El uso de la estadística computacional con el fin de hacer predicciones, aprovecha la optimización numérica para estudiar métodos que son útiles para reconocer patrones. En el curso de Aprendizaje estadístico se estudian modelos lineales para reconocimiento de patrones, clasificación, y predicción, la regresión múltiple y descenso en gradiente, las redes neuronales (y deep learning), máquinas de soporte vectorial, los árboles y bosques aleatorios. En el curso de Métodos analíticos se ven otras técnicas de minería de datos, tales como el análisis de market basket, local sensitivity hashing (LHS), la minería de flujos de datos, los algoritmos de recomendación y la minería de texto. Finalmente, debido a la importancia antes mencionada del uso de varias variables en los modelos estadísticos actuales, hay nuevas técnicas estadísticas para estudiar fenómenos multivariados desde una perspectiva bayesiana, como las redes bayesianas, los modelos gráficos no dirigidos, las redes markovianas, los modelos para datos faltantes, modelos de variables latentes, como los modelos de rasgos latentes (LTM), los modelos de perfiles latentes (LPM), los modelos de clases latentes (LCM), y los modelos markovianos de estados ocultos (HMM). Todas estas técnicas se ven en el curso de Estadística multivariada. En este curso nuestro enfoque tendrá una persepctiva frecuentista. 1.6 Tarea Recordemos que la devianza la definimos como \\(-2\\) multiplicado por la log-verosimilitud: \\[ D = -2\\, \\mbox{log}{\\left(p(X|\\hat{\\theta})\\right)} \\] donde \\(X\\) son los datos observados y \\(\\hat{\\theta}\\) es el parámetro a estimar. Generalmente nos interesa disminuir la devianza. Con los datos del ejemplo de discriminación a hispanos modela la probabilidad \\(p_{ijk}\\) de cada categoría \\((i,j,k)\\) correspondiente al gasto, raza y categoría de edad, respectivamente. Puedes utilizar la función loglin vista en clase o la función loglm del paquete MASS. Esta última te permite especificar el modelo en forma de función. Puedes ver la ayuda así: library(MASS) ?loglm Filtra los datos para aquellas observaciones que sólo sean de hispanos o blancos no hispanos y llena la siguiente tabla utilizando factores de interacción como lo vimos anteriormente con las variables G (categoría del gasto del gobierno en discapacitados), H (Hispano o Blanco no hispano), y E (Categoría de edad). Para esto deberás crear una variable de categoría de gasto del gobierno. Puede hacerlo utilizando cuantiles con la función cut2 del paquete Hmisc. Puedes usar el número de grupos que creas que es más adecuado. Por ejemplo, el modelo “G + H + E” representa el modelo de independencias. Para el modelo “GH + GE + HE” hay interacciones entre: GH: gasto en discapacitados y si es hispano, GE: gasto y edad, y HE: hispano y edad. El último modelo tendría parámetros \\(u\\), \\(a_i\\), \\(b_{ij}\\), \\(c_j\\), \\(d_{ik}\\), \\(e_{jk}\\), y \\(f_k\\) tales que \\[ \\mbox{log}(p_{ijk}) = u + a_i + b_{ij} + c_j + d_{ik} + e_{jk} + f_k. \\] Modelo Devianza Grados de libertad Número de parámetros G + H + E GH + E GE + H G + HE GH + GE GH + HE GH + GE + HE Di qué modelo es mejor, tanto en términos del ajuste de la devianza y su interpretabilidad y explica por qué. ¿Hay algún modelo que no esté en la lista que sea el más apropiado para ajustar los datos? Manda tu tarea por correo electrónico a: andreuboadadeatela@gmail.com con el asunto “EAPLICADA3-Tarea-[XX]-[clave única 1]-[clave única 2]” donde [XX] es el número de la tarea (en este caso es la tarea 01), y [clave única 1] y [clave única 2] son tu clave y la de tu compañero con quien vas a trabajar durante el semestre. Referencias "],
["Rintro.html", "Clase 2 Temas selectos de R 2.1 ¿Qué ventajas tiene R? 2.2 Flujo básico de trabajo para el análisis de datos en R. 2.3 Introducción a R como lenguaje de programación, y la plataforma interactiva de RStudio. 2.4 Estructuras de datos 2.5 R Markdown 2.6 Proyectos de RStudio 2.7 Otros aspectos importantes de R 2.8 Tarea", " Clase 2 Temas selectos de R .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } El lenguaje de programación R ha surgido como un avance en el desarrollo de software para análisis estadítico. Hace unos años era común el uso de productos de software proprietario, tales como GAUSS, RATS, EVIEWS, SPSS, SAS, Matlab, Minitab, Stata y software que en principio ni siquiera es apropiado para el análisis de datos, como Microsoft Excel. Estos programas generalmente son demasiado costosos y tienen un rendimiento bajo. Si es necesario hacer un análisis más complejo, entonces los archivos se vuelven demasiado grandes y el todo el proceso se vuelve infactible. 2.1 ¿Qué ventajas tiene R? R es la herramienta más sobresaliente para la estadística, el análisis de datos y el aprendizaje estadístico. Es más que un paquete estadístico; es un lenguaje de programación, por lo que puede crear sus propios objetos, funciones y paquetes. Hablando de paquetes, hay más de 12,000 innovadores paquetes aportados por los usuarios, y que están disponibles en CRAN (The Comprehensive R Archive Network), eso sin mencionar Bioconductor. Para tener una idea de qué paquetes hay disponibles, puedes leer posts en R-bloggers y ver el . Muchos paquetes son enviados por miembros prominentes de sus respectivos campos. Al igual que todos los programas, los programas de R documentan explícitamente los pasos de su análisis y esto facilita la reproducibilidad del análisis estadístico. Además, provee de herramientas para probar rápidamente muchas ideas y corregir fácilmente los problemas que puedan surgir. R puedes usarlo fácilmente en cualquier lugar. Es independiente de la plataforma, por lo que puede usarlo en cualquier sistema operativo. Y es gratis, por lo que puede usarlo en cualquier empleador sin tener que persuadir a su jefe para comprar una licencia. 2.1.1 R es gratuito y de código abierto R está disponible bajo una licencia de código abierto, lo que significa que cualquiera puede descargar y modificar el código. Esta libertad a menudo se conoce como la de software libre (“free as in speech”). R también está disponible de manera gratuita (“free as in beer”). En términos prácticos, esto significa que podemos descargar y usar R gratis. Otro beneficio, aunque un poco más indirecto, es que cualquiera puede acceder al código fuente, modificarlo y mejorarlo. Como resultado, muchos programadores excelentes han contribuido con mejoras y correcciones al código de R. Por esta razón, R es muy estable y confiable. Cualquier libertad también tiene asociadas ciertas obligaciones. En el caso de R, estas obligaciones se describen en las condiciones de la licencia bajo la cual se publica: Licencia Pública General de GNU (GPL), Versión 2. Estas obligaciones te pertienen si solamente haces uso de R. Sin embargo, si haces cambios en su código fuente R y lo redistribuyes, entonces estos cambios se deben poner a disposición de todos los usuarios. 2.1.2 R tiene una comunidad comprometida Muchas personas que usan R eventualmente comienzan a ayudar a los nuevos usuarios y proponen el uso de R en sus lugares de trabajo y círculos profesionales. Por ejemplo, si tienes dudas sobre algún aspecto de R, podrás encontrar ayuda en Stack Overflow. R-Ladies CDMX es parte de R-Ladies Global, una organización mundial que busca generar una comunidad fuerte para compartir dudas, habilidades y apoyo sobre #RStats en una comunidad con perspectiva de género. 2.2 Flujo básico de trabajo para el análisis de datos en R. En el análisis de datos nos interesan técnicas cuantitativas cómo: recolectar, organizar, entender, interpretar y extraer información de colecciones de datos predominantemente numéricos. Estas tareas se resumen en el proceso de análisis del siguiente diagrama: Primero debe importar los datos en R. Esto generalmente significa llevar los datos almacenados en un archivo, una base de datos, o uan Web API, a un data frame de R. Limpiar y transformar los datos es necesario, para que la forma en que se almacenan los datos coincida con la semántica de los datos. En términos generales, cada columna debe ser una variable y cada rengón una observación. La visualización es una actividad fundamentalmente humana. Una buena visualización te puede mostrar cosas que no esperabas y puede ayudarte a plantear nuevas preguntas acerca de los datos. Una buena visualización también puede ayudar a determinar si se está haciendo una pregunta equivocada sobre los datos, o si es encesario recolectar más datos, o bien, obtener datos de fuentes distintas. Las visualizaciones pueden sorprenderte, pero requieren de un ser humano para interpretarlas. Por otro lado, los modelos son una herramienta para complementar las visualizaciones. Los modelos los utilizamos como un instrumento matemático y computacional para responder preguntas precisas acerca de los datos. Por último, la comunicación de los resultados es una parte absolutamente crítica para cualquier proyecto de análisis de datos. 2.3 Introducción a R como lenguaje de programación, y la plataforma interactiva de RStudio. Notas basadas en el material de Teresa Ortiz y Sonia Mendizábal y en el libro “R for Data Science” escrito por Hadley Wickham y Garret Grolemund (Wickham and Grolemund 2016). Hay cuatro cosas que necesitan para ejecutar el código en este taller: R, RStudio, una colección de paquetes de R, llamada tidyverse, y otros paquetes que vamos a ir instalando progresivamente. Los paquetes son la unidad fundamental del código reproducible en R. Incluyen funciones que se pueden utilizar en un ámbito general, su documentación que describe cómo usarlas y datos de ejemplo con código de ejemplo como ayuda para los usuarios. 2.3.1 ¿Cómo entender R? Hay una sesión de R corriendo. La consola de R es la interfaz entre R y nosotros. En la sesión hay objetos. Todo en R es un objeto: vectores, tablas, funciones, etc. Operamos aplicando funciones a los objetos y creando nuevos objetos. 2.3.2 ¿Por qué R? R funciona en casi todas las plataformas (Mac, Windows, Linux e incluso en Playstation 3). R promueve la investigación reproducible. R está actualizado gracias a que tiene una activa comunidad. R se puede combinar con otras herramientas. R permite integrar otros lenguajes (C/C++, Java, Julia, Python) y puede interactuar con muchas fuentes de datos: bases de datos compatibles con ODBC y paquetes estadísticos. 2.3.2.1 Descargar R: versión 3.4.3 Sigue las instrucciones del instalador: OSX: http://cran.stat.ucla.edu/bin/macosx/R-3.4.3.pkg Windows: http://cran.stat.ucla.edu/bin/windows/base/R-3.4.3-win.exe 2.3.2.2 Descargar RStudio: versión 1.1.414 OSX: https://download1.rstudio.org/RStudio-1.1.414.dmg Windows: https://download1.rstudio.org/RStudio-1.1.414.exe RStudio es libre y gratis. Es un ambiente de desarrollo integrado para R: incluye una consola, un editor de texto y un conjunto de herramientas para administrar el espacio de trabajo cuando se utiliza R. Algunos shortcuts útiles en RStudio son: En el editor command/ctrl + enter: enviar código a la consola ctrl + 2: mover el cursor a la consola En la consola flecha hacia arriba: recuperar comandos pasados ctrl + flecha hacia arriba: búsqueda en los comandos ctrl + 1: mover el cursor al editor Más alt + shift + k: muestra los shortcuts disponibles. Para que el código sea reproducible es importante que RStudio únicamente guarde lo relevante para hacer los cálculos, es decir, los scripts y no los cálculos en sí. Con tus scripts de R (y los datos), siempre podemos volver a crear las variables de ambiente. Sin embargo, es casi imposible recuperar un script únicamente a partir de tus variables de ambiente. Por lo tanto, se recomienda ampliamente configurar RStudio para que jamás guarde el ambiente en memoria. 2.3.2.3 Paquetes de R Una de las ventajas de R es la gran comunidad que aporta al desarrollo por medio de paquetes que dan funcionalidad adicional. Esta es la mejor manera de usar R para análisis de datos. Existen dos formas de instalar paquetes: Desde RStudio: Desde la consola: install.packages(&#39;tidyverse&#39;) Una vez instalados los paquetes, se cargan a la sesión de R mediante library. Por ejemplo, para cargar el paquete readr que instalamos anteriormente, hacemos: library(&#39;tidyverse&#39;) print(read_csv) #&gt; function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), #&gt; na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;, #&gt; trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, #&gt; n_max), progress = show_progress()) #&gt; { #&gt; tokenizer &lt;- tokenizer_csv(na = na, quoted_na = TRUE, quote = quote, #&gt; comment = comment, trim_ws = trim_ws) #&gt; read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, #&gt; locale = locale, skip = skip, comment = comment, n_max = n_max, #&gt; guess_max = guess_max, progress = progress) #&gt; } #&gt; &lt;bytecode: 0x4fab4f0&gt; #&gt; &lt;environment: namespace:readr&gt; Como el paquete readr está cargado en la sesión podemos llamar a la función read_csv que se usará más adelante. Importante: Los paquetes se instalan una vez únicamente después de descargar una nueva versión de R. Las librerías se cargan en cada sesión de R nueva. 2.3.2.4 Ayuda en R Existen diferentes formas de pedir ayuda en R. help.start(): ayuda en general help(fun) o ?fun: ayuda sobre la función fun apropos(&quot;fun&quot;): lista de funciones que contiene la palabra fun example(fun): muestra un ejemplo de la función fun help(read_csv) ?read_csv2 2.4 Estructuras de datos Todo lo que existe en R es un objeto. En R se puede trabajar con distintas estructuras de datos, algunas son de una sola dimensión y otras permiten más, como indica el siguiente diagrama: 2.4.1 Vectores Los vectores son estructuras de datos de una dimensión. Un vector se define con la función c(), que concatena diferentes elementos del mismo tipo, esto determina el tipo del vector. Nota: En R, la asignación de un nombre al vector, o en general a cualquier objeto, se realiza con el símbolo &lt;-. Se recomienda usar el shortcut alt - genera &lt;-. Los vectores en R pueden ser de diferentes tipos o clases, a continuación se presentan algunos casos. En R, la clase de cada vector se extrae con la función class(). Vectores numéricos: a &lt;- c(1,2.5,3,4.5,5,6.9) a #&gt; [1] 1.0 2.5 3.0 4.5 5.0 6.9 # clase del vector class(a) #&gt; [1] &quot;numeric&quot; Vectores lógicos: bool &lt;- c(T, F, TRUE, FALSE) bool #&gt; [1] TRUE FALSE TRUE FALSE # clase del vector class(bool) #&gt; [1] &quot;logical&quot; Vectores de caracteres: fruits &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;) fruits #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; class(fruits) #&gt; [1] &quot;character&quot; Para la manipulación de caracteres es recomendable el paquete stringr que permite realizar operaciones sobre este tipo de elementos. Más adelante se presenta un ejemplo. La selección de elementos de un vector se realiza con [ ] para indicar la posición. A diferencia de otros lenguajes de programación las posiciones en R incian en 1. # elemento en la posición 1 fruits[1] #&gt; [1] &quot;apple&quot; # elemento en la posición 1 y 5 fruits[c(1,5)] #&gt; [1] &quot;apple&quot; &quot;lemon&quot; En R es posible extraer un valor del vector indexándolo con posiciones negativas: # omitir el elemento en la primera posición fruits[-1] #&gt; [1] &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; Una característica particular de vectores en R, es que cada elemento puede ser nombrado. Para hacer esto se usa la función names(). Por ejemplo, al vector fruits agregemos el nombre en español de la fruta para como el nombre de cada elemento. names(fruits) &lt;- c(&#39;manzana&#39;, &#39;platano&#39;, &#39;naranja&#39;, &#39;piña&#39;, &#39;limón&#39;, &#39;kiwi&#39;) fruits #&gt; manzana platano naranja piña limón kiwi #&gt; &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; # cada elemento tiene un nombre asignado fruits[5] #&gt; limón #&gt; &quot;lemon&quot; Para eliminar los nombres asignados a cada elemento, se asigna NULL a los nombres del vector: #&gt; NULL #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; Los tipos que pueden tener los vectores se muestran en la siguiente figura. Veamos que regresan los siguientes comandos: typeof(TRUE) typeof(1L) typeof(1.5) typeof(&quot;a&quot;) Cada vector tiene 3 propiedades: x &lt;- 1:5 Tipo typeof(x) #&gt; [1] &quot;integer&quot; Longitud length(x) #&gt; [1] 5 Atributos attributes(x) #&gt; NULL Existe la función is.vector(x) para determinar si un objeto es un vector: is.vector(1:3) #&gt; [1] TRUE ¿Qué regresa ìs.vector(factor(1:3))? TRUE FALSE NA Ninguna de las anteriores Ejemplo Del vector de seis frutas diferentes llamado fruits, localiza únicamente las frutas que tengan la letra w. # Cargamos la librería library(stringr) fruits &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;) fruits #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; Esto es posible con la función str_detect(), que regresa un vector booleano para cada elemento del vector donde encontró el patron w. str_detect(fruits, pattern = &#39;w&#39;) #&gt; [1] FALSE FALSE FALSE FALSE FALSE TRUE Ahora, seleccionamos únicamente los elementos del vector que tienen la letra w: # Selecciona el elemento con valor TRUE: kiwi fruits[str_detect(fruits, pattern = &#39;w&#39;)] #&gt; [1] &quot;kiwi&quot; 2.4.1.1 Operaciones de vectores En R las operaciones de vectores son componente a componente. Sumas, multiplicaciones y potencias: # Suma del vector longitud 6 y un vector longitud 1 a &lt;- c(1, 2.5, 3, 4.5, 5, 6.9) b &lt;- 1 a + b #&gt; [1] 2.0 3.5 4.0 5.5 6.0 7.9 # Multiplicaciones componente a componente misma longitud a &lt;- c(1, 2.5, 3, 4.5, 5, 6.9) a*a #&gt; [1] 1.00 6.25 9.00 20.25 25.00 47.61 # Multiplicaciones y potencias a &lt;- c(1, 2.5, 3, 4.5, 5, 6.9) c &lt;- (a^2 + 5)*3 c #&gt; [1] 18.0 33.8 42.0 75.8 90.0 157.8 Comparaciones: En este tipo de operación se obtiene un vector lógico dependiendo si la condición se cumple o no. # Comparar el vector dado un valor específico a &gt; 3 #&gt; [1] FALSE FALSE FALSE TRUE TRUE TRUE a[a &gt; 3] # únicamente elementos que cumple la condicion de ser mayores a 3 #&gt; [1] 4.5 5.0 6.9 fruits != &#39;apple&#39; #&gt; [1] FALSE TRUE TRUE TRUE TRUE TRUE fruits[fruits != &#39;apple&#39;] # únicamente elementos que no son apple #&gt; [1] &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; # Comparar el vector dado otro vector de la misma dimensión x &lt;- c(1, 2, 3, 4, 5, 6) a == x #&gt; [1] TRUE FALSE TRUE FALSE TRUE FALSE a[a == x] # unicamente los elementos iguales y en la misma posición entre a y x #&gt; [1] 1 3 5 Funciones predeterminadas: Algunas funciones predeterminadas del paquete básico de R son muy útiles para la manipulación de vectores y el análisis de datos. A continuación se enlistan algunasde las más comúnes: length: número de elementos en el vector class: clase del vector summary: resumen de información del vector unique: valores unicos del vector table: tabla de frecuencias para cada elemento del vector sum: suma de los elementos del vector mean: promedio de elementos del vector sd: desviación estándar de los elementos del vector cumsum: suma acumulada de elemento anterior del vector Aplica las funciones comúnes enlistadas antes en el vector x &lt;- c(1, 2, 3, 4, 5, 6) 2.4.1.2 Otros tipos de vectores: Existen tipos de vectores con características importantes: Secuencias: los vectores de secuencias se pueden crear con la función seq() o con :, de la siguiente forma: # secuecia de 1 al 10 1:10 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 # secuecia de pares de 0 al 10 seq(0, 10, by = 2) #&gt; [1] 0 2 4 6 8 10 Vectores de fechas: se pueden hacer operaciones y algunas funciones definidas de fechas. El paquete lubridate permite manejar fechas con mayor facilidad. Se incia la secuencia el 08 de agosto de 2016 y se asigna la clase de fecha con la función as.Date(). Se generan en total 10 fechas length.out = 10 y con una distancua semanal by=&quot;1 week&quot;, es decir, se tiene la fecha de 10 semanas consecutivas: library(lubridate) tenweeks &lt;- seq( as.Date(&quot;2016-08-08&quot;), length.out = 10, by=&quot;1 week&quot;) tenweeks #&gt; [1] &quot;2016-08-08&quot; &quot;2016-08-15&quot; &quot;2016-08-22&quot; &quot;2016-08-29&quot; &quot;2016-09-05&quot; #&gt; [6] &quot;2016-09-12&quot; &quot;2016-09-19&quot; &quot;2016-09-26&quot; &quot;2016-10-03&quot; &quot;2016-10-10&quot; class(tenweeks) #&gt; [1] &quot;Date&quot; Se pueden hacer algunas operaciones como se ejemplifica en el siguiente código. # Aumenta un día a cada fecha tenweeks + 1 #&gt; [1] &quot;2016-08-09&quot; &quot;2016-08-16&quot; &quot;2016-08-23&quot; &quot;2016-08-30&quot; &quot;2016-09-06&quot; #&gt; [6] &quot;2016-09-13&quot; &quot;2016-09-20&quot; &quot;2016-09-27&quot; &quot;2016-10-04&quot; &quot;2016-10-11&quot; # Aumenta un día a cada fecha tenweeks #&gt; [1] &quot;2016-08-08&quot; &quot;2016-08-15&quot; &quot;2016-08-22&quot; &quot;2016-08-29&quot; &quot;2016-09-05&quot; #&gt; [6] &quot;2016-09-12&quot; &quot;2016-09-19&quot; &quot;2016-09-26&quot; &quot;2016-10-03&quot; &quot;2016-10-10&quot; weekdays(tenweeks) # Día de la semana de cada fecha #&gt; [1] &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; #&gt; [8] &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; Vectores de factores: este tipo de vector es usado comúnmente para variables categóricas. En R existe la clase factor que se asigna con la función homónima factor() o as.factor(). Un vector de factores tiene dos elementos importantes, levels o niveles y labels o etiquetas. Los niveles determinan las categorías únicas del vector y pueden ser etiquetadas, como se muestra en le siguiente código para un vector de frutas. En este ejemplo se tienen valores de frutas repetidos, se asigna un orden de niveles específicos y etiquetas específicas para cada nivel. fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;, &quot;apple&quot;) # Vector de caracteres a vector de factores fruits.fac &lt;- factor(fruits, levels = c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;), labels = c(&#39;manzana&#39;, &#39;platano&#39;, &#39;naranja&#39;, &#39;piña&#39;, &#39;limón&#39;, &#39;kiwi&#39;) ) fruits.fac #&gt; [1] platano manzana platano naranja piña limón kiwi manzana #&gt; Levels: manzana platano naranja piña limón kiwi # Clase class(fruits.fac) #&gt; [1] &quot;factor&quot; # Niveles etiquetados levels(fruits.fac) #&gt; [1] &quot;manzana&quot; &quot;platano&quot; &quot;naranja&quot; &quot;piña&quot; &quot;limón&quot; &quot;kiwi&quot; # Niveles únicos as.numeric(fruits.fac) #&gt; [1] 2 1 2 3 4 5 6 1 # Agregar un nuevo valor fruits.fac[7] &lt;- &#39;melon&#39; #&gt; Warning in `[&lt;-.factor`(`*tmp*`, 7, value = &quot;melon&quot;): invalid factor level, #&gt; NA generated fruits.fac #&gt; [1] platano manzana platano naranja piña limón &lt;NA&gt; manzana #&gt; Levels: manzana platano naranja piña limón kiwi Importante: En R los vectores no pueden combinar diferentes tipos de elementos. El tipo de elementos es lo que define la clase del vector. Es por esto que en el ejemplo, al sustituir la posición 7 por melon se obtiene un NA, melón no está incluído en los niveles definidos del vector. Existen también los factores ordenados. Por ejemplo, consideremos los datos de flores de iris de Fisher: library(forcats) iris %&gt;% sample_n(10) %&gt;% knitr::kable() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 13 4.8 3.0 1.4 0.1 setosa 125 6.7 3.3 5.7 2.1 virginica 89 5.6 3.0 4.1 1.3 versicolor 24 5.1 3.3 1.7 0.5 setosa 2 4.9 3.0 1.4 0.2 setosa 68 5.8 2.7 4.1 1.0 versicolor 72 6.1 2.8 4.0 1.3 versicolor 42 4.5 2.3 1.3 0.3 setosa 105 6.5 3.0 5.8 2.2 virginica 109 6.7 2.5 5.8 1.8 virginica Este conjunto de datos multivariados fue presentado por el estadístico y biólogo británico Ronald Fisher en su artículo de 1936 “El uso de mediciones múltiples en problemas taxonómicos como un ejemplo de análisis discriminante lineal”. Edgar Anderson recopiló los datos para cuantificar la variación morfológica de las flores de iris de tres especies relacionadas. Los datos fueron recolectadas en la Península de Gaspé. (Fisher 1936) El conjunto de datos consiste de 50 observaciones de cada una de las tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midieron cuatro características de cada muestra: la longitud y el ancho de los sépalos y pétalos, en centímetros. Con base en la combinación de estas cuatro características, Fisher desarrolló un modelo discriminante lineal para distinguir las especies entre sí. Supongamos que queremos analizar la distribución del ancho del sépalo por especie de flor de iris: Esto ocurre porque los factores están ordenados alfabéticamente: levels(iris$Species) #&gt; [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Sería mejor que las especies estuvieran ordenadas por la mediana de la distribución para poder hacer mejores comparaciones. Notemos el uso de la función fct_reorder del paquete forcats. library(forcats) iris$Species_ord &lt;- fct_reorder(iris$Species, iris$Sepal.Width, fun = median) levels(iris$Species_ord) #&gt; [1] &quot;versicolor&quot; &quot;virginica&quot; &quot;setosa&quot; 2.4.2 Data Frames Un data.frame es un conjunto de vectores del mismo tamaño agrupado en una tabla. Son estructuras rectangulares donde cada columna tiene elementos de la misma clase, pero columnas distintas pueden tener diferentes clases. Por ejemplo: tabla &lt;- data.frame( n = 1:6, frutas = fruits[1:6], valor = c(1, 2.5, 3, 4.5, 5, 6.9) ) tabla #&gt; n frutas valor #&gt; 1 1 banana 1.0 #&gt; 2 2 apple 2.5 #&gt; 3 3 banana 3.0 #&gt; 4 4 orange 4.5 #&gt; 5 5 pineapple 5.0 #&gt; 6 6 lemon 6.9 Similar a las funciones de vectores, en data.frames existen funciones predeterminadas que ayudan a su manipulación. head permite ver los primeros 6 elemento del data.frame: head(mtcars) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 #&gt; Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 str describe el tipo de variables en el data.frame: str(mtcars) #&gt; &#39;data.frame&#39;: 32 obs. of 11 variables: #&gt; $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... #&gt; $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... #&gt; $ disp: num 160 160 108 258 360 ... #&gt; $ hp : num 110 110 93 110 175 105 245 62 95 123 ... #&gt; $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... #&gt; $ wt : num 2.62 2.88 2.32 3.21 3.44 ... #&gt; $ qsec: num 16.5 17 18.6 19.4 17 ... #&gt; $ vs : num 0 0 1 1 0 1 0 1 1 1 ... #&gt; $ am : num 1 1 1 0 0 0 0 0 0 0 ... #&gt; $ gear: num 4 4 4 3 3 3 3 4 4 4 ... #&gt; $ carb: num 4 4 1 1 2 1 4 2 2 4 ... dim muestra la dimensión (renglones, columnas) del data.frame: dim(mtcars) #&gt; [1] 32 11 colnames y names muestran los nombres de las columnas del data.frame: names(mtcars) #&gt; [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; #&gt; [11] &quot;carb&quot; rownames muestra el nombre de los renglones del data.frame: rownames(mtcars) #&gt; [1] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; #&gt; [4] &quot;Hornet 4 Drive&quot; &quot;Hornet Sportabout&quot; &quot;Valiant&quot; #&gt; [7] &quot;Duster 360&quot; &quot;Merc 240D&quot; &quot;Merc 230&quot; #&gt; [10] &quot;Merc 280&quot; &quot;Merc 280C&quot; &quot;Merc 450SE&quot; #&gt; [13] &quot;Merc 450SL&quot; &quot;Merc 450SLC&quot; &quot;Cadillac Fleetwood&quot; #&gt; [16] &quot;Lincoln Continental&quot; &quot;Chrysler Imperial&quot; &quot;Fiat 128&quot; #&gt; [19] &quot;Honda Civic&quot; &quot;Toyota Corolla&quot; &quot;Toyota Corona&quot; #&gt; [22] &quot;Dodge Challenger&quot; &quot;AMC Javelin&quot; &quot;Camaro Z28&quot; #&gt; [25] &quot;Pontiac Firebird&quot; &quot;Fiat X1-9&quot; &quot;Porsche 914-2&quot; #&gt; [28] &quot;Lotus Europa&quot; &quot;Ford Pantera L&quot; &quot;Ferrari Dino&quot; #&gt; [31] &quot;Maserati Bora&quot; &quot;Volvo 142E&quot; La forma de indexar data.frames es similar a la de un vector [ ], pero en este caso es posible indexar renglones y columnas: # por posiciones de renglones mtcars[1:4, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 # por posiciones de columnas mtcars[1:4, c(1, 4, 6)] #&gt; mpg hp wt #&gt; Mazda RX4 21.0 110 2.62 #&gt; Mazda RX4 Wag 21.0 110 2.88 #&gt; Datsun 710 22.8 93 2.32 #&gt; Hornet 4 Drive 21.4 110 3.21 # por nombre de renglones específico mtcars[c(&#39;Mazda RX4&#39;, &#39;Mazda RX4 Wag&#39;), ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 mtcars[str_detect(rownames(mtcars), &quot;Mazda&quot; ), ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 También se puede seleccionar o filtrar el data.frame dado una condición: mtcars[mtcars$cyl == 6, ] # Selecciona los carros con número de cilindros mayor a 6 #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 #&gt; Merc 280 19.2 6 168 123 3.92 3.44 18.3 1 0 4 4 #&gt; Merc 280C 17.8 6 168 123 3.92 3.44 18.9 1 0 4 4 #&gt; Ferrari Dino 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6 rbind permite unir dos data.frames por renglones, si y solo si, tiene el mismo número de columnas: rbind(mtcars[str_detect(rownames(mtcars), &quot;Mazda&quot; ), ], mtcars[str_detect(rownames(mtcars), &quot;Hornet&quot;), ]) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 cbind permite unir dos data.frames por columna, si y solo si, tiene el mismo número de renglones: tabla &lt;- data.frame( n = 1:6, frutas = c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;apple&quot;), valor = runif(6) ) tabla #&gt; n frutas valor #&gt; 1 1 apple 0.8746 #&gt; 2 2 banana 0.1749 #&gt; 3 3 orange 0.0342 #&gt; 4 4 pineapple 0.3204 #&gt; 5 5 lemon 0.4023 #&gt; 6 6 apple 0.1957 tabla.color &lt;- data.frame( peso = rnorm(6), color = c(&#39;rojo&#39;, &#39;amarillo&#39;, &#39;naranje&#39;, &#39;amarillo&#39;, &#39;amarillo&#39;, &#39;rojo&#39;) ) tabla.color #&gt; peso color #&gt; 1 -0.244 rojo #&gt; 2 -0.283 amarillo #&gt; 3 -0.554 naranje #&gt; 4 0.629 amarillo #&gt; 5 2.065 amarillo #&gt; 6 -1.631 rojo cbind(tabla, tabla.color) #&gt; n frutas valor peso color #&gt; 1 1 apple 0.8746 -0.244 rojo #&gt; 2 2 banana 0.1749 -0.283 amarillo #&gt; 3 3 orange 0.0342 -0.554 naranje #&gt; 4 4 pineapple 0.3204 0.629 amarillo #&gt; 5 5 lemon 0.4023 2.065 amarillo #&gt; 6 6 apple 0.1957 -1.631 rojo Nota: Una forma de seleccionar una columna es con el símbolo $ (pesitos) y el nombre de la columna. Ejercicio: Del data.frame mtcars realiza lo siguiente: Calcula el promedio de cilindros cyl en los datos. Calcula el número de autos con peso wt mayor a 2. Extrae la información de los coches Merc. Calcula el promedio de millas por galón mpg de los autos Merc. 2.4.3 Listas La lista es una estructura de datos de una dimensión que permite distintas clases de elementos en el objeto. La función list() permite crear objetos de esta clase. Por ejemplo: lista &lt;- list( n = 100, x = &#39;hello&#39;, frutas = fruits, tabla = tabla, ejemlista = list(a = 15:20, b = 1:5) ) lista #&gt; $n #&gt; [1] 100 #&gt; #&gt; $x #&gt; [1] &quot;hello&quot; #&gt; #&gt; $frutas #&gt; [1] &quot;banana&quot; &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; #&gt; [7] &quot;kiwi&quot; &quot;apple&quot; #&gt; #&gt; $tabla #&gt; n frutas valor #&gt; 1 1 apple 0.8746 #&gt; 2 2 banana 0.1749 #&gt; 3 3 orange 0.0342 #&gt; 4 4 pineapple 0.3204 #&gt; 5 5 lemon 0.4023 #&gt; 6 6 apple 0.1957 #&gt; #&gt; $ejemlista #&gt; $ejemlista$a #&gt; [1] 15 16 17 18 19 20 #&gt; #&gt; $ejemlista$b #&gt; [1] 1 2 3 4 5 La lista anterior contiene numeros, caracteres, vectores, data.frames e incluso otra lista con distintas secuencias. Se puede indexar una lista de varias formas: Usando [ ]: extrae el objeto como una lista, incluyendo el nombre asignado: lista[1] #&gt; $n #&gt; [1] 100 Usando [[ ]]: extrae únicamente el objeto respetando la clase de éste y sin incluir nombres: lista[[1]] #&gt; [1] 100 Usando $ mas el nombre: extrae únicamente el objeto: lista$ejemlista$a #&gt; [1] 15 16 17 18 19 20 2.5 R Markdown R Markdown es un sistema para crear documentos, en los cuales se combina tu código de R, los resultados y el texto que escribes como comentario en forma de prosa. Algunas ventajas y características de R Markdown son: cualquier R markdown Rmd es totalmente reproducible admite docenas de formatos de salida, como archivos PDF, Word, presentaciones de diapositivas y más. es muy útil para los tomadores de decisiones, quienes quieren enfocarse en las conclusiones, no en el código detrás del análisis. permite colaborar con otras personas de estadística que estén interesadas en tus conclusiones y cómo llegaste a ellas 2.5.1 ¿Qué es R Markdown? R Markdown integra código de R, comandos de TeX y muchas herramientas externas. Cuando construyes el documento, R Markdown envía un archivo con formato .Rmd a otro paquete llamado knitr, http://yihui.name/knitr/, que ejecuta el código de todos los chunks y crea un nuevo archivo de markdown con formato md que ya incluye el código y los resultados. Este archivo de markdown generado por knitr después es procesado por pandoc, http://pandoc.org/, que es el que crea el archivo final. La ventaja de este flujo de trabajo de dos pasos es que te permite crear una amplia gama de formatos de salida. 2.5.2 Estructura básica de R Markdown Éste es un R Markdown, un archivo de texto sin formato que tiene la extensión .Rmd: cat(htmltools::includeText(&quot;rmarkdown/ejemplo.Rmd&quot;)) #&gt; --- #&gt; title: &quot;Ejemplo de R Markdown&quot; #&gt; date: 2018-01-22 #&gt; output: html_document #&gt; --- #&gt; #&gt; Veamos unos datos de diamantes para analizar la distribución #&gt; del quilataje de aquellos diamantes que tiene quilataje #&gt; menor a 2.5: #&gt; #&gt; ```{r setup, include = FALSE} #&gt; library(ggplot2) #&gt; library(dplyr) #&gt; #&gt; smaller &lt;- diamonds %&gt;% #&gt; filter(carat &lt;= 2.5) #&gt; ``` #&gt; #&gt; En el __chunk__ de arriba se hizo el filtro adecuado, ahora #&gt; veamos una muestra de tamaño 10 de los datos: #&gt; #&gt; ```{r, echo = FALSE} #&gt; smaller %&gt;% #&gt; sample_n(10) %&gt;% #&gt; knitr::kable() #&gt; ``` #&gt; #&gt; Los datos corresponde a `r nrow(diamonds)` diamantes. Solamente #&gt; `r nrow(diamonds) - nrow(smaller)` son de más de 2.5 quilates. #&gt; La distribución de los diamantes de menor quilataje se muestra abajo: #&gt; #&gt; ```{r, echo = FALSE} #&gt; smaller %&gt;% #&gt; ggplot(aes(carat)) + #&gt; geom_freqpoly(binwidth = 0.01) #&gt; ``` Contiene tres tipos importantes de contenido: 1 Un encabezado en formato YAML rodeado por ---s.. Chunks de código de R rodeados por ```. Texto mezclado con formato de texto simple como # heading y _italics_. Cuando abres un .Rmd, RStudio muestra una interfaz de tipo notebook en la cual tanto el código como la salida están intercalados. Puedes ejecutars cada chunk de código presionando el botón de “Run” (en la parte superior derecha de la ventana de script), o bien, Cmd/Ctrl + Shift + Enter. RStudio ejecuta el código y muestra los resultados junto con el código. Para generar un informe completo que contenga todo el texto, el código y los resultados, presiona el botón “Knit”, o bien, Cmd/Ctrl + Shift + K. Esto generará un reporte en una nueva ventana y creará un archivo HTML independiente que podrás compartir con los demás. Para comenzar con tu propio archivo .Rmd, selecciona File &gt; New File &gt; R Markdown… en la barra superior. RStudio te mostrará un asistente que puedes usar para crear un archivo de R Markdown con ejemplos básicos. Como R Markdown integara varias herramientas, entonces no es posible que la ayuda esté autocontenida en RStudio. Esto significa que gran parte de la ayuda no la podrás encontrar a través de ?. Hay mucha documentación en línea y un recurso es particularmente útil son los cheatsheets de RStudio, que están disponibles en http://rstudio.com/cheatsheets. 2.6 Proyectos de RStudio Los proyectos de RStudio son útiles para mantener juntos todos los archivos asociados a un análisis (o proyecto) específico: datos de entrada, scripts de R, resultados, gráficas, datos de salida. Ésta es una práctica limpia y ordenada de trabajar y RStudio tiene soporte integrado para esto a través de los proyectos. Hagamos un proyecto. Para esto debes presionar File &gt; New Project, luego: Puedes cerrar el proyecto y después hacer doble click en el archivo .Rproj para volver a abrir el proyecto. Observa que regresas a donde estabas, en el mismo directorio de trabajo, con el mismo historial de comandos, y todos los archivos en los que estaba trabajando siguen abiertos. En resumen, los proyectos de RStudio te brindan un flujo de trabajo sólido que te servirá en el futuro: Creas un proyecto de RStudio para cada proyecto de análisis de datos. Mantienes los archivos de datos ahí mismo para después cargarlos en un script. Mantienes tus scripts organizados en el mismo directorio, y los puedes encontrar fácilmente para editarlos y ejecutarlos. Puedes guardar ahí mismo las salidas del código, como gráficas y datos limpios. Solamente utilizas rutas relativas, no absolutas. Todo lo que necesitas está en un solo lugar y separado de los demás proyectos en los que estés trabajando. 2.7 Otros aspectos importantes de R 2.7.1 Valores faltantes En R los datos faltantes se expresan como NA. La función is.na() regresa un vector lógico sobre los valores que son o no NA. is.na(c(4, 2, NA)) #&gt; [1] FALSE FALSE TRUE El default de R es propagar los valores faltantes, esto es, si se desconoce el valor de una de las componentes de un vector, también se desconoce la suma del mismo, en general, cualquier operación. sum(c(4, 2, NA)) #&gt; [1] NA mean(c(4, 2, NA)) #&gt; [1] NA 3 &gt; NA #&gt; [1] NA (NA == NA) #&gt; [1] NA Sin embargo, muchas funciones tienen un argumento na.rm para removerlos. sum(c(4, 2, NA), na.rm = T) #&gt; [1] 6 mean(c(4, 2, NA), na.rm = T) #&gt; [1] 3 2.7.2 Funciones Todo lo que sucede en R es una función. R es un lenguaje de programación funcional. Es decir, proporciona muchas herramientas para la creación y manipulación de funciones. En R las funciones, al igual que los vectores, se pueden asignar a variables, guardarlas en listas, usarlas como argumentos en otras funciones, crearlas dentro de otras funciones, e incluso regresar como resultado de una función más funciones. Una caja negra Una función puede verse como una caja negra que realiza un proceso o serie de instrucciones condicionadas a un valor de entrada, y cuyo resultado es un valor de salida. En R existen algunas funciones pre cargadas que ya hemos usado, por ejemplo, .la función mean(). input &lt;- c(1:5) output &lt;- mean( input ) output #&gt; [1] 3 Sin embargo, también es posible escribir nuestras propias funciones. Escibir una función En R es posible escribir funciones y es muy recomendable para dar soluciones a problemas simples. Existen ocasiones en las que al programar copias y pegas cierto código varias veces para una meta en especial. En ese momento, es necesario pasar el código a una función. Una función soluciona un problema en particular. La función function() nos permite crear funciones con la siguiente estructura: my_fun &lt;- function( arg1 ){ body return() } En general, esta estructura se respeta en las funciones predeterminadas de R. Creamos una función que sume uno a cualquier número. suma_uno_fun &lt;- function( x ){ y = x + 1 return(y) } Aplicamos la función: suma_uno_fun(5) #&gt; [1] 6 Podemos ver que en nuestra sesión ya existe la función con la función ls(). ls() #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;bool&quot; &quot;c&quot; #&gt; [5] &quot;fruits&quot; &quot;fruits.fac&quot; &quot;input&quot; &quot;iris&quot; #&gt; [9] &quot;lista&quot; &quot;output&quot; &quot;suma_uno_fun&quot; &quot;tabla&quot; #&gt; [13] &quot;tabla.color&quot; &quot;tenweeks&quot; &quot;x&quot; Esta función en lista los objetos existente en la sesión actual. Argumentos de funciones En R los argumentos de las funciones pueden llamarse por posición o nombre. Consideremos la siguiente función en la que se eleva un numero a un exponente determinado. potencia_fun &lt;- function(base, exponente){ base^exponente } Los argumentos pueden indicarse por posición: potencia_fun(2, 3) #&gt; [1] 8 O bien por nombre: potencia_fun(exponente = 2, base = 3) #&gt; [1] 9 Argumentos predeterminados En una función es posible asignar valores predeterminados a los argumentos. Por ejemplo, modificamos la función para asignar un valor predeterminado del exponente. potencia_default_fun &lt;- function(base, exponente = 2){ base^exponente } Al llamar la función, no es necesario definir un valor para el exponente y en automático tomará el valor exponente = 2. potencia_default_fun(2) #&gt; [1] 4 Argumentos nulos Una función puede no tener argumentos y simplemente correr un proceso. En este caso, usaremos la función sample() que elige una muestra aleatoria de tamaño 1 de un vector de 1 a 6 imitando un dado dentro la la función lanza_dado(). lanza_dado &lt;- function() { numero &lt;- sample(1:6, size = 1) numero } Ahora tiraremos dos veces los dados. Primer lanzamiento: lanza_dado() #&gt; [1] 5 Segundo lanzamiento: lanza_dado() #&gt; [1] 5 Alcance de la función Es importante mencionar que las variables que son definidas dentro de la función no son accesibles fuera de la función. Es decir, las funciones en R tienen un ambiente local. Por ejemplo, al correr la siguiente función e intentar imprimir el objeto x regresa un error. xs_fun &lt;- function(a){ x &lt;- 2 a*x } xs_fun(2) #&gt; [1] 4 # print(x) La función crea un ambiente nuevo dentro de la misma, en caso de no encontrar el valor de la variable en el ambiente local, sube un nivel. Este nuevo nivel puede ser el ambiente global. Por ejemplo: y &lt;- 10 ys_fun &lt;- function(a){ a*y } ys_fun(2) #&gt; [1] 20 Si la función está contenida en otra función, primero buscará en el ambiente local, después en el ambiente local de la función superior y luego en el ambiente global. Por ejemplo: y &lt;- 10 mas_uno_fun &lt;- function(a){ c &lt;- 1 y &lt;- 1 ys_add_fun &lt;- function(a){ a*y + c } ys_add_fun(a) } Si llamamos la función con un valor a = 2 al igual que en el ejemplo anterior, ¿por qué da el siguiente resultado y no 21 o 20? mas_uno_fun(a = 2) #&gt; [1] 3 Funciones para funciones O bien funciones para entender las partes de la función. body() body(suma_uno_fun) #&gt; { #&gt; y = x + 1 #&gt; return(y) #&gt; } args() args(mean.default) #&gt; function (x, trim = 0, na.rm = FALSE, ...) #&gt; NULL if() Una función que se usa al programar funciones es if() que permite agregar una condición. divide_fun &lt;- function(num, den){ if(den == 0){ return(&quot;Denominador es cero&quot;) }else{ return(num/den) } } Al ejecutar la función y tener cero en el denominador imprime el string. divide_fun(10, 0) #&gt; [1] &quot;Denominador es cero&quot; Al no tener cero en el denominador la operación se ejecuta. divide_fun(10, 2) #&gt; [1] 5 Todas las operaciones en R son producto de la llamada a una función, esto incluye operaciones como +, operadores que controlan flujo como for, if y while, e incluso operadores para obtener subconjuntos como [ ] y $. x &lt;- 3 y &lt;- 4 `+`(x, y) #&gt; [1] 7 for (i in 1:2) print(i) #&gt; [1] 1 #&gt; [1] 2 `for`(i, 1:2, print(i)) #&gt; [1] 1 #&gt; [1] 2 Cuando llamamos a una función podemos especificar los argumentos con base en la posición, el nombre completo o el nombre parcial: f &lt;- function(abcdef, bcde1, bcde2) { list(a = abcdef, b1 = bcde1, b2 = bcde2) } str(f(1, 2, 3)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 str(f(2, 3, abcdef = 1)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 Podemos abreviar el nombre de los argumentos: str(f(2, 3, a = 1)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 Siempre y cuando la abreviación no sea ambigua: #f(1, 3, b = 1) Los argumentos de las funciones en R se evaluan conforme se necesitan: f &lt;- function(a, b){ a ^ 2 } f(2) #&gt; [1] 4 La función anterior nunca utiliza el argumento b, de tal manera que f(2) no produce ningún error. 2.7.3 Funcionales La familia de funciones apply pertenece a la librería base en R y facilitan la manipulación de datos de forma repetitiva. Las funciones de esta familia son: apply(), lapply(), sapply(), vapply(), mapply(), rapply(), y tapply(). La estructura de los datos de entrada y el formato del resultado o salida determinarán cual función usar. En este taller solo se verán las primeras tres funciones. apply() Esta es la función que manipula arreglos homogéneos, en particular, se revisa el caso de matrices que son arreglos de dos dimensiones. La función tiene los siguientes argumentos: apply(X, MARGIN, FUN, ...) X representa el arreglo de dos dimensiones. MARGIN representa la dimensión sobre la que se va a resumir la información. Donde 1 = renglon o primera dimensión y 2 = columna o segunda dimensión. FUN representa la función que resume la información. Tomemos la siguiente matriz de simulaciones: set.seed(1) mat_norm &lt;- matrix(rnorm(24, mean = 0, sd = 1), nrow = 4, ncol = 6) mat_norm #&gt; [,1] [,2] [,3] [,4] [,5] [,6] #&gt; [1,] -0.626 0.330 0.576 -0.6212 -0.0162 0.9190 #&gt; [2,] 0.184 -0.820 -0.305 -2.2147 0.9438 0.7821 #&gt; [3,] -0.836 0.487 1.512 1.1249 0.8212 0.0746 #&gt; [4,] 1.595 0.738 0.390 -0.0449 0.5939 -1.9894 Deseamos obtener la suma de cada columna de la matriz. El primer método, quizá el mas intuitivo en este momento, es obtener cada elemento o columna, aplicar la función a cada elemento y concatenar: prom_col_m1 &lt;- c(sum(mat_norm[, 1]), sum(mat_norm[, 2]), sum(mat_norm[, 3]), sum(mat_norm[, 4]), sum(mat_norm[, 5]), sum(mat_norm[, 6])) prom_col_m1 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Segundo método: prom_col_m2 &lt;- vector( length = ncol(mat_norm)) for(j in 1:ncol(mat_norm)){ prom_col_m2[j] &lt;- sum(mat_norm[, j]) } prom_col_m2 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Tercer método: prom_col_m3 &lt;- apply(X = mat_norm, MARGIN = 2, FUN = sum) prom_col_m3 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Cuarto método: prom_col_m4 &lt;- colSums(mat_norm) prom_col_m4 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Ahora, para obtener la suma por renglón usando el tercer método de la función apply(), únicamente es necesario cambiar la dimensión sobre la que voy a resumir con el argumento MARGIN = 1. prom_row_m3 &lt;- apply(mat_norm, 1, sum) prom_row_m3 #&gt; [1] 0.56 -1.43 3.18 1.28 Esto es equivalente al primer método que usamos: prom_row_m1 &lt;- c(sum(mat_norm[1, ]), sum(mat_norm[2, ]), sum(mat_norm[3, ]), sum(mat_norm[4, ])) prom_row_m1 #&gt; [1] 0.56 -1.43 3.18 1.28 La ventaja de usar la función apply() es que se puede usar cualquier función. Por ejemplo, obtener la desviación estándar. apply(mat_norm, 1, sd) #&gt; [1] 0.634 1.172 0.834 1.207 O bien, una crear una función propia (definida por el usuario) con la función function(): cv_vec_m3 &lt;- apply(mat_norm, 1, function(reng){ cv &lt;- mean(reng)/sd(reng) return(cv) }) cv_vec_m3 #&gt; [1] 0.147 -0.204 0.636 0.177 Funciones Anónimas: A este tipo de funciones se les llama funciones anónimas porque no se nombran ni guardan en el ambiente de R y únicamente funcionan dentro del comando que las llama. lapply() La función lapply() aplica una función sobre una lista o un vector y regresa el resultado en otra lista. Vector de ciudades: ciudades_vec &lt;- c(&quot;Aguascalientes&quot;, &quot;Monterrey&quot;, &quot;Guadalajara&quot;, &quot;México&quot;) ciudades_vec #&gt; [1] &quot;Aguascalientes&quot; &quot;Monterrey&quot; &quot;Guadalajara&quot; &quot;México&quot; res_nchar_l &lt;- lapply(ciudades_vec, nchar) res_nchar_l #&gt; [[1]] #&gt; [1] 14 #&gt; #&gt; [[2]] #&gt; [1] 9 #&gt; #&gt; [[3]] #&gt; [1] 11 #&gt; #&gt; [[4]] #&gt; [1] 6 Esta función permite implementar funciones que regresen objetos de diferentes tipos, porque la listas permiten almacenar contenido heterogéneo. La función lapply() permite incluir argumentos de las funciones que implementa. Estos argumentos se incluyen dentro de lapply() después de la función a implementar. Por ejemplo, usamos la función potencia que se creó antes. potencia_fun &lt;- function(base, exponente){ base^exponente } El objetivo es aplicar a cada elemento de la siguiente lista la función potencia y elevarlo al cubo. nums_lista &lt;- list(1, 3, 4) nums_lista #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 3 #&gt; #&gt; [[3]] #&gt; [1] 4 En la función lapply() se agrega el argumento exponente = 3 como último argumento. potencia_lista &lt;- lapply(nums_lista, potencia_fun, exponente = 3) potencia_lista #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 27 #&gt; #&gt; [[3]] #&gt; [1] 64 Una forma de reducir la lista obtenida a un vector es con la función unlist() que vimos antes. unlist(potencia_lista) #&gt; [1] 1 27 64 sapply() La función sapply() es muy similar a lapply(). La única diferencia es la s que surge de simplified apply. Al igual que lapply() aplica una función sobre una lista o un vector pero simplifica el resultado en un arreglo. res_nchar_s &lt;- sapply(ciudades_vec, nchar) res_nchar_s #&gt; Aguascalientes Monterrey Guadalajara México #&gt; 14 9 11 6 Esta función es peligrosa ya que únicamente simplifica la estructura del resultado cuando es posible, de lo contrario, regresará una lista igual que lapply(). 2.7.3.1 Funciones map Un problema con sapply() y lapply() es que puedes no tener control sobre el tipo que obtienes y esto es importante si el código está dentro de una función: x1 &lt;- list( c(0.27, 0.37, 0.57, 0.91, 0.20), c(0.90, 0.94, 0.66, 0.63, 0.06), c(0.21, 0.18, 0.69, 0.38, 0.77) ) x2 &lt;- list( c(0.50, 0.72, 0.99, 0.38, 0.78), c(0.93, 0.21, 0.65, 0.13, 0.27), c(0.39, 0.01, 0.38, 0.87, 0.34) ) threshold &lt;- function(x, cutoff = 0.8) x[x &gt; cutoff] x1 %&gt;% sapply(threshold) %&gt;% str() #&gt; List of 3 #&gt; $ : num 0.91 #&gt; $ : num [1:2] 0.9 0.94 #&gt; $ : num(0) x2 %&gt;% sapply(threshold) %&gt;% str() #&gt; num [1:3] 0.99 0.93 0.87 Las funciones del paquete purrr son útiles porque hacen que los loops sobre vectores sean sencillos: tienen nombres similares, y tienen argumentos consistentes. Hay una función para cada tipo de salida: map() crea una lista. map_lgl() crea un vector lógico. map_int() crea un vector de enteros. map_dbl() crea un vector numérico. map_chr() crea un vector de tipo caracter. Cada función toma un vector como entrada, aplica una función a cada elemento y luego devuelve un nuevo vector que tiene la misma longitud (y tiene los mismos nombres) que la entrada, y el tipo de vector está determinado por el sufijo de la función. Por ejemplo, supongamos que deseamos calcular el número de valores únicos en cada columna de iris. Utilizamos, entonces, la función map_int porque deseamos obtener como resultado un vector de enteros donde cada entero represente el número de valores únicos en cada columna: map_int(.x = iris, .f = function(x){length(unique(x)) }) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 35 23 43 22 3 #&gt; Species_ord #&gt; 3 Para generar cuatro vectores cada uno de tamaño 5 de valores que provienen de una distribución normal con medias \\(\\mu=-10, 0, 10, 100\\) podemos utilizar la función map: mu &lt;- list(-10, 0, 10, 100) mu %&gt;% map(rnorm, n = 5) %&gt;% str() #&gt; List of 4 #&gt; $ : num [1:5] -9.38 -10.06 -10.16 -11.47 -10.48 #&gt; $ : num [1:5] 0.4179 1.3587 -0.1028 0.3877 -0.0538 #&gt; $ : num [1:5] 8.62 9.59 9.61 9.94 11.1 #&gt; $ : num [1:5] 100.8 99.8 99.7 100.7 100.6 Podemos usar map2() para iterar sobre dos vectores en paralelo: sigma &lt;- list(1, 5, 10, 100) map2(mu, sigma, rnorm, n = 5) %&gt;% str() #&gt; List of 4 #&gt; $ : num [1:5] -10.69 -10.71 -9.64 -9.23 -10.11 #&gt; $ : num [1:5] 4.41 1.99 -3.06 1.71 -5.65 #&gt; $ : num [1:5] 24.33 29.804 6.328 -0.441 15.697 #&gt; $ : num [1:5] 86.5 340.2 96.1 169 102.8 Las funciones de map también preservan nombres: z &lt;- list(x = 1:3, y = 4:5) map_int(z, length) #&gt; x y #&gt; 3 2 Hay algunos shortcuts que podemos usar con .f para guardar un poco de tipeo. Supongamos que queremos ajustar un modelo lineal a cada subconjunto en un conjunto de datos. Pensemos en los datos de mtcars divididos en tres subconjuntos (uno para cada valor de cilindro) y se ajusta el mismo modelo lineal en cada subconjunto (millas por galón mpg vs peso wt): models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~lm(mpg ~ wt, data = .)) Con el símbolo ~ se puede declarar una función anónima con un argumento al cual se hace referencia después utilizando el .. Cuando vemos muchos modelos, generalmente deseamos extraer un resumen estadístico como la \\(R^2\\). Para hacer eso necesitamos ejecutar primero summary() y luego extraer el componente llamado r.squared. Podríamos hacer eso usando la abreviatura de funciones anónimas: models %&gt;% map(summary) %&gt;% map_dbl(~.$r.squared) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 Pero purrr tiene un shortcut para extraer estos componentes en una cadena: models %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 También se puede usar un número entero para seleccionar elementos por posición: x &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9)) x %&gt;% map_dbl(2) #&gt; [1] 2 5 8 La función pmap() recibe una lista de argumentos para aplicarlos a una función: params &lt;- tribble( ~mean, ~sd, ~n, 5, 1, 1, 10, 5, 3, -3, 10, 5 ) params %&gt;% pmap(rnorm) #&gt; [[1]] #&gt; [1] 4.26 #&gt; #&gt; [[2]] #&gt; [1] 10.944 0.975 17.328 #&gt; #&gt; [[3]] #&gt; [1] -1.47 18.73 1.76 -10.10 3.11 Hay un paso más en la complejidad: además de variar los argumentos para una función, también puedes variar la función misma y para esto se usa la función invoke_map(): sim &lt;- tribble( ~f, ~params, &quot;runif&quot;, list(min = -1, max = 1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim %&gt;% mutate(sim = invoke_map(f, params, n = 10)) %&gt;% str() #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3 obs. of 3 variables: #&gt; $ f : chr &quot;runif&quot; &quot;rnorm&quot; &quot;rpois&quot; #&gt; $ params:List of 3 #&gt; ..$ :List of 2 #&gt; .. ..$ min: num -1 #&gt; .. ..$ max: num 1 #&gt; ..$ :List of 1 #&gt; .. ..$ sd: num 5 #&gt; ..$ :List of 1 #&gt; .. ..$ lambda: num 10 #&gt; $ sim :List of 3 #&gt; ..$ : num -0.65 0.493 -0.79 0.729 0.229 ... #&gt; ..$ : num 0.372 -2.948 -2.843 -0.676 5.89 ... #&gt; ..$ : int 11 10 8 13 13 12 15 11 5 10 2.7.4 Rendimiento en R “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.” -Donald Knuth Diseña primero, luego optimiza. La optimización del código es un proceso iterativo: Encuentra el cuello de botella (más importante). Intenta eliminarlo (no siempre se puede). Repite hasta que tu código sea lo suficientemente rápido. Diagnosticar Una vez que tienes código que se puede leer y funciona, el perfilamiento (profiling) del código es un método sistemático que nos permite conocer cuanto tiempo se esta usando en diferentes partes del programa. Comenzaremos con la función system.time (no es perfilamiento aún), esta calcula el timepo en segundos que toma ejecutar una expresión (si hay un error, regresa el tiempo hasta que ocurre el error): library(Lahman) Batting %&gt;% sample_n(10) %&gt;% knitr::kable() playerID yearID stint teamID lgID G AB R H X2B X3B HR RBI SB CS BB SO IBB HBP SH SF GIDP 86418 gilesma01 2005 1 ATL NL 152 577 104 168 45 4 15 63 16 3 64 108 1 5 4 4 13 32692 weathro01 1946 1 NYA AL 2 2 0 1 0 0 0 0 0 0 0 0 NA 0 0 NA 0 80489 tynerja01 2000 2 TBA AL 37 83 6 20 2 0 0 8 6 1 4 12 0 1 5 1 1 27504 stephwa01 1937 1 PHI NL 10 23 1 6 0 0 0 2 0 NA 2 3 NA 0 0 NA 1 22480 cohenan01 1928 1 NY1 NL 129 504 64 138 24 7 9 59 3 NA 31 17 NA 2 11 NA NA 53133 lemonch01 1975 1 CHA AL 9 35 2 9 2 0 0 1 1 0 2 6 0 0 1 0 0 27651 chapmsa01 1938 1 PHA AL 114 406 60 105 17 7 17 63 3 4 55 94 NA 4 1 NA NA 18626 weavebu01 1920 1 CHA AL 151 629 102 208 34 8 2 75 19 17 28 23 NA 6 27 NA NA 53314 perezto01 1975 1 CIN NL 137 511 74 144 28 3 20 109 1 2 54 101 6 3 0 6 12 57859 littljo01 1980 1 SLN NL 52 11 1 0 0 0 0 0 0 0 0 3 0 0 1 0 0 system.time(lm(R ~ AB + teamID, Batting)) #&gt; user system elapsed #&gt; 2.740 0.076 2.816 user time: Tiempo usado por el CPU(s) para evaluar esta expresión, tiempo que experimenta la computadora. elapsed time: tiempo en el reloj, tiempo que experimenta la persona. El tiempo de usuario (user) usualmente es menor que el tiempo transcurrido: system.time(readLines(&quot;http://www.jhsph.edu&quot;)) #&gt; user system elapsed #&gt; 0.024 0.000 0.654 library(parallel) system.time(mclapply(2000:2006, function(x){ sub &lt;- subset(Batting, yearID == x) lm(R ~ AB, sub) }, mc.cores = 5)) #&gt; user system elapsed #&gt; 0.064 0.072 0.087 Comparemos la velocidad de dplyr con funciones que se encuentran en R estándar y plyr. # dplyr dplyr_st &lt;- system.time({ Batting %&gt;% group_by(playerID) %&gt;% summarise(total = sum(R, na.rm = TRUE), n = n()) %&gt;% dplyr::arrange(desc(total)) }) # plyr plyr_st &lt;- system.time({ Batting %&gt;% plyr::ddply(&quot;playerID&quot;, plyr::summarise, total = sum(R, na.rm = TRUE), n = length(R)) %&gt;% dplyr::arrange(-total) }) # estándar lento est_l_st &lt;- system.time({ players &lt;- unique(Batting$playerID) n_players &lt;- length(players) total &lt;- rep(NA, n_players) n &lt;- rep(NA, n_players) for(i in 1:n_players){ sub_batting &lt;- Batting[Batting$playerID == players[i], ] total[i] &lt;- sum(sub_batting$R, na.rm = TRUE) n[i] &lt;- nrow(sub_batting) } batting_2 &lt;- data.frame(playerID = players, total = total, n = n) batting_2[order(batting_2$total, decreasing = TRUE), ] }) # estándar rápido est_r_st &lt;- system.time({ batting_2 &lt;- aggregate(. ~ playerID, data = Batting[, c(&quot;playerID&quot;, &quot;R&quot;)], sum) batting_ord &lt;- batting_2[order(batting_2$R, decreasing = TRUE), ] }) dplyr_st #&gt; user system elapsed #&gt; 0.124 0.000 0.126 plyr_st #&gt; user system elapsed #&gt; 5.928 0.004 5.932 est_l_st #&gt; user system elapsed #&gt; 44.83 1.66 46.49 est_r_st #&gt; user system elapsed #&gt; 0.536 0.016 0.555 La función system.time supone que sabes donde buscar, es decir, que expresiones debes evaluar, una función que puede ser más útil cuando uno desconoce cuál es la función que alenta un programa es Rprof(). Rprof es un perfilador de muestreo que registra cambios en la pila de funciones, funciona tomando muestras a intervalos regulares y tabula cuánto tiempo se lleva en cada función. Rprof(&quot;out/lm_rprof.out&quot;, interval = 0.015, line.profiling = TRUE) mod &lt;- lm(R ~ AB + teamID, Batting) Rprof(NULL) Usamos la función `summaryRprof para tabular las salidas de Rprof y calcular cuánto tiempo se toma en cada función. summaryRprof(&quot;out/lm_rprof.out&quot;) #&gt; $by.self #&gt; self.time self.pct total.time total.pct #&gt; &quot;lm.fit&quot; 5.04 95.18 5.05 95.47 #&gt; &quot;.External2&quot; 0.16 3.12 0.20 3.68 #&gt; &quot;as.character&quot; 0.04 0.85 0.04 0.85 #&gt; &quot;anyDuplicated.default&quot; 0.03 0.57 0.03 0.57 #&gt; &quot;c&quot; 0.02 0.28 0.02 0.28 #&gt; #&gt; $by.total #&gt; total.time total.pct self.time self.pct #&gt; &quot;&lt;Anonymous&gt;&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;block_exec&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;call_block&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;do.call&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;eval.parent&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;eval&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;evaluate_call&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;evaluate&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;handle&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;in_dir&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;knitr::knit&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;lm&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;local&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;process_file&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;process_group.block&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;process_group&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;timing_fn&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;withCallingHandlers&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;withVisible&quot; 5.29 100.00 0.00 0.00 #&gt; &quot;lm.fit&quot; 5.05 95.47 5.04 95.18 #&gt; &quot;.External2&quot; 0.20 3.68 0.16 3.12 #&gt; &quot;model.matrix.default&quot; 0.16 3.12 0.00 0.00 #&gt; &quot;model.matrix&quot; 0.16 3.12 0.00 0.00 #&gt; &quot;as.character&quot; 0.04 0.85 0.04 0.85 #&gt; &quot;model.response&quot; 0.04 0.85 0.00 0.00 #&gt; &quot;anyDuplicated.default&quot; 0.03 0.57 0.03 0.57 #&gt; &quot;[.data.frame&quot; 0.03 0.57 0.00 0.00 #&gt; &quot;[&quot; 0.03 0.57 0.00 0.00 #&gt; &quot;anyDuplicated&quot; 0.03 0.57 0.00 0.00 #&gt; &quot;model.frame.default&quot; 0.03 0.57 0.00 0.00 #&gt; &quot;na.omit.data.frame&quot; 0.03 0.57 0.00 0.00 #&gt; &quot;na.omit&quot; 0.03 0.57 0.00 0.00 #&gt; &quot;stats::model.frame&quot; 0.03 0.57 0.00 0.00 #&gt; &quot;c&quot; 0.02 0.28 0.02 0.28 #&gt; #&gt; $sample.interval #&gt; [1] 0.015 #&gt; #&gt; $sampling.time #&gt; [1] 5.29 Hay dos métodos para normalizar los datos de Rprof: by.total divide el tiempo que se toma en cada función entre el tiempo total en correr. by.self similar a by.total pero primero resta el tiempo que se toman las funciones en la cima de la pila. Rprof(&quot;out/plyr_rprof.out&quot;) Batting %&gt;% plyr::ddply(&quot;playerID&quot;, plyr::summarise, total = sum(R, na.rm = TRUE), n = length(R)) %&gt;% plyr::arrange(-total) %&gt;% head() Rprof(NULL) summaryRprof(&quot;out/plyr_rprof.out&quot;)$by.self[1:10, ] # Rprof(&quot;out/slow_rprof.out&quot;) # players &lt;- unique(batting$playerID) # n_players &lt;- length(players) # total &lt;- rep(NA, n_players) # n &lt;- rep(NA, n_players) # for(i in 1:n_players){ # sub_batting &lt;- batting[batting$playerID == players[i], ] # total[i] &lt;- sum(sub_batting$R) # n[i] &lt;- nrow(sub_batting) # } # batting_2 &lt;- data.frame(playerID = players, total = total, n = n) # batting_2[order(batting_2$total, decreasing = TRUE), ] # Rprof(NULL) summaryRprof(&quot;out/slow_rprof.out&quot;)$by.self[1:10, ] #&gt; self.time self.pct total.time total.pct #&gt; &quot;[.data.frame&quot; 45.50 54.74 82.74 99.54 #&gt; &quot;==&quot; 24.78 29.81 24.78 29.81 #&gt; &quot;attr&quot; 6.70 8.06 6.70 8.06 #&gt; &quot;NextMethod&quot; 3.42 4.11 3.44 4.14 #&gt; &quot;[[&quot; 1.32 1.59 2.12 2.55 #&gt; &quot;[[.data.frame&quot; 0.22 0.26 0.80 0.96 #&gt; &quot;&lt;Anonymous&gt;&quot; 0.20 0.24 0.46 0.55 #&gt; &quot;all&quot; 0.10 0.12 0.10 0.12 #&gt; &quot;sys.call&quot; 0.10 0.12 0.10 0.12 #&gt; &quot;%in%&quot; 0.08 0.10 0.22 0.26 Estrategias para mejorar desempeño Utilizar apropiadamente funciones de R, o funciones de paquetes que muchas veces están mejor escritas de lo que nosotros podríamos hacer. Hacer lo menos posible. Usar funciones vectorizadas en R (casi siempre). No hacer crecer objetos (es preferible definir su tamaño antes de operar en ellos). Paralelizar. La más simple y muchas veces la más barata: conseguie una máquina más grande (por ejemplo Amazon Web Services). 1 Utilizar apropiadamente funciones de R Si el cuello de botella es la función de un paquete vale la pena buscar alternativas, CRAN task views es un buen lugar para buscar. 2 Hacer lo menos posible Utiliza funciones más específicas, por ejemplo: rowSums(), colSums(), rowMeans() y colMeans() son más rápidas que las invocaciones equivalentes de apply(). Si quieres checar si un vector contiene un valor any(x == 10) es más veloz que 10 %in% x, esto es porque examinar igualdad es más sencillo que examinar inclusión en un conjunto. Este conocimiento requiere que conozcas alternativas, para ello debes construir tu vocabulario, puedes comenzar por lo básico e ir incrementando conforme lees código. Otro caso es cuando las funciones son más rápidas cunado les das más información del problema, por ejemplo: read.csv(), especificar las clases de las columnas con colClasses. factor() especifica los niveles con el argumento levels. 3.1 Usar funciones vectorizadas en R Es común escuchar que en R vectorizar es conveniente, el enfoque vectorizado va más allá que evitar ciclos for: Pensar en objetos, en lugar de enfocarse en las compoentes de un vector, se piensa únicamente en el vector completo. Los ciclos en las funciones vectorizadas de R están escritos en C, lo que los hace más veloces. Las funciones vectorizadas programadas en R pueden mejorar la interfaz de una función pero no necesariamente mejorar el desempeño. Usar vectorización para desempeño implica encontrar funciones de R implementadas en C. Al igual que en el punto anterior, vectorizar requiere encontrar las funciones apropiadas, algunos ejemplos incluyen: _rowSums(), colSums(), rowMeans() y colMeans(). 3.2 Evitar copias Otro aspecto importante es que generalmente conviene asignar objetos en lugar de hacerlos crecer (es más eficiente asignar toda la memoria necesaria antes del cálculo que asignarla sucesivamente). Esto es porque cuando se usan instrucciones para crear un objeto más grande (e.g. append(), cbind(), c(), rbind()) R debe primero asignar espacio a un nuevo objeto y luego copiar al nuevo lugar. Para leer más sobre esto The R Inferno es una buena referencia. Veamos unos ejemplos de vectorización y de asignar objetos. aciertos &lt;- FALSE system.time( for (i in 1:1e+05) { if (runif(1) &lt; 0.3) aciertos[i] &lt;- TRUE }) #&gt; user system elapsed #&gt; 0.232 0.012 0.245 aciertos &lt;- rep(FALSE, 1e+06) system.time( for (i in 1:1e+05) { if (runif(1) &lt; 0.3) aciertos[i] &lt;- TRUE }) #&gt; user system elapsed #&gt; 0.224 0.000 0.224 Usando rbind: crecer_rbind &lt;- function(){ mi.df &lt;- data.frame(a = character(0), b = numeric(0)) for(i in 1:1e3) { mi.df &lt;- rbind(mi.df, data.frame(a = sample(letters, 1), b = runif(1))) } mi.df } system.time(mi.df.1 &lt;- crecer_rbind()) #&gt; user system elapsed #&gt; 0.704 0.000 0.703 Si definimos el tamaño del data.frame obtenemos mejoras: crecer_rbind_2 &lt;- function() { mi.df &lt;- data.frame(a = rep(NA, 1000), b = rep(NA, 1000)) for (i in 1:1000) { mi.df$a[i] &lt;- sample(letters, 1) mi.df$b[i] &lt;- runif(1) } mi.df } system.time(mi.df.1 &lt;- crecer_rbind_2()) #&gt; user system elapsed #&gt; 0.056 0.000 0.057 Finalmente, veamos un enfoque totalmente vectorizado porcolumna_df &lt;- function(){ a &lt;- sample(letters, 1000, replace = TRUE) b &lt;- runif(1000) mi.df &lt;- data.frame(a = a, b = b) mi.df } system.time(mi.df.2 &lt;- porcolumna_df()) #&gt; user system elapsed #&gt; 0.000 0.000 0.001 A pesar de que aumentamos la velocidad conforme aumentamos el nivel de vectorización, este incremento conlleva un costo en memoria. Si comparamos la versión mas lenta con la más rápida, en la última debemos asignar a, b y mi.df. Entonces, no siempre es mejor vectorizar, pues si consumimos la memoria, entonces la versión vectorizada puede enfrentarse al problema de uso de memoria en disco, que tiene aun más grandes penalizaciones en el desempeño que los ciclos que hemos visto. 4 Paralelizar Paralelizar usa varios cores para trabajar de manera simultánea en varias secciones de un problema, no reduce el tiempo computacional pero incrementa el tiempo del usuario pues aprovecha los recursos. Como referencia está Parallel Computing for Data Science de Norm Matloff. 2.8 Tarea Considerando la lista siguiente, cdmx_list &lt;- list( pop = 8918653, delegaciones = c(&quot;Alvaro Obregón&quot;, &quot;Azcapotzalco&quot; ,&quot;Benito Juárez&quot; , &quot;Coyoacán&quot; ,&quot;Cuajimalpa de Morelos&quot; ,&quot;Cuauhtémoc&quot; , &quot;Gustavo A. Madero&quot; , &quot;Iztacalco&quot; ,&quot;Iztapalapa&quot; , &quot;Magdalena Contreras&quot; ,&quot;Miguel Hidalgo&quot; ,&quot;Milpa Alta&quot; , &quot;Tláhuac&quot; ,&quot;Tlalpan&quot; , &quot;Venustiano Carranza&quot; ,&quot;Xochimilco&quot;), capital = TRUE ) obtén la clase de cada elemento con la función lapply(). lapply( , class) La siguiente función extrae la letra de menor posición y mayor posición en orden alfabético. min_max_fun &lt;- function(nombre){ nombre_sinespacios &lt;- gsub(&quot; &quot;, &quot;&quot;, nombre) letras &lt;- strsplit(nombre_sinespacios, split = &quot;&quot;)[[1]] c(minimo = min(letras), maximo = max(letras)) } Es decir, si incluimos las letras abcz la letra mínima es a y la máxima es z. min_max_fun(&quot;abcz&quot;) #&gt; minimo maximo #&gt; &quot;a&quot; &quot;z&quot; El siguiente vector incluye el nombre de las 16 delegaciones de la Ciudad de México. delegaciones &lt;- c(&quot;Alvaro Obregon&quot;, &quot;Azcapotzalco&quot; ,&quot;Benito Juarez&quot; , &quot;Coyoacan&quot; ,&quot;Cuajimalpa de Morelos&quot; ,&quot;Cuauhtemoc&quot; , &quot;Gustavo Madero&quot; , &quot;Iztacalco&quot; ,&quot;Iztapalapa&quot; , &quot;Magdalena Contreras&quot; ,&quot;Miguel Hidalgo&quot; ,&quot;Milpa Alta&quot; , &quot;Tlahuac&quot; ,&quot;Tlalpan&quot; , &quot;Venustiano Carranza&quot; ,&quot;Xochimilco&quot;) Aplica la función sapply() para obtener un arreglo con la letra máxima y mínima de cada nombre. sapply(, ) El siguiente vector incluye el precio de la gasolina en diferentes estados del país en julio de 2017. gas_cdmx &lt;- c(15.82, 15.77, 15.83, 15.23, 14.95, 15.42, 15.55) gas_cdmx #&gt; [1] 15.8 15.8 15.8 15.2 14.9 15.4 15.6 Crea una función que convierta el precio a dolares suponiendo que un dolar equivale a 17.76 pesos. conv_fun &lt;- function(precio){ /17.76 return() } Usando la función lapply() convierte el precio de la gasolina a dolares. gas_cdmx_usd_lista &lt;- lapply(, conv_fun) Usa la función unlist() para convertir la lista a un vector. gas_cdmx_usd &lt;- unlist() print(gas_cdmx_usd) Estadísticos importantes estadisticos &lt;- c(&quot;GAUSS:1777&quot;, &quot;BAYES:1702&quot;, &quot;FISHER:1890&quot;, &quot;PEARSON:1857&quot;) split_estadisticos &lt;- strsplit(estadisticos, split = &quot;:&quot;) split_estadisticos #&gt; [[1]] #&gt; [1] &quot;GAUSS&quot; &quot;1777&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;BAYES&quot; &quot;1702&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;FISHER&quot; &quot;1890&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;PEARSON&quot; &quot;1857&quot; Utiliza la función predefinida tolower() y lapply() para convertir a minúsculas cada letra de la lista split_estadisticos. split_lower &lt;- lapply( , ) print(split_lower) Usando el vector split_estadísticos del ejercicio anterior. str(split_estadisticos) #&gt; List of 4 #&gt; $ : chr [1:2] &quot;GAUSS&quot; &quot;1777&quot; #&gt; $ : chr [1:2] &quot;BAYES&quot; &quot;1702&quot; #&gt; $ : chr [1:2] &quot;FISHER&quot; &quot;1890&quot; #&gt; $ : chr [1:2] &quot;PEARSON&quot; &quot;1857&quot; Crea una función que regrese la primera posición. primera_pos_fun &lt;- function(lista){ } Crea una función que regrese la segunda posición. segunda_pos_fun &lt;- function(lista){ } Usando lapply() crea una lista con los nombres de los estadísticos y otra con la fecha de nacimiento. nombres &lt;- lapply() fechas &lt;- lapply() Usando una función anónima y el vector split_estadísticos en un solo lapply() o sapply() obtén un vector compuesto de la primera posición, es decir el nombre, en minúsculas. Tip: si usas lapply() recuerda usar la función unlist(). nombre_estadisticos &lt;- (split_estadisticos, function(elemento){ tolower() }) nombre_estadisticos En la siguiente lista se presenta el registro de temperatura de tres ciudades a las 07:00 am, 10:00 am, 01:00 pm, 04:00 pm y 07:00 pm. temp_lista &lt;- list( cdmx = c(13, 15, 19, 22, 20), guadalajara = c(18, 18, 22, 26, 27), tuxtla_gtz = c(22, 24, 29, 32, 28) ) str(temp_lista) #&gt; List of 3 #&gt; $ cdmx : num [1:5] 13 15 19 22 20 #&gt; $ guadalajara: num [1:5] 18 18 22 26 27 #&gt; $ tuxtla_gtz : num [1:5] 22 24 29 32 28 Completa la siguiente función que obtiene el promedio entre el valor mínimo y máximo registrados. promedio_extremos_fun &lt;- function(x) { ( min() + max() ) / 2 } Aplica la función a la lista y obtén la temperatura promedio de extremos para cada ciudad usando lapply() y sapply(). lapply(,) sapply(,) Crea una función en la que mientras la velocidad sea mayor a 50 km/hr se reduzca de la siguiente forma: Si es mayor a 80 km/hr se reducen 20 km/hr e imprime ¡Demasido rápido!. Si es menor o igual a 80km/hr se reducen únicamente 5 km/hr. velocidad_act &lt;- 140 while(velocidad_act &gt; ){ if(velocidad_act &gt; ){ print() velocidad_act &lt;- } if(velocidad_act &lt; ){ velocidad_act &lt;- } velocidad_act } Referencias "],
["manipulacion-y-visualizacion-de-datos.html", "Clase 3 Manipulación y visualización de datos 3.1 El principio de datos limpios 3.2 Limpieza de datos 3.3 Separa-aplica-combina 3.4 Muertes por armas de fuego en EUA 3.5 El Cuarteto de Anscombe 3.6 The Grammar of Graphics de Leland Wilkinson 3.7 ggplot 3.8 Un histograma de las muertes en Iraq 3.9 Inglehart–Welzel: un mapa cultural del mundo 3.10 Poniendo todo junto 3.11 Tarea", " Clase 3 Manipulación y visualización de datos .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } “Happy families are all alike; every unhappy family is unhappy in its own way.” — Leo Tolstoy “Tidy datasets are all alike; but every messy dataset is messy in its own way.” — Hadley Wickham Comencemos nuevamente cargando el paquete tidyverse: library(tidyverse) La visualización es una herramienta importante para generar información. Sin embargo, es muy raro obtener los datos exactamente en la forma en que se necesitan. Es común tener que crear nuevas variables o hacer resúmenes a partir de algunas variables, o tal vez sólo sea necesario cambiar el nombre de las variables o reordenar las observaciones con el fin de facilitar el análisis de datos. Pipeline La idea de pipeline intenta hacer el desarrollo de código más fácil, en menor tiempo, fácil de leerlo, y por lo tanto, más fácil mantenerlo. En el análisis de datos es común hacer varias operaciones y se vuelve difícil leer y entender el código. La dificultad radica en que usualmente los parámetros se asignan después del nombre de la función usando (). La forma en que esta idea logra hacer las cosas más faciles es con el operador forwad pipe %&gt;%que envía un valor a una expresión o función. Este cambio en el orden funciona como el parámetro que precede a la función es enviado (“piped”) a la función. Es decir, supongamos x es una valor y sea f una función, entonces, x %&gt;% f es igual a f(x). Por ejemplo, sea \\(f(x)\\) la función de probabilidad de la distribución normal con media \\(\\mu = 0\\) y desviación estándar \\(\\sigma = 1\\): \\[ f(x) = \\dfrac{ 1 }{\\sqrt{2\\pi}} e^{- \\frac{1}{2} x^2 } \\] f &lt;- function(x){ exp(-(x^2)/2)/sqrt(2*pi) } # Con el operador de pipe 0 %&gt;% f #&gt; [1] 0.399 que de forma tradicional se realiza: # Forma tradicional f(0) #&gt; [1] 0.399 En resumen %&gt;% funciona como se muestra en la siguiente figura: Nota: Se puede insertar el pipe %&gt;% utilizando: Cmd/Ctrl + Shift + M. ¿Qué hace el siguiente código? ¿Qué hace .? df &lt;- data_frame( x = runif(5), y = rnorm(5) ) df %&gt;% .$x df %&gt;% ggplot(data = ., aes(x = x, y = y)) + geom_point() Tibbles Tibbles son dataframes con algunas modificaciones que permitirán trabajar mejor con los paquetes de limpieza y manipulación de datos tidyr y dplyr. Una diferencia son los tipos de columnas que maneja: lgl: vectores de valores lógicos, vectores que contienen TRUE o FALSE. int: vectores de números enteros. dbl: vectores de números reales. chr: vectores de caracteres, strings. Imprime ds y as.data.frame(ds). ¿Cuál es la diferencia entre ambas? ds &lt;- tbl_df(mtcars) ds as.data.frame(ds) Nota: Para mayor información de este tipo de dataframes consulta la documentación de la libreria tibble. 3.1 El principio de datos limpios Los principios de datos limpios (Tidy Data de Hadley Wickham) proveen una manera estándar de organizar la información: Cada variable forma una columna. Cada observación forma un renglón. Cada tipo de unidad observacional forma una tabla. Nota: La mayor parte de las bases de datos en estadística tienen forma rectangular por lo que únicamente se trataran este tipo de estructura de datos. Una base de datos es una colección de valores numéricos o categóricos. Cada valor pertenece a una variable y a una observación. Una variable contiene los valores del atributo (genero, fabricante, ingreso) de la variable por unidad. Una observación contiene todos los valores medidos por la misma unidad (personas, día, autos, municipios) para diferentes atributos. Ejemplo: Supongamos un experimento con 3 pacientes cada uno tiene resultados de dos tratamientos (A y B): tratamientoA tratamientoB Juan Aguirre - 2 Ana Bernal 16 11 José López 3 1 La tabla anterior también se puede estructurar de la siguiente manera: Juan Aguirre Ana Bernal José López tratamientoA - 16 3 tratamientoB 2 11 1 Si vemos los principios, entonces ¿las tablas anteriores los cumplen? Para responder la pregunta veamos: ¿Cuáles son los valores? En total se tienen 18 valores en el conjunto de datos. ¿Cuáles son las variables? Se tienen tres variables: Persona/nombre: Juan Aguirre, Ana Bernal, y José López Tratamiento: A y B Resultado: -, 2, 16, 11, 3, 1 ¿Cuáles son las observaciones? Existen 6 observaciones. Entonces, siguiendo los principios de datos limpios obtenemos la siguiente estructura: nombre tratamiento resultado Juan Aguirre a - Ana Bernal a 16 José López a 3 Juan Aguirre b 2 Ana Bernal b 11 José López b 1 Una vez que identificamos los problemas de una base de datos podemos proceder a la limpieza. 3.2 Limpieza de datos Algunos de los problemas más comunes en las bases de datos que no están limpias son: Los encabezados de las columnas son valores y no nombres de variables. Más de una variable por columna. Las variables están organizadas tanto en filas como en columnas. Más de un tipo de observación en una tabla. Una misma unidad observacional está almacenada en múltiples tablas. La mayor parte de estos problemas se pueden arreglar con pocas herramientas, a continuación veremos como limpiar datos usando dos funciones del paquete tidyr de Hadley Wickham: gather: recibe múltiples columnas y las junta en pares de nombres y valores, convierte los datos anchos en largos. spread: recibe 2 columnas y las separa, haciendo los datos más anchos. Repasaremos los problemas más comunes que se encuentran en conjuntos de datos sucios y mostraremos cómo se puede manipular la tabla de datos (usando las funciones gather y spread) con el fin de estructurarla para que cumpla los principios de datos limpios. 1. Los encabezados de las columnas son valores Analicemos los datos que provienen de una encuesta de Pew Research que investiga la relación entre ingreso y afiliación religiosa. ¿Cuáles son las variables en estos datos? pew &lt;- read_csv(&quot;datos/pew.csv&quot;) knitr::kable(pew) religion &lt;$10k $10-20k $20-30k $30-40k $40-50k $50-75k $75-100k $100-150k &gt;150k Don’t know/refused Agnostic 27 34 60 81 76 137 122 109 84 96 Atheist 12 27 37 52 35 70 73 59 74 76 Buddhist 27 21 30 34 33 58 62 39 53 54 Catholic 418 617 732 670 638 1116 949 792 633 1489 Don’t know/refused 15 14 15 11 10 35 21 17 18 116 Evangelical Prot 575 869 1064 982 881 1486 949 723 414 1529 Hindu 1 9 7 9 11 34 47 48 54 37 Historically Black Prot 228 244 236 238 197 223 131 81 78 339 Jehovah’s Witness 20 27 24 24 21 30 15 11 6 37 Jewish 19 19 25 25 30 95 69 87 151 162 Mainline Prot 289 495 619 655 651 1107 939 753 634 1328 Mormon 29 40 48 51 56 112 85 49 42 69 Muslim 6 7 9 10 9 23 16 8 6 22 Orthodox 13 17 23 32 32 47 38 42 46 73 Other Christian 9 7 11 13 13 14 18 14 12 18 Other Faiths 20 33 40 46 49 63 46 40 41 71 Other World Religions 5 2 3 4 2 7 3 4 4 8 Unaffiliated 217 299 374 365 341 528 407 321 258 597 Para limpiarla es necesario apilar las columnas, es decir, pasar los datos a forma larga. Esto lo realizaremos con la función gather(): pew_tidy &lt;- pew %&gt;% gather(income, frequency, -religion) # vemos las primeras líneas de nuestros datos alargados pew_tidy %&gt;% head() %&gt;% knitr::kable() religion income frequency Agnostic &lt;$10k 27 Atheist &lt;$10k 12 Buddhist &lt;$10k 27 Catholic &lt;$10k 418 Don’t know/refused &lt;$10k 15 Evangelical Prot &lt;$10k 575 La nueva estructura de la base de datos nos permite, por ejemplo, hacer fácilmente una gráfica donde podemos comparar las diferencias en las frecuencias. library(dplyr) by_religion &lt;- group_by(pew_tidy, religion) pew_tidy_2 &lt;- pew_tidy %&gt;% filter(income != &quot;Don&#39;t know/refused&quot;) %&gt;% group_by(religion) %&gt;% mutate(percent = frequency / sum(frequency)) %&gt;% filter(sum(frequency) &gt; 1000) ggplot(pew_tidy_2, aes(x = income, y = percent, group = religion)) + facet_wrap(~ religion) + geom_bar(stat = &quot;identity&quot;, fill = &quot;darkgray&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) En el código de arriba utilizamos las funciones group_by, filter y mutate que estudiaremos más adelante. Otro ejemplo, billboard &lt;- tbl_df(read.csv(&quot;datos/billboard.csv&quot;, stringsAsFactors = FALSE)) billboard %&gt;% sample_n(5) %&gt;% knitr::kable() year artist track time date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 wk9 wk10 wk11 wk12 wk13 wk14 wk15 wk16 wk17 wk18 wk19 wk20 wk21 wk22 wk23 wk24 wk25 wk26 wk27 wk28 wk29 wk30 wk31 wk32 wk33 wk34 wk35 wk36 wk37 wk38 wk39 wk40 wk41 wk42 wk43 wk44 wk45 wk46 wk47 wk48 wk49 wk50 wk51 wk52 wk53 wk54 wk55 wk56 wk57 wk58 wk59 wk60 wk61 wk62 wk63 wk64 wk65 wk66 wk67 wk68 wk69 wk70 wk71 wk72 wk73 wk74 wk75 wk76 2000 Backstreet Boys, The The One 3:46 2000-05-27 58 50 43 37 31 30 39 47 55 61 76 90 93 93 100 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 Sisqo Incomplete 3:52 2000-06-24 77 66 61 61 61 55 2 1 1 2 2 4 5 5 7 8 10 10 9 14 17 20 25 31 32 46 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 Madison Avenue Don’t Call Me Baby 3:44 2000-07-08 98 96 93 93 93 92 92 92 90 92 88 88 88 95 93 98 93 92 90 97 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 Cagle, Chris My Love Goes On And … 3:02 2000-10-21 99 94 94 87 84 83 76 76 79 83 91 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 3 Doors Down Kryptonite 3:53 2000-04-08 81 70 68 67 66 57 54 53 51 51 51 51 47 44 38 28 22 18 18 14 12 7 6 6 6 5 5 4 4 4 4 3 3 3 4 5 5 9 9 15 14 13 14 16 17 21 22 24 28 33 42 42 49 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Queremos apilar las semanas de manera que sea una sola columna (nuevamente alargamos los datos): library(tidyr) billboard_long &lt;- gather(billboard, week, rank, wk1:wk76, na.rm=TRUE) billboard_long %&gt;% sample_n(10) %&gt;% knitr::kable() year artist track time date.entered week rank 2000 Lil’ Zane Callin’ Me 3:43 2000-07-29 wk9 45 2000 DMX Party Up (Up In Here… 3:45 2000-02-26 wk10 27 2000 Aguilera, Christina Come On Over Baby (A… 3:38 2000-08-05 wk6 18 2000 Thomas, Carl Emotional 4:31 2000-11-25 wk15 63 2000 Amber Sexual 4:38 1999-07-17 wk17 98 2000 Goo Goo Dolls Broadway 3:54 2000-04-22 wk20 87 2000 Westlife Swear It Again 4:07 2000-04-01 wk3 66 2000 Lonestar What About Now 3:30 2000-06-10 wk1 78 2000 Madonna Music 3:45 2000-08-12 wk6 1 2000 DMX Party Up (Up In Here… 3:45 2000-02-26 wk8 32 La instrucción na.rm = TRUE se utiliza para eliminar los valores faltantes en las columnas wk1 a wk76. Realizamos una limpieza adicional creando mejores variables de fecha. billboard_tidy &lt;- billboard_long %&gt;% mutate( week = parse_number(week), date = as.Date(date.entered) + 7 * (week - 1)) %&gt;% select(-date.entered) billboard_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() year artist track time week rank date 2000 Ginuwine The Best Man I Can B… 4:06 4 94 2000-01-29 2000 Dion, Celine That’s The Way It Is 4:03 8 30 2000-01-01 2000 Avant Separated 4:13 2 32 2000-05-06 2000 Vassar, Phil Carlene 4:07 7 47 2000-04-15 2000 Creed With Arms Wide Open 3:52 35 5 2001-01-06 2000 Aguilera, Christina Come On Over Baby (A… 3:38 6 18 2000-09-09 2000 Jagged Edge Let’s Get Married 4:23 14 14 2000-08-05 2000 Walker, Clay The Chain Of Love 5:03 15 71 2000-07-22 2000 Creed Higher 5:16 65 49 2000-12-02 2000 Lonestar Amazed 4:25 37 18 2000-02-12 Nuevamente, podemos hacer gráficas fácilmente. tracks &lt;- billboard_tidy %&gt;% filter(track %in% c(&quot;Come On Over Baby (A...&quot;, &quot;What A Girl Wants&quot;, &quot;Say My Name&quot;, &quot;Jumpin&#39; Jumpin&#39;&quot;, &quot;Bye Bye Bye&quot;)) ggplot(tracks, aes(x = date, y = rank)) + geom_line() + facet_wrap(~track, nrow = 1) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) 2. Una columna asociada a más de una variable La siguiente base de datos proviene de la Organización Mundial de la Salud y contiene el número de casos confirmados de tuberculosis por país y año, la información esta por grupo demográfico de acuerdo a sexo (m, f), y edad (0-4, 5-14, etc). library(countrycode) tb &lt;- read_csv(&quot;datos/tb.csv&quot;) tb$country_name &lt;- countrycode(tb$iso2, &#39;iso2c&#39;, &#39;country.name&#39;) tb %&gt;% sample_n(5) %&gt;% knitr::kable() iso2 year new_sp_m04 new_sp_m514 new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 new_sp_m4554 new_sp_m5564 new_sp_m65 new_sp_mu new_sp_f04 new_sp_f514 new_sp_f014 new_sp_f1524 new_sp_f2534 new_sp_f3544 new_sp_f4554 new_sp_f5564 new_sp_f65 new_sp_fu country_name PT 1980 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Portugal AT 2002 NA NA 1 8 14 32 43 20 25 NA NA NA 0 8 13 7 5 7 21 NA Austria LU 1984 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Luxembourg OM 1999 NA NA 2 10 11 23 15 7 10 NA NA NA 3 16 4 6 1 4 8 NA Oman NZ 1985 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA New Zealand De manera similar, utilizando la función gather() se busca apilar las columnas correspondientes a sexo-edad. ¿Cómo podemos separar la “variable” sexo-edad en dos columnas? tb_long &lt;- tb %&gt;% gather(demog, casos, new_sp_m04:new_sp_fu, na.rm=TRUE) tb_long %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name demog casos BZ 2001 Belize new_sp_m014 0 AN 2004 NA new_sp_m3544 4 CA 1991 Canada new_sp_m4554 37 NP 2002 Nepal new_sp_f1524 1203 RO 2006 Romania new_sp_m65 580 BH 1995 Bahrain new_sp_m65 3 PH 1998 Philippines new_sp_f2534 109 GN 2004 Guinea new_sp_f65 63 GU 2001 Guam new_sp_m2534 4 TM 1999 Turkmenistan new_sp_m2534 225 Las variables sexo y edad se obtienen separando la columna demog, para esto se usa la función separate()con los siguientes argumentos: tidyr::separate(data, col = name_variabletoseparate, into = c(vector with names using &quot;&quot;), sep) tb_tidy &lt;- tb_long %&gt;% separate(col = demog, into = c(&quot;sex&quot;, &quot;age&quot;), sep = 8) tb_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name sex age casos GN 2004 Guinea new_sp_f 2534 521 NG 2008 Nigeria new_sp_m u 0 PY 2006 Paraguay new_sp_f 1524 130 AT 2007 Austria new_sp_f 2534 14 BO 1997 Bolivia new_sp_m 1524 1214 MR 1999 Mauritania new_sp_f 3544 110 NI 2007 Nicaragua new_sp_f 3544 100 VU 2005 Vanuatu new_sp_f 65 2 NG 2005 Nigeria new_sp_f 65 415 JO 2006 Jordan new_sp_m 5564 4 Ahora para hacer mejor variable sex y age usaremos la función mutate() que permite crear nuevas variables sin modificar la dimensión del dataframe. library(stringr) tb_tidy &lt;- tb_long %&gt;% separate(col = demog, into = c(&quot;sex&quot;, &quot;age&quot;), sep = 8) %&gt;% mutate(sex = str_sub(sex, 8, 8), age = factor(age, levels = c(&quot;014&quot;, &quot;04&quot;, &quot;1524&quot;, &quot;2534&quot;, &quot;3544&quot;, &quot;4554&quot;, &quot;514&quot;, &quot;5564&quot;, &quot;65&quot;,&quot;u&quot;), labels = c(&quot;0-14&quot;, &quot;0-4&quot;, &quot;15-24&quot;, &quot;25-34&quot;, &quot;35-44&quot;, &quot;45-54&quot;, &quot;5-14&quot;, &quot;55-64&quot;, &quot;65+&quot;,&quot;unknown&quot;) ) ) tb_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name sex age casos LB 2004 Lebanon m 65+ 6 GT 2007 Guatemala m 45-54 203 EG 2007 Egypt m 25-34 853 BS 2003 Bahamas f 0-14 2 ZW 2007 Zimbabwe m 65+ 153 SG 1999 Singapore f 35-44 18 PA 1999 Panama m 25-34 209 SL 1998 Sierra Leone f 25-34 294 MX 2000 Mexico m 0-14 214 UA 1999 Ukraine m 45-54 1825 Se puede separar la columna demog en dos variables, sexo y edad, utilizando la función separate. Se debe indicar la posición de donde deseamos “cortar”: tb_tidy &lt;- tidyr::separate(tb_long, demog, c(&quot;sex&quot;, &quot;age&quot;), 8) tb_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name sex age casos KZ 1998 Kazakhstan new_sp_f 4554 204 PL 2001 Poland new_sp_m 3544 603 NR 2003 Nauru new_sp_f 014 0 NC 1996 New Caledonia new_sp_m 4554 5 BH 2008 Bahrain new_sp_f 1524 12 LK 1995 Sri Lanka new_sp_m 2534 361 GH 2000 Ghana new_sp_f 65 176 ES 2002 Spain new_sp_f 014 17 GA 1998 Gabon new_sp_f 014 15 SE 1999 Sweden new_sp_m 3544 12 3. Variables almacenadas en filas y columnas El problema más difícil es cuando las variables están tanto en filas como encolumnas, veamos una base de datos de clima en Cuernavaca. ¿Cuáles son las variables en estos datos? clima &lt;- tbl_df(read.delim(&quot;datos/clima.txt&quot;, stringsAsFactors=FALSE)) clima %&gt;% sample_n(10) %&gt;% knitr::kable() id year month element d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 d11 d12 d13 d14 d15 d16 d17 d18 d19 d20 d21 d22 d23 d24 d25 d26 d27 d28 d29 d30 d31 MX000017004 2010 5 TMIN NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 182 NA NA NA NA MX000017004 2010 4 TMIN NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 167 NA NA NA NA MX000017004 2010 1 TMAX NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 278 NA MX000017004 2010 5 TMAX NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 332 NA NA NA NA MX000017004 2010 12 TMAX 299 NA NA NA NA 278 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA MX000017004 2010 11 TMIN NA 163 NA 120 79 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 121 142 NA NA NA NA MX000017004 2010 4 TMAX NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 363 NA NA NA NA MX000017004 2010 11 TMAX NA 313 NA 272 263 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 281 277 NA NA NA NA MX000017004 2010 6 TMIN NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 175 NA NA NA NA NA NA NA NA NA NA NA 180 NA NA MX000017004 2010 3 TMIN NA NA NA NA 142 NA NA NA NA 168 NA NA NA NA NA 176 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Estos datos tienen variables en columnas individuales (id, año, mes), en múltiples columnas (día, d1-d31) y en filas (tmin, tmax). Comencemos por apilar las columnas. clima_long &lt;- clima %&gt;% gather(day, value, d1:d31, na.rm = TRUE) head(clima_long) %&gt;% knitr::kable() id year month element day value MX000017004 2010 12 TMAX d1 299 MX000017004 2010 12 TMIN d1 138 MX000017004 2010 2 TMAX d2 273 MX000017004 2010 2 TMIN d2 144 MX000017004 2010 11 TMAX d2 313 MX000017004 2010 11 TMIN d2 163 Podemos crear algunas variables adicionales. clima_vars &lt;- clima_long %&gt;% mutate(day = extract_numeric(day), value = value / 10) %&gt;% select(id, year, month, day, element, value) %&gt;% arrange(id, year, month, day) #&gt; extract_numeric() is deprecated: please use readr::parse_number() instead head(clima_vars) %&gt;% knitr::kable() id year month day element value MX000017004 2010 1 30 TMAX 27.8 MX000017004 2010 1 30 TMIN 14.5 MX000017004 2010 2 2 TMAX 27.3 MX000017004 2010 2 2 TMIN 14.4 MX000017004 2010 2 3 TMAX 24.1 MX000017004 2010 2 3 TMIN 14.4 Finalmente, la columna element no es una variable, sino que almacena el nombre de dos variables, la operación que debemos aplicar (spread) es el inverso de apilar (gather): clima_tidy &lt;- clima_vars %&gt;% spread(element, value) clima_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() id year month day TMAX TMIN MX000017004 2010 8 23 26.4 15.0 MX000017004 2010 8 29 28.0 15.3 MX000017004 2010 4 27 36.3 16.7 MX000017004 2010 11 27 27.7 14.2 MX000017004 2010 7 14 29.9 16.5 MX000017004 2010 10 28 31.2 15.0 MX000017004 2010 6 17 28.0 17.5 MX000017004 2010 3 5 32.1 14.2 MX000017004 2010 8 13 29.8 16.5 MX000017004 2010 2 2 27.3 14.4 Ahora es inmediato no solo hacer gráficas sino también ajustar un modelo. # ajustamos un modelo lineal donde la variable respuesta es temperatura # máxima, y la variable explicativa es el mes clima_lm &lt;- lm(TMAX ~ factor(month), data = clima_tidy) summary(clima_lm) #&gt; #&gt; Call: #&gt; lm(formula = TMAX ~ factor(month), data = clima_tidy) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.65 -0.92 -0.02 1.05 3.18 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 27.800 1.861 14.94 5.3e-13 *** #&gt; factor(month)2 -0.050 2.081 -0.02 0.9810 #&gt; factor(month)3 4.767 2.149 2.22 0.0372 * #&gt; factor(month)4 8.500 2.632 3.23 0.0039 ** #&gt; factor(month)5 5.400 2.632 2.05 0.0523 . #&gt; factor(month)6 1.250 2.279 0.55 0.5889 #&gt; factor(month)7 1.450 2.279 0.64 0.5312 #&gt; factor(month)8 0.471 1.990 0.24 0.8149 #&gt; factor(month)10 1.100 2.039 0.54 0.5949 #&gt; factor(month)11 0.320 2.039 0.16 0.8767 #&gt; factor(month)12 1.050 2.279 0.46 0.6496 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.86 on 22 degrees of freedom #&gt; Multiple R-squared: 0.618, Adjusted R-squared: 0.445 #&gt; F-statistic: 3.56 on 10 and 22 DF, p-value: 0.0062 4. Mas de un tipo de observación en una misma tabla En ocasiones las bases de datos involucran valores en diferentes niveles, endiferentes tipos de unidad observacional. En la limpieza de datos, cada unidad observacional debe estar almacenada en su propia tabla (esto esta ligado a normalización de una base de datos), es importante para evitar inconsistencias en los datos. ¿Cuáles son las unidades observacionales de los datos de billboard? billboard_tidy %&gt;% arrange(artist, track, year, time) %&gt;% head(20) #&gt; # A tibble: 20 x 7 #&gt; year artist track time week rank date #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; #&gt; 1 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 1 87 2000-02-26 #&gt; 2 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 2 82 2000-03-04 #&gt; 3 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 3 72 2000-03-11 #&gt; 4 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 4 77 2000-03-18 #&gt; 5 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 5 87 2000-03-25 #&gt; 6 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 6 94 2000-04-01 #&gt; # ... with 14 more rows Separemos esta base de datos en dos: la tabla canción que almacena artista, nombre de la canción y duración; la tabla rank que almacena el ranking de la canción en cada semana. song &lt;- billboard_tidy %&gt;% select(artist, track, year, time) %&gt;% unique() %&gt;% arrange(artist) %&gt;% mutate(song_id = row_number(artist)) song %&gt;% sample_n(10) %&gt;% knitr::kable() artist track year time song_id Zombie Nation Kernkraft 400 2000 3:30 316 Brooks, Garth Do What You Gotta Do 2000 2:56 48 Larrieux, Amel Get Up 2000 4:02 167 Sisqo Thong Song 2000 4:05 265 Next Wifey 2000 4:03 224 Diffie, Joe The Quittin’ Kind 2000 3:23 79 Lil Wayne Tha Block Is Hot 2000 4:13 172 SheDaisy This Woman Needs 2000 3:20 258 Aaliyah Try Again 2000 4:03 9 Jay-Z Big Pimpin’ 2000 3:55 145 rank &lt;- billboard_tidy %&gt;% left_join(song, c(&quot;artist&quot;, &quot;track&quot;, &quot;year&quot;, &quot;time&quot;)) %&gt;% select(song_id, date, week, rank) %&gt;% arrange(song_id, date) %&gt;% tbl_df rank %&gt;% sample_n(10) %&gt;% knitr::kable() song_id date week rank 253 2000-07-15 23 26 255 2000-03-11 21 4 126 2000-04-08 8 77 65 2000-09-23 1 97 131 1999-10-30 4 43 214 2000-10-28 8 10 156 2000-09-16 7 36 211 2000-12-02 13 45 157 2000-08-19 3 75 264 2000-07-01 2 66 5. Una misma unidad observacional está almacenada en múltiples tablas También es común que los valores sobre una misma unidad observacional estén separados en muchas tablas o archivos, es común que estas tablas esten divididas de acuerdo a una variable, de tal manera que cada archivo representa a una persona, año o ubicación. Para juntar los archivos hacemos lo siguiente: Leemos los archivos en una lista de tablas. Para cada tabla agregamos una columna que registra el nombre del archivo original. Combinamos las tablas en un solo data frame. Veamos un ejemplo, la carpeta specdata contiene 332 archivos csv que almacenan información de monitoreo de contaminación en 332 ubicaciones de EUA. Cada archivo contiene información de una unidad de monitoreo y el número de identificación del monitor es el nombre del archivo. Los pasos en R (usando el paquete plyr), primero creamos un vector con los nombres de los archivos en un directorio, aligiendo aquellos que contengan las letras “.csv”. paths &lt;- dir(&quot;datos/specdata&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) Después le asignamos el nombre del csv al nombre de cada elemento del vector. Este paso se realiza para preservar los nmobres de los archivos ya que estos los asignaremos a una variable mas adelante. names(paths) &lt;- basename(paths) La función map_df del paquete purrr itera sobre cada dirección, lee el csv en dicha dirección y los combina en un data frame. specdata_US &lt;- map_df(paths, read.csv, stringsAsFactors = FALSE) specdata &lt;- specdata_US %&gt;% mutate(monitor = extract_numeric(ID), date = as.Date(Date)) %&gt;% select(id = ID, monitor, date, sulfate, nitrate) glimpse(specdata) #&gt; Observations: 772,087 #&gt; Variables: 5 #&gt; $ id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... #&gt; $ monitor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... #&gt; $ date &lt;date&gt; 2003-01-01, 2003-01-02, 2003-01-03, 2003-01-04, 2003-... #&gt; $ sulfate &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... #&gt; $ nitrate &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... 6. Otras consideraciones En las buenas prácticas es importante tomar en cuenta los siguientes puntos: Incluir un encabezado con el nombre de las variables. Los nombres de las variables deben ser entendibles (e.g. age_at_diagnosis es mejor que AgeDx). En general los datos se deben guardar en un archivo por tabla. Escribir un script con las modificaciones que se hicieron a los datos crudos (reproducibilidad). Otros aspectos importantes en la limpieza de datos son: selección del tipo de variables (por ejemplo fechas), datos faltantes, typos y detección de valores atípicos. 3.3 Separa-aplica-combina Muchos problemas de análisis de datos involucran la aplicación de la estrategia split-apply-combine de Hadley Whickam, 2011. Esto se traduce en realizar filtros, cálculos y agregación de datos. Split-apply-combine Separa la base de datos original. Aplica funciones a cada subconjunto. Combina los resultados en una nueva base de datos. Consiste en romper un problema en pedazos (de acuerdo a una variable de interés), operar sobre cada subconjunto de manera independiente (calcular la media de cada grupo) y después unir los pedazos nuevamente. Cuando pensamos como implementar la estrategia divide-aplica-combina es natural pensar en iteraciones para recorrer cada grupo de interés y aplicar las funciones. Para esto usaremos la librería dplyr que contiene funciones que facilitan la implementación de la estrategia. Son importantes las siguientes funciones de la librería dplyr: filter: obtiene un subconjunto de las filas de acuerdo a una condición. select: selecciona columnas de acuerdo al nombre. arrange: re ordena las filas. mutate: agrega nuevas variables. summarise: reduce variables a valores (crear nuevas bases de datos). Para mostrar las funciones se usará el siguiente dataframe. df_ej &lt;- data.frame(genero = c(&quot;mujer&quot;, &quot;hombre&quot;, &quot;mujer&quot;, &quot;mujer&quot;, &quot;hombre&quot;), estatura = c(1.65, 1.80, 1.70, 1.60, 1.67)) df_ej %&gt;% knitr::kable() genero estatura mujer 1.65 hombre 1.80 mujer 1.70 mujer 1.60 hombre 1.67 Filtrar Filtrar una base de datos dependiendo de una condición requiere la función filter() que tiene los siguientes argumentos dplyr::filter(data, condition). df_ej %&gt;% filter(genero == &quot;mujer&quot;) #&gt; genero estatura #&gt; 1 mujer 1.65 #&gt; 2 mujer 1.70 #&gt; 3 mujer 1.60 Seleccionar Elegir columnas de un conjunto de datos se puede hacer con la función select() que tiene los siguientes argumentos dplyr::select(data, seq_variables). df_ej %&gt;% select(genero) #&gt; genero #&gt; 1 mujer #&gt; 2 hombre #&gt; 3 mujer #&gt; 4 mujer #&gt; 5 hombre También, existen funciones que se usan exclusivamente en select(): starts_with(x, ignore.case = TRUE): los nombres empiezan con x. ends_with(x, ignore.case = TRUE): los nombres terminan con x. contains(x, ignore.case = TRUE): selecciona las variable que contengan x. matches(x, ignore.case = TRUE): selecciona las variable que igualen la expresión regular x. num_range(&quot;x&quot;, 1:5, width = 2): selecciona las variables (numéricamente) de x01 a x05. one_of(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;): selecciona las variables que estén en un vector de caracteres. everything(): selecciona todas las variables. Por ejemplo: df_ej %&gt;% select(starts_with(&quot;g&quot;)) #&gt; genero #&gt; 1 mujer #&gt; 2 hombre #&gt; 3 mujer #&gt; 4 mujer #&gt; 5 hombre Arreglar Arreglar u ordenar de acuerdo al valor de una o más variables es posible con la función arrange() que tiene los siguientes argumentos dplyr::arrange(data, variables_por_las_que_ordenar). La función desc() permite que se ordene de forma descendiente. df_ej %&gt;% arrange(desc(estatura)) #&gt; genero estatura #&gt; 1 hombre 1.80 #&gt; 2 mujer 1.70 #&gt; 3 hombre 1.67 #&gt; 4 mujer 1.65 #&gt; 5 mujer 1.60 Mutar Mutar consiste en crear nuevas variables con la función mutate() que tiene los siguientes argumentos dplyr::mutate(data, nuevas_variables = operaciones): df_ej %&gt;% mutate(estatura_cm = estatura * 100) #&gt; genero estatura estatura_cm #&gt; 1 mujer 1.65 165 #&gt; 2 hombre 1.80 180 #&gt; 3 mujer 1.70 170 #&gt; 4 mujer 1.60 160 #&gt; 5 hombre 1.67 167 Resumir Los resúmenes permiten crear nuevas bases de datos que son agregaciones de los datos originales. La función summarise() permite realizar este resumendplyr::summarise(data, nuevas_variables = operaciones): df_ej %&gt;% dplyr::summarise(promedio = mean(estatura)) #&gt; promedio #&gt; 1 1.68 También es posible hacer resúmenes agrupando por variables determinadas de la base de datos. Pero, primero es necesario crear una base agrupada con la función group_by() con argumentos dplyr::group_by(data, add = variables_por_agrupar): df_ej %&gt;% group_by(genero) #&gt; # A tibble: 5 x 2 #&gt; # Groups: genero [2] #&gt; genero estatura #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 Después se opera sobre cada grupo, creando un resumen a nivel grupo y uniendo los subconjuntos en una base nueva: df_ej %&gt;% group_by(genero) %&gt;% dplyr::summarise(promedio = mean(estatura)) #&gt; # A tibble: 2 x 2 #&gt; genero promedio #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 hombre 1.74 #&gt; 2 mujer 1.65 3.4 Muertes por armas de fuego en EUA Los datos que vamos a utilizar provienen principalmente de la base de datos de causas múltiples de la muerte de los Centros para el Control y Prevención de la Enfermedad (CDCs) de Estados Unidos, de certificados de defunción de los 50 estados. Se considera que esta fuente de información es la base de datos más completa de muertes por armas de fuego. Para más información puedes leer el artículo: https://fivethirtyeight.com/features/gun-deaths/ Comencemos leyendo los datos para los años 2012, 2013 y 2014: guns_12 &lt;- read_csv(&quot;datos/guns_12.csv&quot;, na = &quot;&quot;) guns_13 &lt;- read_csv(&quot;datos/guns_13.csv&quot;, na = &quot;&quot;) guns_14 &lt;- read_csv(&quot;datos/guns_14.csv&quot;, na = &quot;&quot;) Las tres tablas tienen las mismas variables en el mismo orden. Examinemos la tabla para el año 2012: glimpse(guns_12) #&gt; Observations: 33,096 #&gt; Variables: 40 #&gt; $ res_status &lt;int&gt; 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ... #&gt; $ education_89 &lt;chr&gt; &quot;16&quot;, &quot;13&quot;, &quot;16&quot;, &quot;17&quot;, &quot;12&quot;, &quot;10&quot;, &quot;12&quot;, &quot;12... #&gt; $ education_03 &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ education_flag &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... #&gt; $ month &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;03... #&gt; $ sex &lt;chr&gt; &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, ... #&gt; $ detail_age &lt;int&gt; 1034, 1021, 1060, 1064, 1031, 1017, 1048, 104... #&gt; $ age_flag &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ age_recode &lt;int&gt; 32, 30, 38, 38, 32, 29, 35, 34, 36, 32, 30, 3... #&gt; $ age_recode2 &lt;chr&gt; &quot;12&quot;, &quot;10&quot;, &quot;18&quot;, &quot;18&quot;, &quot;12&quot;, &quot;09&quot;, &quot;15&quot;, &quot;14... #&gt; $ age_group &lt;chr&gt; &quot;05&quot;, &quot;04&quot;, &quot;08&quot;, &quot;08&quot;, &quot;05&quot;, &quot;04&quot;, &quot;07&quot;, &quot;06... #&gt; $ age_infant &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ death_place &lt;int&gt; 4, 7, 7, 4, 7, 4, 1, 7, 7, 4, 4, 4, 2, 4, 4, ... #&gt; $ marital &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;M&quot;, &quot;S&quot;, &quot;U&quot;, &quot;S&quot;, &quot;W&quot;, &quot;M&quot;, &quot;M&quot;, ... #&gt; $ day_of_week &lt;int&gt; 6, 4, 7, 7, 1, 7, 5, 1, 7, 5, 1, 5, 4, 4, 5, ... #&gt; $ data_year &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 201... #&gt; $ at_work &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, ... #&gt; $ death_manner &lt;chr&gt; &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;5&quot;, &quot;2&quot;, &quot;1&quot;, ... #&gt; $ burial &lt;chr&gt; &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, ... #&gt; $ autopsy &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, ... #&gt; $ activity &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ... #&gt; $ injury_place &lt;int&gt; 0, 4, 8, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, ... #&gt; $ underlying_cause &lt;chr&gt; &quot;X74&quot;, &quot;X74&quot;, &quot;X72&quot;, &quot;X74&quot;, &quot;X72&quot;, &quot;X73&quot;, &quot;Y2... #&gt; $ cause_recode358 &lt;int&gt; 429, 429, 429, 429, 429, 429, 446, 429, 407, ... #&gt; $ cause_recode113 &lt;int&gt; 125, 125, 125, 125, 125, 125, 132, 125, 119, ... #&gt; $ cause_recode130 &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ cause_recode39 &lt;int&gt; 40, 40, 40, 40, 40, 40, 42, 40, 39, 40, 40, 4... #&gt; $ race &lt;chr&gt; &quot;68&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;03&quot;, &quot;01&quot;, &quot;03... #&gt; $ race_bridged &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ race_flag &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ race_recode &lt;int&gt; 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, ... #&gt; $ race_recode2 &lt;int&gt; 4, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 3, ... #&gt; $ hispanic &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, ... #&gt; $ hispanic_recode &lt;int&gt; 8, 6, 6, 6, 6, 8, 6, 8, 6, 6, 8, 6, 8, 6, 8, ... #&gt; $ intent &lt;chr&gt; &quot;Suicide&quot;, &quot;Suicide&quot;, &quot;Suicide&quot;, &quot;Suicide&quot;, &quot;... #&gt; $ police &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... #&gt; $ weapon &lt;chr&gt; &quot;Other/unknown&quot;, &quot;Other/unknown&quot;, &quot;Handgun&quot;, ... #&gt; $ year &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 201... #&gt; $ age &lt;int&gt; 34, 21, 60, 64, 31, 17, 48, 41, 50, 30, 21, 4... #&gt; $ place &lt;chr&gt; &quot;Home&quot;, &quot;Street&quot;, &quot;Other specified&quot;, &quot;Home&quot;, ... Para pegar las tablas para los 3 años vamos a utilizar la función bind_rows() del paquete dplyr: guns &lt;- guns_12 %&gt;% bind_rows(guns_13) %&gt;% bind_rows(guns_14) Veamos otro ejemplo de cómo recodificar variables categóricas, en este caso para la variable de nivel educativo: guns &lt;- guns %&gt;% mutate(education = ifelse(education_flag == 1, cut(as.numeric(education_03), breaks = c(0, 2, 3, 5, 8, 9, labels = c(&quot;Less than HS&quot;, &quot;HS/GED&quot;, &quot;Some college&quot;, &quot;BA+&quot;, NA))), cut(as.numeric(education_89), breaks = c(0, 11, 12, 15, 17, 99), labels = c(&quot;Less than HS&quot;, &quot;HS/GED&quot;, &quot;Some college&quot;, &quot;BA+&quot;, NA)))) Otro ejemplo, para la variable de raza: guns &lt;- guns %&gt;% mutate(race = as.integer(race), race = ifelse(hispanic &gt; 199 &amp; hispanic &lt;996, &quot;Hispanic&quot;, ifelse(race == &quot;01&quot;, &quot;White&quot;, ifelse(race == &quot;02&quot;, &quot;Black&quot;, ifelse(as.numeric(race) &gt;= 4 &amp; as.numeric(race) &lt;= 78, &quot;Asian/Pacific Islander&quot;,&quot;Native American/Native Alaskan&quot;)))), race = ifelse(is.na(race), &quot;Unknown&quot;, race)) Para quedarnos con las variables con las que vamos a trabajar utilizamos la función select(): guns &lt;- guns %&gt;% select(year, month, intent, police, sex, age, race, hispanic, place, education) Veamos de nuevo cómo es la estructura de la tabla: str(guns) #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 99396 obs. of 10 variables: #&gt; $ year : int 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ... #&gt; $ month : chr &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;02&quot; ... #&gt; $ intent : chr &quot;Suicide&quot; &quot;Suicide&quot; &quot;Suicide&quot; &quot;Suicide&quot; ... #&gt; $ police : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ sex : chr &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; ... #&gt; $ age : int 34 21 60 64 31 17 48 41 50 30 ... #&gt; $ race : chr &quot;Asian/Pacific Islander&quot; &quot;Native American/Native Alaskan&quot; &quot;Native American/Native Alaskan&quot; &quot;Native American/Native Alaskan&quot; ... #&gt; $ hispanic : int 100 100 100 100 100 100 100 100 100 100 ... #&gt; $ place : chr &quot;Home&quot; &quot;Street&quot; &quot;Other specified&quot; &quot;Home&quot; ... #&gt; $ education: int 4 3 4 4 2 1 2 2 3 3 ... Supongamos que nos interesa analizar el número de suicidios por arma de fuego para cada uno de los tres años. Esto quiere decir que es necesario agrupar y usar una función de resumen: guns %&gt;% filter(intent == &quot;Suicide&quot;) %&gt;% group_by(year) %&gt;% summarize(suicides = n()) #&gt; # A tibble: 3 x 2 #&gt; year suicides #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2012 20663 #&gt; 2 2013 21172 #&gt; 3 2014 21333 Supongamos que deseamos filtar (quitar las observaciones) de homicidios para los cuales se tiene la categoría de “Other”, para ello utilizamos la función filter(): guns_sin_especificar &lt;- guns %&gt;% filter(place != &quot;Other unspecified&quot; &amp; place != &quot;Other specified&quot;) Podemos analizar la siguiente gráfica de mosaico: ggplot(guns_sin_especificar, aes(x=as.factor(place), fill=as.factor(intent))) + geom_bar(position=&#39;fill&#39;) + coord_flip() + theme(aspect.ratio = 1,legend.position=&quot;bottom&quot;, axis.text.y=element_text(color=&#39;black&#39;,size=10), axis.text.x=element_text(color=&#39;black&#39;,size=10), axis.title.x=element_text(size=10), axis.title.y=element_text(size=10), legend.text=element_text(size=10)) + scale_fill_discrete(&quot;&quot;) + ylab(&#39;Proporción&#39;) + xlab(&quot;Lugar&quot;) + ggtitle(&quot;Lugar de homicidios por intención&quot;) Se podría concluir, por ejemplo, que si un homicidio ocurrió en una granja, entonces lo más probable es que haya sido un suicidio. 3.5 El Cuarteto de Anscombe “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey En 1971 un estadístico llamado Frank Anscombe (fundador del departamento de Estadística de la Universidad de Yale) encontró cuatro conjuntos de datos (I, II, III y IV). Cada uno consiste de 11 observaciones y tienen las mismas propiedades estadísticas. anscombe \\(x_1\\) \\(y_1\\) \\(x_2\\) \\(y_2\\) \\(x_3\\) \\(y_3\\) \\(x_4\\) \\(y_4\\) 10.0 8.04 10.0 9.14 10.0 7.46 8.0 6.58 8.0 6.95 8.0 8.14 8.0 6.77 8.0 5.76 13.0 7.58 13.0 8.74 13.0 12.74 8.0 7.71 9.0 8.81 9.0 8.77 9.0 7.11 8.0 8.84 11.0 8.33 11.0 9.26 11.0 7.81 8.0 8.47 14.0 9.96 14.0 8.10 14.0 8.84 8.0 7.04 6.0 7.24 6.0 6.13 6.0 6.08 8.0 5.25 4.0 4.26 4.0 3.10 4.0 5.39 19.0 12.50 12.0 10.84 12.0 9.13 12.0 8.15 8.0 5.56 7.0 4.82 7.0 7.26 7.0 6.42 8.0 7.91 5.0 5.68 5.0 4.74 5.0 5.73 8.0 6.89 Por ejemplo, todos los conjuntos de datos I, II, III, y IV, tienen exactamente misma media de \\(x\\), \\(\\bar{x}_i = \\bar{x}_j\\), y misma media de \\(y\\), \\(\\bar{y}_i = \\bar{y}_j\\) para toda \\(i,j=1,2,3,4\\). Además, se puede ver que todos tienen misma varianza muestral de \\(x\\) y de \\(y\\). En cada conjunto de datos la correlación entre \\(x\\) y \\(y\\) es la misma, y por consiguiente, los coeficientes de la regresión lineal \\(\\beta_0\\) y \\(\\beta_1\\) también son iguales. Propiedad Valor Media de \\(x\\) 9 Varianza muestral de \\(x\\) 11 Media de \\(y\\) 7.50 Varianza muestral de \\(y\\) 4.12 Correlación entre \\(x\\) y \\(y\\) 0.816 Línea de regresión lineal \\(y = 3.00 + 0.500x\\) ¿En qué son diferentes estos conjuntos de datos? ¿Es posible con la información anterior concluir que los cuatro conjuntos de datos deben ser similares? ¿Que tengan estadísticas similares asegura que provienen de un mismo modelo? Cuando analizamos los datos de manera gráfica en un histograma encontramos rápidamente que los conjuntos de datos son muy distintos. “Una imagen dice más que mil palabras.” En la gráfica del primer conjunto de datos, se ven datos como los que se tendrían en una relación lineal simple con un modelo que cumple los supuestos de normalidad. La segunda gráfica (la de arriba a la derecha) muestra unos datos que tienen una asociación pero definitivamente no es lineal y el coeficiente de correlación no es relevante en este caso. En la tercera gráfica (abajo a la izquierda) están puntos alineados perfectamente en una línea recta, excepto por uno de ellos. En la última gráfica podemos ver un ejemplo en el cual basta tener una observación atípica para que se produzca un coeficiente de correlación alto aún cuando en realidad no existe una asociación lineal entre las dos variables. Edward Tufte usó el cuarteto en la primera página del primer capítulo de su libro The Visual Display of Quantitative Information, para enfatizar la importancia de mirar los datos antes de analizarlos. (Tufte and Graves-Morris 2014) 3.6 The Grammar of Graphics de Leland Wilkinson Una ventaje de ggplot es que implementa una gramática de gráficas de forma organizada y con sentido orientada a esta forma de asociar variables con geometrías (Wilkinson 2005). En lugar de tener una lista enorme y conceptualmente plana de opciones para hacer gráficas, ggplot parte en varios pasos el procedimiento para realizar una gráfica: primero, se debe proporcionar información a la función sobre qué datos y qué variables se van a utilizar. segundo, se debe vincular las variables que se van a utilizar en la gráfica con las características específicas que se requiere tener en la gráfica. tercero, se debe elegir una función geom_ para indicar qué tipo de gráfica se dibujará, un diagrama de dispersión, una gráfica de barras o un diagrama de caja. En general, según Leland Wilkinson, hay dos principios generales que se deben seguir: La geometría utilizada debe coincidir con los datos que se están visualizando. La geometría utilizada debe ser fácil de interpretar. 3.7 ggplot Vamos a ver cómo visualizar los datos usando ggplot2. R tiene varios sistemas para hacer gráficas, pero ggplot2 es uno de los más elegantes y versátiles. ggplot2 implementa la gramática de gráficas, un sistema consistente para describir y construir gráficas. Con ggplot2, pueden hacerse cosas más rápido, aprendiendo un único sistema consistente, y aplicándolo de muchas formas. Para mayor información sobre los fundamentos teóricos de ggplot2 se recomienda leer el artículo titulado “The Layered Grammar of Graphics”, visitando la siguiente liga: http://vita.had.co.nz/papers/layered-grammar.pdf. Lo más importante para entender ggplot es comprender la estructura y la lógica para hacer una gráfica. El código debe decir cuáles son las conexiones entre las variables en los datos y los elementos de la gráfica tal como los vamos a ver en la pantalla, los puntos, los colores y las formas. En ggplot, estas conexiones lógicas entre los datos y los elementos de la gráfica se denominan asignaciones estéticas o simplemente estéticas. Se comienza una gráfica indicando a ggplot cuáles son los datos, qué variables en los datos se van a usar y luego cómo las variables en estos datos se mapean lógicamente en la estética de la gráfica. Luego, toma el resultado y se indica qué tipo de gráfica se desea, por ejemplo, un diagrama de dispersión, una gráfica de barras, o una gráfica de línea. En ggplot este tipo general de gráficas se llama geom. Cada geom tiene una función que lo crea. Por ejemplo, geom_point() hace diagramas de dispersión, geom_bar() hace gráficas de barras, geom_line() hace gráficas de línea, y así sucesivamente. Para combinar estas dos piezas, el objeto ggplot() y el geom se suman literalmente en una expresión, utilizando el símbolo “+”. ¿Qué geometrías son más adecuadas para cada tipo de variable? Usaremos los datos de gapminder para hacer nuestras primeras gráficas. Vamos a asegurarnos de que la biblioteca que contiene los datos esté cargada: library(gapminder) Esto hace que una tabla de datos esté disponible para su uso. Para ver un pedazo de la tabla utilizamos la función glimpse(): library(tidyverse) glimpse(gapminder) #&gt; Observations: 1,704 #&gt; Variables: 6 #&gt; $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, ... #&gt; $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia... #&gt; $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992... #&gt; $ lifeExp &lt;dbl&gt; 28.8, 30.3, 32.0, 34.0, 36.1, 38.4, 39.9, 40.8, 41.7... #&gt; $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 1488... #&gt; $ gdpPercap &lt;dbl&gt; 779, 821, 853, 836, 740, 786, 978, 852, 649, 635, 72... Supongamos que queremos graficar la esperanza de vida vs el PIB per cápita para todos los años y países en los datos. Haremos esto creando un objeto que contenga parte de la información necesario y a partir de ahí vamos a construir nuestra gráfica. Primero debemos indicarle a la función ggplot() qué datos estamos utilizando: p &lt;- ggplot(data = gapminder) p En este punto, ggplot sabe cuáles son nuestros datos, pero no cuál es el mapeo, es decir, qué variables de los datos deben correlacionarse con qué elementos visuales de la trama. Tampoco sabe qué tipo de trama queremos. En ggplot, las asignaciones se especifican utilizando la función aes(). Me gusta esta: Hasta este punto ggplot conoce qué datos se van a utilizar para hacer la gráfico, pero no el mapeo o asociación de qué variables se van a relacionar con los elementos visuales de la gráfica. Tampoco se sabe qué tipo de gráfica se va a hacer. En ggplot, las asignaciones se especifican utilizando la función aes(): p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) El argumento mapping = aes(...) vincula variables a cosas que se van a ver en la gráfica. Los valores de \\(x\\) y \\(y\\) son los más obvios. Otras asignaciones estéticas pueden incluir, por ejemplo, el color, la forma, el tamaño y el tipo de línea (si una línea es sólida o discontinua, o algún otro patrón). Un mapeo no dice directamente qué formas o colores van a aparecer en la gráfica. Más bien, dicen qué variables en los datos serán representadas por los elementos visuales como color, forma o un punto. ¿Qué sucede si simplemente escribimos p en la consola y ejecutamos? p El objeto p ha sido creado por la función ggplot(), y ya tiene información sobre las asignaciones que queremos, junto con mucha otra información añadida por defecto. (Si quiere ver cuánta información hay en el objeto p, intente solicitar str(p)). Sin embargo, no le hemos dado ninguna instrucción acerca de qué tipo de diagrama dibujar. Necesitamos agregar una capa a la trama. Esto significa elegir una función geom_*. Usaremos geom_point(). Sabe cómo tomar valores xey y trazarlos en un diagrama de dispersión. Se ha creado el objeto p utilizando la función ggplot() y este objeto ya tiene información de las asignacionesque queremos. Sin embargo, no se le ha dado ninguna instrucción sobre qué tipo de gráfica se quiere dibujar. Necesitamos agregar una capa a la gráfica. Esto se hace mediante el símbolo +. Esto significa elegir una función geom_. Utilizaremos geom_point() para hacer un diagrama de dispersión. p + geom_point() El mapeo de las propiedades estéticas se denomina escalamiento y depende del tipo de variable, las variables discretas (por ejemplo, genero, escolaridad, país) se mapean a distintas escalas que las variables continuas (variables numéricas como edad, estatura, etc.), los defaults para algunos atributos son (estos se pueden modificar): aes Discreta Continua Color (color) Arcoiris de colores Gradiente de colores Tamaño (size) Escala discreta de tamaños Mapeo lineal entre el área y el valor Forma (shape) Distintas formas No aplica Transparencia (alpha) No aplica Mapeo lineal a la transparencia Los geoms controlan el tipo de gráfica: p + geom_smooth() Podemos ver de inmediato que algunos de estos geoms hacen mucho más que simplemente poner puntos en una cuadrícula. Aquí geom_smooth() ha calculado una línea suavizada y la región sombreada representa el error estándar de la línea suavizada. Si queremos ver los puntos de datos y la línea juntos, simplemente agregamos geom_point() de nuevo como una capa adicional utilizando +: p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp)) p + geom_point() + geom_smooth() El mensaje de la consola de R nos dice que la función geom_smooth() está utilizando un método llamado gam, que en este caso significa que se ajusta a un modelo aditivo generalizado. Esto sugiere que tal vez haya otros métodos en geom_smooth(). Podemos intentar agregar method = &quot;lm&quot; (para “modelo lineal”) como un argumento para geom_smooth(): p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp)) p + geom_point() + geom_smooth(method=&quot;lm&quot;) Se puede agregar al mapeo del color de la línea el continente y del relleno de los puntos (fill) también el continente para obtener una gráfica que nos dé una idea más general de como se tiene esta relación por continente. p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent)) p + geom_point(size = 0.5) + geom_smooth(method=&#39;loess&#39;) + scale_x_log10() 3.8 Un histograma de las muertes en Iraq Iraq Body Count (IBC) mantiene la base de datos pública más grande sobre muertes violentas de civiles desde la invasión en Iraq del 2003. Los datos de IBC provienen de informes de medios cruzados, de hospitales, morgue, ONG y cifras o registros oficiales. Para mayor información puedes visitar https://www.iraqbodycount.org/. Los datos los leemos con la función read_csv() de la librería readr: ibc &lt;- read_csv(&quot;datos/ibc-incidents-2016-8-8.csv&quot;) ibc %&gt;% sample_n(10) %&gt;% knitr::kable(&quot;html&quot;) %&gt;% kableExtra::kable_styling(font_size = 10) IBC_code Start_Date End_Date Time Location Target Weapons Deaths_recorded Sources m2046 2-Sep-13 2-Sep-13 PM 13 Street, Al-Bayaa, southwest Baghdad employee at the Ministry of Education in front of his home gunfire 1 AIN 2 Sep, Sotaliraq 2 Sep d2329 9-Feb-07 9-Feb-07 NA near Al-Shimal Garage, Mosul NA explosive device 1 MO 09 Feb, WP 04 Apr (MoH) k7919 29-Oct-07 29-Oct-07 8:00-9:00 AM Ishbilliyah Square, central Baquba police recruits awaiting training suicide bomber 28 AFP 31 Oct, NINA 29 Oct, REU 29 Oct, DPA 29 Oct s1254 22-Apr-15 22-Apr-15 NA Al-Mansour, west Baghdad civilian in Al-Mansour drive-by shooting 1 NINA 22 Apr h0379 25-Feb-14 25-Feb-14 NA Saba’ Abkar, north Baghdad Mohamed Taha Mohamed, Iraqi Sports Channel Director car in hit-and-run 0 AIN 25 Feb, INN 25 Feb k18626 13-Mar-12 14-Mar-12 NA Hoswa, Karma, east of Falluja civilian car roadside bomb 1 AKnews 14 Mar, Al-Shorfa 13 Mar k1708 1-Aug-05 1-Aug-05 PM west Baghdad Shaikh Akil al-Ma’adhidi, a cleric from al-Muhajirin mosque, brother also killed gunfire 2 Al-Jaz 02 Aug, AFP 02 Aug k10200 15-Apr-08 15-Apr-08 NA al-Zahraa, east Mosul ‘operator for a private electricity generator’ gunfire 1 VOI 15 Apr m3013 2-Dec-13 2-Dec-13 AM Refaq, east Mosul civilian in his car magnetic bomb 1 AIN 2 Dec, NINA 2 Dec k3289e 18-Jun-06 18-Jun-06 NA Al-Sha’b, Baghdad bodies found shot, tortured gunfire, executed, tortured 1 Al-Shar 18 Jun, DPA 18 Jun Primero filtramos los incidentes en los que hubo al menos cinco fatalidades: ibc_fatalidades &lt;- ibc %&gt;% filter(Deaths_recorded &gt;= 5) Una forma fácil de dibujar un histograma es utilizando la geometría geom_histogram(): ggplot(ibc_fatalidades, aes(x=Deaths_recorded)) + geom_histogram() + scale_x_log10() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.9 Inglehart–Welzel: un mapa cultural del mundo Los teóricos de la modernización de Karl Marx a Daniel Bell han sostenido que el desarrollo económico trae cambios culturales penetrantes. Pero otros, desde Max Weber hasta Samuel Huntington, han afirmado que los valores culturales son una influencia duradera y autónoma sobre la sociedad. En un artículo de la ciencia política, los autores Inglehart y Welzel de la Universidad de Michigan, afirman que el desarrollo económico está vinculado a cambios sistemáticos en los valores culturales. Utilizando los datos de la encuesta de valores mundiales WVS (World Values Survey), crearon dos índices: uno que pone énfasis en valores tradicionales y otro que pone énfasis en valores de supervivencia. Características de valores tradicionales en una sociedad: fuerte sentimiento de orgullo nacional le da más importancia a que un niño aprenda obediencia y fé religiosa en lugar de independencia y determinación el aborta nunca es justificada fuerte sentido de orgullo nacional favorece más el respeto por la autoridad. Los valores seculares o racionales enfatizan lo opuesto. Características de valores de supervivencia en una sociedad: le da prioridad a la economía sobre la calidad de vida se describe como no muy feliz aún no ha firmado o jamás firmaría una petición la homosexualidad nuna es justificada se debe ser muy cuidadoso al confiar en las personas. Los valores de autoexpresión enfatizan lo opuesto. Ronald Inglehart en su artículo de 1971 The silent revolution in Europe. Intergenerational change in post-industrial societies. publicado en el American Political Science Review, propone una medida de los valores postmaterialistas de una sociedad. Esta medida se conoce como índice post-materialista de Inglehart (4-item) . La siguiente pregunta de la encuesta es el punto de partida para medir el materialismo o el post-materialismo: “Si tuvieras que elegir entre las siguientes cosas, ¿cuáles son las dos que te parecen más deseables?” Mantener el orden en la nación. Dando a la gente más voz en importantes decisiones políticas. La lucha contra el aumento de los precios. Proteger la libertad de expresión. La medida se basa entonces en la observación de que dos de las cuatro opciones, la primera y la tercera, se consideran como “preferencia hacia el valor adquisitivo en relación con la protección y adquisición de bienes”. Si se eligen las dos opciones postmaterialistas, entonces la puntuación es 3. Si se elige sólo una opción post-materialista, entonces la puntuación es 2, y de lo contrario es 1. Como todas las opciones podrían ser deseables, la medida se relaciona con la “prioridad relativa” de las elecciones materialistas sobre la segunda y cuarta y aborda las concesiones que típicamente conllevan las decisiones políticas. La conceptualización del postmaterialismo a lo largo de un continuo unidimensional está cerca del concepto de la “jerarquía de necesidades” propuesta por Maslow. library(tidyverse) factores_inglehart &lt;- read_csv(file = &quot;datos/factores_inglehart.csv&quot;) glimpse(factores_inglehart) #&gt; Observations: 60 #&gt; Variables: 6 #&gt; $ country_code &lt;int&gt; 112, 12, 152, 156, 158, 170, 196, 218,... #&gt; $ country &lt;chr&gt; &quot;Belarus&quot;, &quot;Algeria&quot;, &quot;Chile&quot;, &quot;China&quot;... #&gt; $ region &lt;chr&gt; &quot;Eastern Europe&quot;, &quot;Northern Africa&quot;, &quot;... #&gt; $ reg &lt;chr&gt; &quot;Europe &amp; Eurasia&quot;, &quot;Middle East &amp; Nor... #&gt; $ traditional_secular &lt;dbl&gt; 0.91766, -0.68003, 0.14525, 1.45307, 1... #&gt; $ survival_selfexpression &lt;dbl&gt; -0.3187, -0.3300, 1.5769, -0.5487, 0.9... 3.9.1 Creando un ggplot Para graficar factores_inglehart, ejecuta este código para poner survival_selfexpression en el eje x (eje horizontal) y traditional_secular en el eje y (eje vertical): ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular)) 3.9.2 Mapeos: Aesthetics “The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey En la gráfica de abajo, un grupo de puntos (en rojo) parece estar fuera de la tendencia lineal. Estos países tienen menores valores de supervivencia de lo que esperaríamos de acuerdo a sus mayores valores de tradicionalismo. Podemos formular la hipótesis de que se trata de países latinoamericanos. Una forma de probar esta hipótesis es con la variable reg. La variable reg del conjunto de datos factores_inglehart clasifica a los países de acuerdo a su región geográfica. Podemos agregar una tercera variable, como reg, a un diagrama de dispersión bidimensional asignándolo a un aesthetic o mapeo. Un mapeo es una propiedad visual de los objetos en la gráfica. Un mapeo incluye cosas como el tamaño, la forma o el color de los puntos. Puede mostrar un punto (como el que se muestra a continuación) de diferentes maneras cambiando los valores de sus propiedades de mapeos. Aquí cambiamos los niveles de tamaño, forma y color de un punto para hacer que el punto sea pequeño, triangular o azul: Podemos transmitir información sobre los datos mapeando los aesthetics en la gráfica a las variables del data frame. Por ejemplo, podemos asignar los colores de los puntos a la variable reg para revelar la región de cada país. ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular, color=reg)) Para asignar una característica a una variable, asociamos el nombre del mapeo al nombre de la variable dentro de aes(). ggplot2 asignará automáticamente un nivel único de dicha característica (o mapeo) a cada valor único de la variable, un proceso conocido como escalamiento. ggplot2 también agregará una leyenda que explique qué niveles corresponden a qué valores. También podríamos agregar etiquetas: 3.9.2.1 Objetos geométricos ¿En qué se parecen las siguiente dos gráficas? Ambas gráficas contienen la misma variable x, la misma variable y, y ambas describen los mismos datos. Pero las gráficas no son idénticas. Cada una utiliza un objeto visual diferente para representar los datos. En la sintaxis de ggplot2, decimos que usan diferentes geoms. Un geom es un objeto geométrico que una gráfica utiliza para representar a los datos. La gente a menudo describe las gráficas por el tipo de geometría que usa la gráfica. Por ejemplo, las gráficas de barras usan geometrías de barras, los gráficos de línea utilizan geoms de línea, los boxplots usan geoms de boxplot, y así sucesivamente. Los diagramas de dispersión rompen la tendencia; Utilizan la geometría de punto. La gráfica de la izquierda utiliza el punto geom, y la gráfica de la derecha utiliza el geom de smooth, una línea ajustada a los datos. Para hacer las gráficas mostradas arriba se puede utilizar el siguiente código. #izquierda ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular)) #derecha ggplot(data = factores_inglehart) + geom_smooth(mapping = aes(x = survival_selfexpression, y = traditional_secular), method = &quot;loess&quot;) Cada función geom en ggplot2 toma un argumento mapping. Sin embargo, no todas las propiedades de aesthetics funciona con cada geom. Podríamos cambiar la forma de un punto, pero no la “forma” de una línea. Por otro lado, podríamos establecer el tipo de línea de una línea. geom_smooth() dibujará una línea diferente, con un tipo de línea diferente, para cada valor único de la variable que se asigna al tipo de línea. ggplot(data = factores_inglehart) + geom_smooth(mapping = aes(x = survival_selfexpression, y = traditional_secular, linetype = reg), method = &quot;loess&quot;, se = F, span = 1) Aquí geom_smooth() separa los países en líneas basándose en su valor de reg (región geográfica). Podemos superponer las líneas encima de los datos sin procesar y luego coloreándolo todo de acuerdo a reg. Para mostrar varios geoms en la misma gráfica, agregamos varias funciones geom a ggplot(): ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular)) + geom_smooth(mapping = aes(x = survival_selfexpression, y = traditional_secular), method = &quot;loess&quot;) Este código genera la misma gráfica que el código anterior: ggplot(data = factores_inglehart, mapping = aes(x = survival_selfexpression, y = traditional_secular)) + geom_point() + geom_smooth(method = &quot;loess&quot;) Si colocan asignaciones en una función geom, ggplot2 las tratará como asignaciones locales para cada capa, de tal forma que usará estas asignaciones para extender o sobrescribir las asignaciones globales para esa capa solamente. Esto hace posible visualizar elementos diferentes en diferentes capas. ggplot(data = factores_inglehart, mapping = aes(x = survival_selfexpression, y = traditional_secular)) + geom_point(mapping = aes(color = reg)) + geom_smooth(method = &quot;loess&quot;) 3.10 Poniendo todo junto El Billboard Hot 100 es un ranking semanal publicado en Estados Unidos y es utilizado en la industria de la música como una medida del rendimiento de las canciones en ventas y en streaming en el país. Por ejemplo, en la página https://www.billboard.com/charts/hot-100/2000-03-18 se puede consultar el chart en la semana del 18 de marzo del año 2000. Con el siguiente código podemos descargar los datos del Billboard Hot 100 para cada semana para obtener un conjunto de datos que vamos a utilizar más adelante. suppressPackageStartupMessages({ library(tidyverse) library(rvest) library(lubridate) }) extract_song_info &lt;- function(html_row) { node_primary &lt;- html_row %&gt;% html_node(css = &#39;.chart-row__primary&#39;) node_secondary &lt;- html_row %&gt;% html_node(css = &#39;.chart-row__secondary&#39;) song_features &lt;- c( &#39;.chart-row__history--rising&#39;, &#39;.chart-row__bullet&#39;, &#39;.chart-row__history--falling&#39;, &#39;.chart-row__award-indicator&#39;, &#39;.chart-row__new-indicator&#39;, &#39;.chart-row__history--steady&#39;) feat_search &lt;- map(.x = song_features, .f = function(y) { node_primary %&gt;% html_nodes(css = y) }) song_node &lt;- node_primary %&gt;% html_node(css = &#39;.chart-row__main-display&#39;) info_node &lt;- song_node %&gt;% html_node(css = &#39;.chart-row__container&#39;) song_name_node &lt;- info_node %&gt;% html_node(css = &#39;.chart-row__song&#39;) song_artist_node &lt;- info_node %&gt;% html_node(css = &#39;.chart-row__artist&#39;) song_stats &lt;- node_secondary %&gt;% html_node(css = &#39;.chart-row__stats&#39;) last_week &lt;- song_stats %&gt;% html_node(css = &#39;.chart-row__last-week .chart-row__value&#39;) %&gt;% html_text() peak_position &lt;- song_stats %&gt;% html_node(css = &#39;.chart-row__top-spot .chart-row__value&#39;) %&gt;% html_text() wks_on_chart &lt;- song_stats %&gt;% html_node(css = &#39;.chart-row__weeks-on-chart .chart-row__value&#39;) %&gt;% html_text() current_week_rank &lt;- node_primary %&gt;% html_node(css = &#39;.chart-row__current-week&#39;) %&gt;% html_text() artist &lt;- song_artist_node %&gt;% html_text() %&gt;% str_replace_all(&#39;\\n&#39;,&#39;&#39;) name &lt;- song_name_node %&gt;% html_text() %&gt;% str_replace_all(&#39;\\n&#39;,&#39;&#39;) song &lt;- tibble(current_week_rank = as.character(current_week_rank), name = str_trim(as.character(name)), artist = str_trim(as.character(artist)), rising = length(feat_search[[1]]) &gt; 0, steady = length(feat_search[[6]]) &gt; 0, falling = length(feat_search[[3]]) &gt; 0, gains_performance = length(feat_search[[2]]) &gt; 0, award = length(feat_search[[4]]) &gt; 0, hot_debut = length(feat_search[[5]]) &gt; 0, last_week = as.character(last_week), peak_position = as.character(peak_position), wks_on_chart = as.character(wks_on_chart) ) cat(sprintf(&#39;%-3s\\t %-40s\\t%s\\n&#39;, song$current_week_rank, song$artist, song$name)) song } billboard_weekchart &lt;- function(fecha){ cat(sprintf(&#39;\\n\\n Fecha: \\t%s\\n\\n&#39;, toString(fecha))) # url base del Billboard Hot 100 base_url &lt;- &quot;http://www.billboard.com/charts/hot-100/&quot; current_url &lt;- paste0(base_url, fecha) webpage &lt;- tryCatch( { Sys.sleep(5) read_html(current_url); }, error=function(cond) { message(&quot;Error: Webpage did not respond succesfully.&quot;) message(cond) cat(&#39;\\n\\n&#39;) return(NA) }, finally={ cat(&#39;\\n&#39;) } ) if(length(webpage) &gt; 1){ chart &lt;- html_nodes(webpage, css=&#39;.chart-data&#39;) rows &lt;-html_nodes(chart, css=&#39;.chart-row&#39;) week_songs &lt;- map_df(.x = rows, .f = extract_song_info) fecha_df &lt;- data.frame(fecha=rep(fecha,nrow(week_songs))) week_songs &lt;- cbind(fecha_df, week_songs) }else{ write(toString(fecha), file=&quot;/home/andreu/scripts/billboard/missing.txt&quot;, append = T) Sys.sleep(60) week_songs &lt;- NULL } if(length(week_songs) &gt; 0){ write_csv(x = week_songs, path = paste0(&quot;/home/andreu/scripts/billboard/data/&quot;,fecha,&quot;.csv&quot;)) val = T }else{ val = F } val } billboard_alltime &lt;- function(start_date = ymd(&#39;1958-08-04&#39;), current_date = ymd(&#39;2018-02-03&#39;)){ file.create(&quot;/home/andreu/scripts/billboard/missing.txt&quot;) fechas &lt;- seq(start_date, current_date, by = &#39;1 week&#39;) alltime_songs &lt;- map_lgl(.x = fechas, .f = billboard_weekchart) alltime_songs } alltime_songs &lt;- billboard_alltime(start_date = ymd(&#39;2018-02-03&#39;)) paths &lt;- dir(&quot;/home/andreu/scripts/billboard/data/&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) billboard &lt;- map_df(paths, read_csv, col_types=paste(rep(&#39;c&#39;, 13), collapse=&#39;&#39;)) No es necesario entender a profundidad el código utilizado para parsear el html de cada página de Billboard, sin embargo, es interesante ver el uso de las funciones map, map_lgl, y map_df en el fragmento de código anteior. La función map se utiliza en la llamada a la primera función extract_song_info. Primero se definen tags del html que se necesitan parsear y posteriormente se utiliza map para buscar cada uno dentro del texto plano en html. La función map_df se utiliza para aplicar cada elemento (rengón) del html en una fecha dada la función extract_song_info y el resultado de aplicar esta función a cada renglón del html da como resultado un data frame con los datos de la información de cada canción para cada fecha. Posteriormente, este conjunto de datos se guarda en formato csv. La función map_lgl aplica la función billboard_weekchart a cada elemento de la lista de fecha y regresa un vector lógico que indica si el parseo del html tuvo éxito para cada una de las fechas. La función map_df al final del fragmento de código lee todos los archivos correspondientes a las fechas utilizadas y como resultado junta todos los data frames en uno solo. Una muestra de los datos obtenidos se puede ver en la siguiente tabla: billboard %&gt;% sample_n(10) %&gt;% knitr::kable(&quot;html&quot;) %&gt;% kableExtra::kable_styling(font_size = 10) fecha current_week_rank name artist rising steady falling gains_performance award hot_debut last_week peak_position wks_on_chart 2010-06-21 23 Undo It Carrie Underwood TRUE FALSE FALSE TRUE FALSE FALSE 25 23 7 1996-11-18 59 Knocks Me Off My Feet Donell Jones TRUE FALSE FALSE TRUE FALSE FALSE 67 59 4 2007-04-23 19 Go Getta Young Jeezy Featuring R. Kelly FALSE FALSE TRUE FALSE FALSE FALSE 18 18 13 1996-07-01 13 Twisted Keith Sweat TRUE FALSE FALSE TRUE FALSE FALSE 21 13 3 1968-04-22 41 She’s Lookin’ Good Wilson Pickett TRUE FALSE FALSE FALSE FALSE FALSE 45 41 3 1980-04-21 18 Hurt So Bad Linda Ronstadt TRUE FALSE FALSE FALSE FALSE FALSE 23 18 3 1965-12-27 3 I Got You (I Feel Good) James Brown And The Famous Flames FALSE TRUE FALSE FALSE FALSE FALSE 3 3 8 2006-01-16 45 Heard ’Em Say Kanye West Featuring Adam Levine FALSE FALSE TRUE FALSE FALSE FALSE 40 26 13 1967-04-24 8 The Happening The Supremes TRUE FALSE FALSE FALSE FALSE FALSE 11 8 4 1980-04-14 75 Steal Away Robbie Dupree TRUE FALSE FALSE FALSE FALSE FALSE 85 75 2 Veamos de qué tipos son cada una de las columnas en los datos. Podemos usar nuevamente la función glimpse: glimpse(billboard) #&gt; Observations: 310,500 #&gt; Variables: 13 #&gt; $ fecha &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08... #&gt; $ current_week_rank &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... #&gt; $ name &lt;chr&gt; &quot;Poor Little Fool&quot;, &quot;Patricia&quot;, &quot;Splish Spla... #&gt; $ artist &lt;chr&gt; &quot;Ricky Nelson&quot;, &quot;Perez Prado And His Orchest... #&gt; $ rising &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ steady &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ falling &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ gains_performance &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ award &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ hot_debut &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ last_week &lt;chr&gt; &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;-... #&gt; $ peak_position &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... #&gt; $ wks_on_chart &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... Supongamos que deseamos hacer una gráfica de barras del número de semanas en el top 10 para los 20 artistas que más semanas han permanecido en el top 10. artistas_top10 &lt;- billboard %&gt;% filter(current_week_rank &gt;= 10) %&gt;% group_by(artist) %&gt;% summarise(num_semanas_top_10 = n()) %&gt;% arrange(desc(num_semanas_top_10)) %&gt;% top_n(20, wt = num_semanas_top_10) Para hacer la gráfica con ggplot debemos primero ordenar los artistas de manera descendente por el número de semanas en el top 10. Para esto utilizamos la función de fct_reorder del paquete forcats. artistas_top10$artist &lt;- forcats::fct_reorder(.f = artistas_top10$artist, .x = artistas_top10$num_semanas_top_10, .desc = T) Por último, hacemos la gráfica indicando a geom_bar que la transformación estadística que debe usar es la de identity, es decir, la longitud de la barra corresponde al valor absoluto de la variable num_semanas_top_10: ggplot(artistas_top10, aes(x = artist, y = num_semanas_top_10)) + geom_bar(stat = &#39;identity&#39;) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) 3.11 Tarea Utiliza los datos del Billboard Hot 100 para contestar las siguientes preguntas: ¿Cuáles son los 10 artistas que han tenido más #1’s en la historia de Billboard? ¿Cuáles son los 10 artistas que han tenido más #1’s en los últimos 10 años? Realiza una gráfica de barras para responder a la pregunta. ¿Cómo se ha comportado el número promedio de semanas que una canción ha estado en el #1 a través de la historia? ¿Existe alguna relación entre el número de presentaciones en vivo (gains_performance) y el número de semanas que permance una canción en #1 desde 1980? Para algunos últimos singles que han permanecido más semanas en el Hot 100 en algunos meses realiza una gráfica de su posición en el tiempo semana a semana. Puedes elegir alguna fecha que te interese, la fecha de tu cumpleaños o la fecha en que estuvo en #1 tu canción favorita. Referencias "],
["teorema-del-limite-central.html", "Clase 4 Teorema del Límite Central 4.1 La distribución de la media 4.2 ¿De dónde proviene la distribución normal? 4.3 Otras observaciones 4.4 Diagramas de caja y brazos 4.5 Gráficas de cuantiles teóricos 4.6 Gráficas de cuantiles para un conjunto de datos 4.7 Gráficas qq-normales 4.8 El TLC y errores estándar 4.9 Ejemplo 4.10 Tarea", " Clase 4 Teorema del Límite Central .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } A continuación discutiremos el Teorema del Límite Central (CLT), el cual nos ayuda a realizar cálculos importantes relacionados con la probabilidad de las cosas. Se utiliza con frecuencia en la ciencia para probar hipótesis estadísticas. Para utilizarlo, tenemos que hacer diferentes supuestos. Sin embargo, si los supuestos son verdaderos, entonces podemos calcular probabilidades exactas de eventos mediante el uso de una fórmula matemática muy sencilla. 4.1 La distribución de la media El Teorema del Límite Central El CLT es uno de los resultados matemáticos más utilizados en la ciencia. Nos dice que cuando el tamaño de la muestra es grande, la media \\(\\bar{Y}\\) de una muestra aleatoria sigue una distribución normal con centro en la media poblacional \\(\\mu_Y\\) y con desviación estándar igual a la desviación estándar de la población \\(\\sigma_Y\\) dividida por la raíz cuadrada del tamaño de la muestra \\(\\sqrt{N}\\). Nos referimos a la desviación estándar de la distribución de una variable aleatoria como el error estándar de la variable aleatoria. Si tomamos muchas muestras de tamaño \\(N\\), entonces la cantidad: \\[ \\frac{\\bar{Y} - \\mu}{\\sigma_Y/\\sqrt{N}} \\] se aproxima con una distribución normal con centro en 0 y con desviación estándar 1. Ahora nos interesa la diferencia entre dos medias muestrales. Aquí, de nuevo, un resultado matemático nos puede ayudar. Si tenemos dos variables aleatorias \\(X\\) y \\(Y\\) con medias \\(\\mu_X\\) y \\(\\mu_Y\\) y varianzas \\(\\sigma_X\\) y \\(\\sigma_Y\\) respectivamente, entonces tenemos el siguiente resultado: la media de la suma \\(Y + X\\) es la suma de las medias \\(\\mu_Y + \\mu_X\\). Esto implica que la media de \\(Y - X = Y + aX\\) con \\(a = -1\\), es \\(\\mu_Y - \\mu_X\\). Sin embargo, el siguiente resultado quizás no sea tan intuitivo. Si \\(X\\) y \\(Y\\) son independientes entre sí, entonces la varianza de \\(Y + X\\) es la suma de las varianzas \\(\\sigma_Y^2 + \\sigma_X^2\\). Esto implica que la varianza de la diferencia \\(Y - X\\) es la varianza de \\(Y + aX\\) con \\(a = -1\\) que es \\(\\sigma^2_Y + a^2\\sigma_X^2 = \\sigma ^ 2_Y + \\sigma_X ^ 2\\). Así que la varianza de la diferencia es también la suma de las varianzas. Si esto parece un resultado contraintuitivo, recordemos que si \\(X\\) y \\(Y\\) son independientes entre sí, el signo realmente no importa. Finalmente, otro resultado útil es que la suma de variables normales (mutuamente independientes) es otra vez normal. El cociente \\[ \\frac{(\\bar{Y}-\\bar{X}) - (\\mu_Y - \\mu_X)}{\\sqrt{\\frac{\\sigma_X^2}{M} + \\frac{\\sigma_Y^2}{N}}} \\] se aproxima por una distribución normal centrada en 0 y con desviación estándar 1. Usando esta aproximación, el cálculo de probabilidades es simples porque conocemos la proporción de la distribución bajo cualquier valor. Por ejemplo, sólo el 5% de estos valores son mayores que 2 (en valor absoluto): pnorm(-2) + (1 - pnorm(2)) #&gt; [1] 0.0455 4.2 ¿De dónde proviene la distribución normal? En 1894, Francis Galton inventó lo que ahora conocemos como tablero de Galton, para ilustrar el Teorema del Límite Central, y en particular, que la distribución binomial es una aproximación a la distribución normal. El tablero consta de una tablero vertical con varias filas de clavos acomodados en forma de arreglo triangular. En la parte inferior hay varias cubetas para cada posible camino que forman los clavos en el tablero. Se deja caer canicas desde la parte superior, las cuales van botando, rebotando y saltando, aleatoriamente, y van depositándose, a medida que caen, en las cubetas de la parte inferior. Las canicas chocarán con el primer clavo teniendo una probabilidad de \\(1/2\\) de ir a la izquierda o la derecha. A medida que caen, cada canica tiene más caminos a donde ir, es decir más posibilidades para desviarse a la izquiera o a la derecha. A lo largo de esta estructura, las canicas toman caminos aleatorios hasta caer en alguna de las cubetas. Es consecuencia de las leyes del universo que está sostenido por una tela sobre la cual subyace lo aleatorio, que las canicas caen con mayor probabilidad en las cubetas que están al centro, mientras que la probabilidad de que caigan en cubetas más alejadas es cada vez menor cuando la canica se aleja más y más de la cubeta del centro. Nota: Puedes ver el script para hacer la simulación del tablero de Galton aquí. La primera versión de este teorema fue postulada por el matemático francés Abraham De Moivre que, en un notable artículo publicado en 1733, usó la distribución normal para aproximar la distribución del número de soles resultante de muchos lanzamientos de una moneda justa. Este hallazgo estaba muy por delante de su tiempo y permaneció en el olvido hasta que el famoso matemático francés Pierre-Simon Laplace lo rescató de la oscuridad en su monumental obra Théorie analytique des probabilités, publicada en 1812. Laplace extendió el hallazgo de De Moivre al aproximar la distribución binomial en general con la distribución normal. El teorema en su forma más general fue demostrado por primera vez por el príncipe de las matemáticas, Carl Friedrich Gauss, en 1813. Hoy en día es conocida en su honor como distribución Gaussiana, cuando en su tiempo no era más que la ley del error. Supongamos que \\(x\\) y \\(y\\) son errores independientes cometidos al azar cuando se han hecho dos mediciones independientemente una de la otra. De tal forma que se cumple que \\[ g(r) \\Delta x \\Delta y = f(x) \\Delta x f(y) \\Delta y, \\] por lo cual \\[ g(r) = f(x)f(y). \\] Esto significa nada más que la magnitud del error \\(g(r)\\) es el producto de las magnitudes de los errores en \\(x\\) y \\(y\\) de forma independiente. Las coordenadas \\(x\\),\\(y\\) son tales que \\[ x = r \\cos(\\theta), \\; \\;\\; y = r\\mbox{sen}(\\theta). \\] Sabemos que \\[\\begin{eqnarray*} \\dfrac{dx}{d\\theta} &amp;=&amp; -r\\mbox{sen}(\\theta)\\\\ \\dfrac{dy}{d\\theta} &amp;=&amp; r \\cos{(\\theta)} \\end{eqnarray*}\\] Derivando con respecto a \\(\\theta\\): \\[\\begin{eqnarray*} 0 &amp;=&amp; \\dfrac{d f(x)}{d \\theta} f(y) + \\dfrac{d f(y)}{d \\theta} f(x)\\\\ &amp;=&amp; \\dfrac{df}{dx}\\cdot \\dfrac{d x}{d \\theta}\\cdot f(y) + \\dfrac{d f}{d y}\\cdot\\dfrac{d y}{d\\theta} \\cdot f(x) \\\\ &amp;=&amp; -r f^\\prime(x) \\mbox{sen}(\\theta)f(y) + f^\\prime(y)\\cdot r \\cos(\\theta) f(x)\\\\ &amp;=&amp; -y f^\\prime(x)f(y) + xf^\\prime(y)f(x). \\end{eqnarray*}\\] Por lo tanto, \\[ yf^\\prime(x)f(y) = x f^\\prime(y)f(x). \\] Se tiene que \\[ \\dfrac{f^\\prime(x)}{f(x)x} = \\dfrac{f^\\prime(y)}{f(y)y}, \\] para toda \\(x\\) y \\(y\\). Como \\(x\\) y \\(y\\) son mediciones arbitrarias, esto implica que \\[ \\dfrac{f^\\prime(x)}{f(x)x} \\] debe ser constante. Por lo tanto, \\[ \\displaystyle{\\int{\\dfrac{f^\\prime(x)}{f(x)x}}\\,dx = \\int{c \\,dx}}. \\] Multiplicando por \\(x\\), \\[ \\displaystyle{\\int{\\dfrac{f^\\prime(x)}{f(x)}}\\,dx = \\int{cx\\, dx}}. \\] Por lo cual, \\[ \\mbox{ln}f(x) = c\\cdot \\dfrac{x^2}{2} + c^\\prime. \\] 4.2.1 ¿Qué signo tiene c? Vemos que \\[ f(x) = Ae^{c\\frac{x^2}{2}}, \\] donde \\(A=e^{c^\\prime}\\). Como \\(f(x)\\) es la función de densidad de este fenómeno de errores independientes entonces se debe cumplir que: \\[ 1 = \\displaystyle{\\int f(x)\\, dx}, \\] y podemos concluir que \\(c&lt;0\\), para que \\(f(x)\\) pueda ser función de densidad. Integramos: \\[ A \\displaystyle{\\int{e^{c\\cdot \\frac{x^2}{2}}}\\, dx}. \\] Sea \\(u=\\sqrt{-\\dfrac{c}{2}}x\\), entonces \\(du = \\sqrt{-\\dfrac{c}{2}}\\,dx\\). Por lo cual, \\[ 1 = A\\sqrt{-\\dfrac{2}{c}} \\displaystyle{\\int_{-\\infty}^{\\infty}{e^{-u^2}}\\,du = A \\sqrt{-\\dfrac{2}{c}} \\cdot \\sqrt{\\pi}}. \\] Para obtener lo anterior, se desea demostrar que \\[ \\boxed{\\int_0^\\infty{e^{-x^2}dx} = \\dfrac{\\sqrt{\\pi}}{2}.} \\] Sea \\[ I = \\int_{-\\infty}^\\infty{e^{-x^2}dx}, \\] entonces \\[ I^2=\\left(\\int_{-\\infty}^\\infty{e^{-x^2}dx}\\right)\\left(\\int_{-\\infty}^\\infty{e^{-y^2}dy}\\right)=\\int_{-\\infty}^\\infty{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}dxdy}}. \\] Si \\(x=r\\mbox{cos}(\\theta)\\) y \\(y=r\\mbox{sen}(\\theta)\\) entonces \\(x^2+y^2=r^2\\) y se puede demostrar que \\(dxdy=rd\\theta dr\\). Por lo tanto, \\[\\begin{eqnarray*} I^2&amp;=&amp;\\int_{-\\infty}^\\infty{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}dxdy}}\\\\ &amp;=&amp;\\int_{0}^\\infty{\\int_{0}^{2\\pi}{re^{-r^2}d\\theta dr}}\\\\ &amp;=&amp;-\\pi\\int_{0}^{\\infty}{-2re^{-r^2}dr}\\\\ &amp;=&amp;-\\pi e^{-r^2}{\\biggr\\rvert_{0}^{\\infty}}\\\\ &amp;=&amp;\\pi. \\end{eqnarray*}\\] Por lo cual, \\(I=\\sqrt{\\pi}\\). Como \\(e^{-x^2}\\) es una función simétrica alrededor de \\(0\\), entonces se tiene, finalmente, que \\[ \\int_{0}^{\\infty}{e^{-x^2}dx}=\\dfrac{1}{2}\\int_{-\\infty}^{\\infty}{e^{-x^2}dx}=\\dfrac{\\sqrt{\\pi}}{2}. \\] Finalmente, \\[ 1 =A \\sqrt{-\\dfrac{2}{c}} \\cdot \\sqrt{\\pi}, \\] y despejando \\(A\\), obtenemos que \\[ A = \\sqrt{-\\dfrac{c}{2\\pi}}. \\] Sean \\(\\mu\\), el valor esperado de \\(X\\), y \\(\\sigma^2\\) la varianza de \\(X\\), \\(E(X)\\) y \\(V(X)\\), respectivamente. Vemos que \\[\\begin{eqnarray*} E(X) &amp;=&amp; \\displaystyle{\\int_{-\\infty}^{\\infty}{Ax e^{c\\frac{x^2}{2}}}\\, dx}\\\\ &amp;=&amp; \\sqrt{-\\dfrac{c}{2\\pi}}\\displaystyle{\\int_{-\\infty}^\\infty{xe^{c\\frac{x^2}{2}}}\\,dx}. \\end{eqnarray*}\\] Por lo tanto, \\[ E(X) = -\\dfrac{1}{c} \\sqrt{-\\dfrac{c}{2\\pi}}\\,e^{c\\frac{x^2}{2}}{\\biggr\\rvert_{-\\infty}^{\\infty}}=0. \\] Ahora bien, \\[ E(X^2) = V(X). \\] Tenemos que \\[ E(X^2) = \\sqrt{-\\dfrac{c}{2\\pi}} \\displaystyle{\\int_{-\\infty}^\\infty{x^2e^{c\\frac{x^2}{2}}}\\,dx}. \\] Integrando por partes (con \\(u=x\\) y \\(dv = xe^{c\\frac{x^2}{2}}\\,dx\\)) ahora obtenemos \\[\\begin{eqnarray*} \\sigma^2 = V(X) &amp;=&amp; \\sqrt{-\\dfrac{c}{2\\pi}} \\left(\\dfrac{1}{c}xe^{cx^2/2}{\\biggr\\rvert_{-\\infty}^{\\infty}} - \\dfrac{1}{c}\\displaystyle{\\int_{-\\infty}^{\\infty}{e^{c{x^2/2}}\\,dx}}\\right) \\\\ &amp;=&amp; \\sqrt{-\\dfrac{c}{2\\pi}} \\left(-\\dfrac{1}{c}\\displaystyle{\\int_{-\\infty}^{\\infty}{e^{cx^2/2}}\\,dx}\\right) \\\\ &amp;=&amp; \\sqrt{-\\dfrac{c}{2\\pi}} \\cdot \\left(\\dfrac{1}{c}\\right) \\cdot \\sqrt{-\\dfrac{2\\pi}{c}}. \\end{eqnarray*}\\] Por lo cual, \\[ c = - \\dfrac{1}{\\sigma^2}. \\] Finalmente, la distribución de \\(X\\) con media \\(0\\) y varianza \\(\\sigma^2\\) es \\[ f(x) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\,e^{-\\frac{1}{2\\sigma^2}x^2}. \\] Si ahora la media es \\(\\mu\\), entonces \\[ f(x) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\,e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}. \\] 4.3 Otras observaciones Otras propiedades de esta distribución se pueden obtener buscando los puntos críticos de su función de densidad \\[ f(x) = Ae^{cx^2/2}. \\] La primera derivada es \\[ f^\\prime(x) = A e^{cx^2/2}\\cdot cx. \\] Por lo que \\(f^\\prime(x)=0\\) cuando \\(x=0\\). La segunda derivada es \\[ f^{\\prime\\prime}(x) = cA\\left(e^{cx^2/2}+xe^{cx^2/2}\\cdot cx\\right). \\] Por lo tanto, \\[\\begin{eqnarray*} f^{\\prime\\prime}(0) &amp;=&amp; cA \\\\ &amp;=&amp; c\\sqrt{-\\dfrac{c}{2\\pi}} \\\\ &amp;=&amp; \\sqrt{\\dfrac{1}{2\\pi\\sigma^2}} &gt; 0. \\end{eqnarray*}\\] Por lo tanto, si \\(\\sigma^2 = 1\\), entonces el máximo de \\(f(x)\\) se alcanza en \\(x=0\\), que coincide con la media, y el valor de \\(f\\) en \\(x=0\\) es \\[ \\sqrt{\\dfrac{1}{2\\pi}} \\approx 0.3989. \\] Ahora bien, \\(f^{\\prime\\prime}(0) = 0\\) si y sólo si \\[ e^{cx^2/2} = -cx^2 e^{cx^2/2}, \\] que ocurre si y sólo si \\[ x = \\pm \\sigma. \\] Esto quiere decir que \\(f(x)\\) tiene puntos de inflexión en \\(-\\sigma\\) y \\(\\sigma\\). 4.4 Diagramas de caja y brazos Los diagramas de caja y brazos son muy populares, e intentan mostrar gráficamente algo similar al resumen de cinco números de Tukey: Imagen de Wikipedia. Como vemos en la imagen superior el método muestra la mediana como una línea horizontal (medida de tendencia central), los bordes de la caja indican los cuartiles inferior y superior (o cuantiles 0.25 y 0.75). La distancia entre estos dos se conoce como rango intercuartílico o IQR por sus siglas en inglés, el IQR es una medida de dispersión. Alrededor del 50% de los datos están entre los cuartiles inferior y superior, es así que si el rango intercuartílico es chico los datos de enmedio están muy cercanos alrededor de la mediana, si el rango intercunatílico es grande los datos de enmedio están dispersos alrededor de la mediana. Adicionalmente, las distancias relativas de los cuartiles a lamediana nos dan información de la forma de la distribución, si una es mayor a la otra la distribucción está sesgada. Las líneas punteadas del diagrama superior indican los valores adyacentes, el valor adyacente superior se calcula de la siguiente forma: se toma el dato más grande que está a no más de \\(1.5IQR\\) del cuartil superior. Los valores adyacentes también nos dan un resumen de la forma y dispersión, pero lo hacen para los valores extremos, o colas de la distribución. Finalmente, los datos mayores (o menores) a los valores adyacentes se grafican de manera individual como puntos. Si hay datos atípicos suelen aparecer como estos puntos graficados individualmente. Ejemplo En el caso de los cantantes obtenemos la siguiente gráfica: library(lattice) library(tidyverse) # calculamos la estatura en centímetros singer$estatura.m &lt;- singer$height * 2.54 Veamos la estructura de los datos: singer %&gt;% sample_n(10) %&gt;% knitr::kable() height voice.part estatura.m 19 62 Soprano 1 157 196 75 Bass 1 190 140 65 Tenor 1 165 37 63 Soprano 2 160 2 62 Soprano 1 157 108 64 Alto 2 163 114 67 Alto 2 170 67 65 Alto 1 165 167 68 Tenor 2 173 175 73 Bass 1 185 singer.medians &lt;- singer %&gt;% group_by(voice.part) %&gt;% mutate(mediana = median(estatura.m), media = mean(estatura.m)) library(forcats) singer.medians$voice.part.2 &lt;- fct_reorder(.f = singer.medians$voice.part, .x = singer.medians$mediana, .fun = median) ggplot(singer.medians, aes(x = voice.part.2, y = estatura.m)) + geom_boxplot() + geom_jitter(position = position_jitter(height = 0.8, width = 0.3), color = &quot;darkgray&quot;, alpha = 0.5) + geom_point(aes(y = media), colour = &quot;red&quot;, size = 2) + coord_flip() Consideramos las siguientes mediciones de ozono en el aire, producidas por la red automática de monitoreo ambiental (SIMA). Las mediciones son concentración de ozono (en ppb o partes por billón) para las estaciones de Tlalnepantla e Iztapalapa, tomadas a las 2 pm, durante 2014. Una exposición de 110 ppb durante una hora se considera aguda. La distribución de ozono (en cualquier estación) es… Simétrica. Tiene sesgo a la derecha. Tiene sesgo a la izquierda. 4.5 Gráficas de cuantiles teóricos Supongamos que \\(G\\) es la función de distribución de una variable aleatoria continua, tal que \\(G\\) es diferenciable y tiene derivada positiva (por ejemplo, si la variable aleatoria tiene densidad positiva y continua en todos los reales). Entonces podemos construir la función \\(q:(0,1) \\to (\\infty, \\infty)\\) dada por: \\[q(f)=G^{-1}(f)\\] para cualquier \\(f \\in (0,1)\\). Decimos que \\(q\\) es la función de cuantiles de la variable aleatoria con distribución \\(G\\). Bajo esta definición, es claro que si \\(X\\) tiene distribución \\(G\\), entonces \\(P(X&lt;q(f))=G(q(f))=f\\). Ejemplo: normal Abajo vemos cómo se ve la gráfica de cuantiles de una variable aleatoria normal estándar. A esta función la denotamos como \\(q_{0,1}(f)\\), y en general, a la función de cuantiles de una distribución \\(Normal(\\mu, \\sigma^2)\\) la denotamos por \\(q_{\\mu, \\sigma}(f)\\). ggplot(data = data.frame(x = 0), mapping = aes(x = x)) + stat_function(fun = qnorm) + xlim(0.001,0.999) + xlab(&#39;Cuantil (f)&#39;) + ylab(&#39;q&#39;) Notemos que \\(q_{\\mu, \\sigma}(f) \\to \\infty\\) cunado \\(f \\to 1\\), y el cuantil \\(1\\) no esta definido. Análogamente el cuantil \\(0\\) tampoco está definido. ¿Cómo se ve la gráfica de cuantiles de una variable aleatoria uniforme? Similar al caso normal (una curva). Como una recta horizontal. Como una recta vertical. Como una diagonal. 4.6 Gráficas de cuantiles para un conjunto de datos Hay varias maneras razonables de definir los cuantiles de un conjunto de datos, (ver Hyndman y Fan 1996 para una resumen de lo que usan los paquetes estadísticos). Nosotros adoptamos la siguiente construcción: Cuantiles de un conjunto de datos. Si \\(x_1,...,x_n\\) es el conjunto de datos, los ordenamos de manera creciente para obtener \\(x_{(1)},...,x_{(n)}\\), donde \\(x_{(1)}\\) es la observación más chica y \\(x_{(n)}\\) la más grande. Definimos \\[f_i=\\frac{i-0.5}{n}\\] y decimos que \\(x_{(i)}\\) es el cuantil \\(f_i\\). Si se deseara calcular otros cuantiles \\(f\\), se podría interpolar o extrapolar con los puntos \\(x_{(1)},...,x_{(n)}\\) y \\(f_1,...,f_n\\), pero esto no tiene tanto sentido. Podemos hacer gráficas de la función de cuantiles de manera fácil. Estas gráficas se hacen, aproximadamente, como sigue: se ordenan los datos del más chico al más grande, se enumeran como índice, y graficamos los pares resultantes con el índice en el eje horizontal. library(ggplot2) library(reshape2) # aquí están los datos de propinas n &lt;- length(tips$total_bill) tips$probs &lt;- (1:n - 0.5) / n tips$cuantiles &lt;- quantile(tips$total_bill, probs = tips$probs, type = 5) ggplot(tips, aes(x=probs, y = cuantiles)) + xlab(&#39;Cuantil (f)&#39;) + ylab(&#39;Dólares&#39;) + geom_point() 4.6.1 ¿Qué buscar en una gráfica de cuantiles? Las gráficas de cuantiles son conceptualmente simples; sin embargo, su interpretación efectiva requiere práctica. Algunas guías son: Podemos leer fácilmente la mediana y los cuartos. Regiones en la escala de medición de los datos (dimensión vertical) con densidades de datos más altas se ven como pendientes bajas en la gráfica. Mientras que pendientes altas indican densidades de datos relativamente más bajas. Una mayor pendiente en la forma general de la gráfica (por ejemplo, en la recta que une los cuartos) indica dispersiones más grandes. Si el conjunto de datos se distribuye aproximadamente uniforme, entonces la gráfica debe parecerse a una recta (diagonal). De manera más general: en las regiones donde el histograma crece conforme aumentan los valores en el conjunto de datos, la pendiente de la gráfica de cuantiles es decreciente (así que la gráfica de cuantiles es cóncava hacia abajo). Cuando el histograma decrece conforme aumentan los valores en el conjunto de datos, la pendiente de la gráfica de cuantiles es creciente (así que observamos concavidad hacia arriba). Si la distribución tiene más dispersión hacia la derecha, la figura general de la gráfica es cóncava hacia arriba. Si tiene más dispersión a la izquierda, es cóncava hacia abajo. ¿Cómo se ve una distribución que parece tener grupos definidos donde se acumulan los datos? num_sim &lt;- 300 grupos &lt;- data.frame( gpo = sample(1:3, size = 300, replace = TRUE, prob = c(0.25, 0.25, 0.5))) grupos$x &lt;- ifelse(grupos$gpo == 1, rnorm(num_sim, mean = 0), ifelse(grupos$gpo == 2, rnorm(num_sim, 10, 2), rnorm(num_sim, mean = 20, 2))) hist(grupos$x) n &lt;- length(grupos$x) grupos$probs &lt;- (1:n - 0.5) / n grupos$cuantiles &lt;- quantile(grupos$x, probs = grupos$probs, type = 5) ggplot(grupos, aes(x=probs, y = cuantiles)) + xlab(&#39;Cuantil (f)&#39;) + ylab(&#39;Dólares&#39;) + geom_point() 4.7 Gráficas qq-normales En las secciones anteriores hemos usado gráficas de cuantiles para graficar cuantiles de un conjunto de datos y cuantiles teóricos dada una función de distribución. También es posible hacer gráficas de conjuntos de datos contra cuantiles teóricos de una distribución, de manera que podamos visualizar el grado de concordancia entre estas dos. La más popular de estas gráficas son las cuantil-cuantil normales (q-q normales). Una manera de hacer estas gráficas para el conjunto de datos \\(x_1,...,x_n\\) es calcular: \\[\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i, s=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\mu)^2}\\] y calcular los cuantiles \\(q_{\\bar{x},s}(f)\\) de la distribución \\(Normal(\\bar{x},s)\\). Entonces calculamos \\(q_{\\bar{x},s}(f)\\) donde \\(f_1,f_2,...,f_n\\) son los cuantiles de los datos y graficamos \\((x_{i},q_{\\bar{x},s}(f_i))\\). Si los puntos no se desvían mucho de la recta \\(x=y\\), entonces el conjunto de datos se distribuye, aproximadamente, de manera normal. Las desviaciones de la recta se interpretan como arriba hicimos con la gráfica cuantil cuantil. Cuando queremos evaluar si la forma de la distribución de los datos es cercana a la normal, no es necesario calcular \\(\\bar{x}\\) y \\(s\\), pues para cualquier \\(\\mu\\) y \\(\\sigma\\) tenemos que: \\[q_{\\mu, \\sigma}(f) = \\sigma q_{0,1}(f)+\\mu,\\] lo que implica que si graficamos los cuantiles \\(q_{0,1}(f_i)\\) contra los del conjunto de datos, los datos se distribuyen aproximadamente normal cuando están dispuestos cerca de una recta. Construcción de una gráfica normal de cuantiles. Si los datos ordenados están dados por \\(x_{1},x_{2},...,x_{n}\\), con valores \\(f\\) correspondientes \\(f_1,f_2,...,f_n\\), entonces graficamos los puntos \\((q_{0,1},x_{i})\\). Ejemplo: cantantes En estas gráficas podemos ver: Cada conjunto de datos es razonablemente bien aproximado por una distribución normal. Muchas de las desviaciones que observamos se deben a redondeo. Aunque las medianas varían de grupo a grupo, las pendientes no varían mucho, esto quiere decir que las dispersiones (por ejemplo, desviaciones estándar) son similares a lo largo de todos los grupos. La variación en la dispersión de cada conjunto de datos no está asociado a la mediana de cada uno. library(ggplot2) library(lattice) library(dplyr) # calculamos la estatura en centímetros singer$estatura.m &lt;- singer$height * 2.54 # calculamos el valor f dentro de cada grupo singer_ord &lt;- arrange(group_by(singer, voice.part), estatura.m) singer_cuant &lt;- mutate(singer_ord, n = n(), valor.f = (1:n[1] - 0.5)/n[1], q.norm = qnorm(valor.f) ) ggplot(singer_cuant, aes(x = q.norm, y = estatura.m)) + geom_point() + facet_wrap(~voice.part, nrow = 2) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 4.8 El TLC y errores estándar En Noviembre del 2017 El Financiero publicó una noticia que afirmab que los estados de Oaxaca, Michoacán, Morelos y Tamaulipas tenían la peor atención a pacientes diabéticos. El índice ICAD (Secretaría de Salud) mide el cuidado que se les da a los pacientes en las unidades de primer nivel, en todas sus jurisdicciones sanitarias. Además toma en cuenta tres aspectos principales: que se pueda retener al paciente, que se tenga acceso a pruebas diagnósticas y si tiene su diabetes controlada. Se cuenta con datos del ICAD de Noviembre del 2016: icad &lt;- read_csv(&quot;datos/icad.csv&quot;) icad %&gt;% sample_n(10) %&gt;% knitr::kable() fecha cve_edo cve_clues nombre calificacion pac_act 20161125 12 GRSSA012004 CSR EL POTRERILLO 53.5 33 20161125 28 TSSSA000034 CSR GUÍA DEL PORVENIR 68.6 21 20161125 25 SLSSA000806 CULIACÁNCITO 60.2 35 20161125 8 CHSSA002356 CS BENITO JUÁREZ 45.3 39 20161125 21 PLSSA008563 PLSSA008563 68.4 224 20161125 30 VZSSA016081 FCO I MADERO 51.4 19 20161125 4 CCSSA000346 CS TIKINMUL 57.8 13 20161125 15 MCSSA000813 S MARTÍN 68.4 24 20161125 15 MCSSA009954 S AGUSTÍN ATLAPULCO 62.0 63 20161125 28 TSSSA000256 CSR FORTINES 66.9 34 Cada unidad de salud o CLUES recibe una calificación promedio y tiene cierto número de pacientes diabéticos activos. Un efecto interesante es que si vemos el promedio de calificación contra el número de pacientes activos vemos el siguiente fenómeno: ggplot(icad, aes(x=pac_act, y=calificacion)) + geom_jitter(width = 0.1, height = 0.1) + scale_x_continuous(limits = c(0,400)) + geom_hline(yintercept = mean(icad$calificacion), color = &#39;red&#39;) La línea roja representa la media nacional de la calificación promedio de todas las unidades del país. Podemos ver que conforme aumenta el número de pacientes en el hospital las observaciones tienden a acercarse más a la media poblacional. Podemos simular la media para diferentes tamaños de muestra y ver cómo se comporta la media para varios tamaños de muestra. En los datos del ICAD la media nacional es de 58.867: set.seed(123456) sim_media_normal &lt;- function(n){ media &lt;- mean(rnorm(n = n, mean = 58.86661, sd = 11.12385)) tibble(n=n, media=media) } sim_1 &lt;- map_df(sample(1:500, 1000, replace = T), sim_media_normal) ggplot(sim_1, aes(x = n, y = media)) + geom_point() + geom_hline(yintercept = mean(icad$calificacion), color = &#39;red&#39;) Nuevamente se observa un fenómeno similar. Este fenómeno del error estándar generalmente se observa en la práctica y una buena estrategia para el modelado sería considerar el número de observaciones (pacientes, alumnos, escuelas) utilizados para calcular la media. Alguien nos podría preguntar: ¿por qué debería el promedio acercarse a la media general cuando aumentamos el tamaño de la muestra?. Debemos interpretar esta pregunta como preguntando por qué el error estándar de la media se reduce a medida que \\(n\\) aumenta. El teorema del límite central muestra que (bajo ciertas condiciones, por supuesto) el error estándar debe hacer esto, y que la media se aproxima a una distribución normal. Pero la pregunta es ¿por qué? La mejor justificación simple puede ser que hay más formas de obtener valores medios que valores extremos; por ejemplo, la media de un lanzamiento de un dado (distribución discreta uniforme en \\(1, 2, ..., 6\\)) es \\(3.5\\). Con un dado, es igualmente probable que obtengas un “promedio” de 3 o de 1. Pero con dos dados hay cinco formas de obtener un promedio de 3, y solo una forma de obtener un promedio de 1. Hay 5 veces más probabilidades de obtener el valor que está más cerca de la media que el que está más lejos. Veamos esto con un ejercicio de simulación: set.seed(110265) tira_dado &lt;- function(i){ res &lt;- sample(x = 1:6, size = 1) tibble(lanzamiento=i, resultado=res) } Lanzamos el dado mil veces y calculamos el promedio en cada lanzamiento. sim_2 &lt;- map_df(1:1000, tira_dado) %&gt;% mutate(media = cummean(resultado)) sim_2 %&gt;% head(5) %&gt;% knitr::kable() lanzamiento resultado media 1 3 3.00 2 2 2.50 3 5 3.33 4 1 2.75 5 5 3.20 Esto ocurre debido a la Ley de los Grandes Números: ggplot(sim_2, aes(x = lanzamiento, y = media)) + geom_line() + geom_hline(yintercept = 3.5, color = &#39;red&#39;) + scale_x_continuous(limits = c(2,1000)) + scale_y_continuous(limits = c(3.3,3.8)) La idea es ver como se aproxima la distribución muestral de la media (cuando las observaciones provienen de distintas distribuciones) a una Normal conforme aumenta el tamaño de muestra. Para esto, aproximamos la distribución muestral de la media usando simulación. Vale la pena observar que hay distribuciones que requieren un mayor tamaño de muestra \\(n\\) para lograr una buena aproximación (por ejemplo la log-normal), ¿a qué se debe esto? ¿Por qué tanto énfasis en el TLC? El error estándar es la manera más común para describir la precisión de una estadística. En términos generales, esperamos que \\(\\bar{x}\\) este a una distancia de \\(\\mu_P\\) menor a un error estándar el 68% del tiempo, y a menos de 2 errores estándar el 95% del tiempo. Estos porcentajes están basados el teorema central del límite que nos dice que bajo ciertas condiciones (bastante generales) de \\(P\\) la distribución de \\(\\bar{x}\\) se aproximará a una distribución normal: \\[\\bar{x} \\overset{\\cdot}{\\sim} N(\\mu_P,\\sigma_P^2/n)\\] Con la siguiente aplicación podemos simular muestras de cualquier distribución y visualizar la distribución de \\(\\bar{X}\\): 4.9 Ejemplo La corporación ALFA vende bicicletas. Basada en su experiencia siente que en los meses de verano es probable que venda 0, 1, 2, 3 ó 4 bicicletas en un día (la firma nunca ha vendido más de 4 bicicletas por día). Sea \\(X\\) el número de bicicletas vendidas en un día. \\(X\\) sigue una distribución uniforme y toma los valores \\(-2,-1,0,1,2\\), es decir, \\[ X=\\left\\{ \\begin{array}{cl} -2 &amp; \\text{con probabilidad 1/5}\\\\ -1 &amp; \\text{con probabilidad 1/5}\\\\ 0 &amp; \\text{con probabilidad 1/5}\\\\ 1 &amp; \\text{con probabilidad 1/5}\\\\ 2 &amp; \\text{con probabilidad 1/5.} \\end{array}\\right. \\] Suponga que el número de bicicletas vendidas el siguiente día es independiente del número vendido el día anterior. Sea \\(S\\) el número de bicicletas vendidas en un periodo de cinco días. Si \\(X_1,X_2,\\ldots,X_{5}\\) son variables aleatorias independientes con la misma distribución, entonces \\[ S = X_1 + X_2 + \\cdots + X_{5}. \\] Primero podemos definir el experimento como una función que reciba el número de realización del experimento y regrese el número de bicicletas vendidas en los 5 días: set.seed(100888) experimento &lt;- function(k){ tibble(k = k, x = as.integer(sum(sample.int(5,5,replace=T) - 1))) } Podemos ver qué regresa la función en una realización del experimento: experimento(1) #&gt; # A tibble: 1 x 2 #&gt; k x #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 12 Ahora utilizamos la fución map_df del paquete purrr para obtener 1000 realizaciones del experimento en un data frame: m &lt;- 1000 df_bicis &lt;- map_df(.x = 1:m, .f = experimento) df_bicis %&gt;% head(10) %&gt;% knitr::kable() k x 1 16 2 11 3 14 4 5 5 6 6 9 7 11 8 7 9 12 10 13 Calculamos la tabla y gráfica de frecuencias: df_bicis_frec &lt;- df_bicis %&gt;% group_by(x) %&gt;% summarise(p_x = n()/m) ggplot(df_bicis_frec, aes(x = x)) + geom_bar(aes(y = p_x), stat = &#39;identity&#39;) + geom_text(stat=&#39;identity&#39;,aes(y=p_x,label=round(p_x,4)),vjust=-1,size=2.5) Si comparamos con los valores que toma una distribución normal con la misma media y la misma desviación estándar vemos que las probabilidades son muy similares. Es un resultado del Teorema del Límite Central que la suma de variables uniformes independientes sigue una distribución normal. De cualquier forma es interesante cómo la suma de sólo 5 uniformes independientes da como resultado una distribución que se asemeja a la de una normal. media_bicis &lt;- mean(df_bicis$x) sd_bicis &lt;- sd(df_bicis$x) dnorm(0:20, mean = media_bicis, sd = sd_bicis) #&gt; [1] 0.000933 0.002325 0.005270 0.010868 0.020393 0.034816 0.054083 #&gt; [8] 0.076440 0.098302 0.115022 0.122455 0.118619 0.104546 0.083838 #&gt; [15] 0.061172 0.040611 0.024531 0.013482 0.006742 0.003068 0.001270 4.10 Tarea La Evaluación Nacional de Logros Académicos en Centros Escolares (ENLACE), es un examen que se pretende realizar cada año en México por la Secretaria de Educación Pública (SEP) a todas las escuelas públicas y privadas de nivel básico; para conocer el nivel de desempeño en las materias de español y matemáticas. Han existido importantes resistencias a la aplicación de este examen y opiniones de intelectuales respecto de las fallas que esta tiene. En este enlace la SEP publicó los resultados de la prueba ENLACE para todas las escuelas de los 32 estados de México: http://www.enlace.sep.gob.mx/content/ba/pages/base_de_datos_completa_2013/ Descarga los datos para todas las localidades y cárgalos en R utilizando la función map_df del paquete purrr. Explica si los datos cumplen los principios de datos limpios. Haz la limpieza necesaria que requieran los datos. Agrega los datos a nivel municipio para calcular el número de alumnos en cada grado y su promedio de puntos en español y matemáticas. Haz una gráfica en la cual cada punto represente un municipio y en el eje \\(x\\) se muestre el número de alumnos que presentaron el examen y en el eje \\(y\\) el promedio de puntos en matemáticas. La siguiente función de R sirve para generar una gráfica de la función de masa de probabilidad de una variable aleatoria \\(\\mbox{Binomial}(n,p)\\). genera_binoms &lt;- function(p, n) { datos &lt;- tibble(x = 0:n, y = dbinom(x, n, p)) media &lt;- n*p sd &lt;- sqrt(n*p*(1-p)) ls &lt;- media + 4*sd li &lt;- media - 4*sd lse &lt;- as.integer(ls) lie &lt;- as.integer(li) + 1 datos %&gt;% filter(x &lt; ls &amp; x &gt; li) %&gt;% ggplot(aes(x, y)) + geom_point(size = 0.6) + geom_segment(aes(x=x,y=0,xend=x,yend=y), color = &#39;red&#39;) + scale_x_continuous(&#39;k&#39;,breaks=lie:lse) + scale_y_continuous(&#39;p(k)&#39;) + geom_text(aes(y=y,label=round(y,3)),stat = &#39;identity&#39;,vjust=-1,size=3) } Por ejemplo, con \\(p=1/3\\) y \\(n=10\\) realizaciones, la gráfica se ve así: genera_binoms(1/3, 10) Utiliza la función para determinar a partir de qué valor de \\(n\\) la distribución de una variable Binomial con probabilidad de éxito \\(p=1/100\\) se asemeja a la de una normal. Regresando al ejemplo de la venta de bicicletas: Utiliza la función de experimento y la función map_df para obtener una simulación de 100 realizaciones del experimento. Haz una gráfica de cuantiles vs cuantiles normales y decide si el conjunto de datos está bien aproximado por una distribución normal. Calcula la distribución de probabilidades del número de bicicletas vendidas en un periodo de cinco días. Para calcular \\(p(k) = P(S=k)\\) considera que deberás contar el número de soluciones de la ecuación \\[ x_1 + x_2 + \\cdots + x_5 = k, \\] donde \\(0 \\leq x_i \\leq 4\\). El número de soluciones enteras es equivalente al coeficiente del término \\(y^k\\) del polinomio \\((1+y+\\cdots+y^4)^5\\) porque cada factor del producto representa el número de bicicletas vendidas por día. Compara las probabilidades calculadas con los valores de la distribución normal obtenidos anteriormente. "],
["analisis-de-datos-categoricos.html", "Clase 5 Análisis de datos categóricos 5.1 Repaso y algunos conceptos 5.2 La \\(\\chi^2\\) de Pearson de una multinomial 5.3 Definiciones 5.4 Asociación en tablas de tamaño \\(I\\times J\\) 5.5 Intervalos de confianza para los parámetros de asociación 5.6 Prueba de independencia 5.7 General Social Survey 1972 - 2016 5.8 La catadora de té 5.9 Modelos multinomiales para conteos 5.10 Modelos log lineales con tres variables categóricas 5.11 Ejemplo: sensitividad y especificidad 5.12 Ejemplo: horóscopos 5.13 Tarea (opcional)", " Clase 5 Análisis de datos categóricos .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } Sólo necesitas instalar un paquete una vez, pero debes volver a cargarlo cada vez que inicies una nueva sesión. library(tidyverse) Las variables categóricas están por doquier. Desde ayudar a decidir cuándo un tratamiento médico es mejor hasta evaluar los factores que afectan nuestras opiniones y conductas, hoy en día los analistas encuentran innumerables usos para los métodos de datos categóricos. Primero vamos a repasar algunos conceptos de probabilidad. 5.1 Repaso y algunos conceptos Recordemos la distribución multinomial. Supongamos que cada uno de \\(n\\) ensayos independientes e idénticos tiene realizaciones en \\(c\\) categorías. Definimos \\(y_{ij}\\) como \\[ y_{ij} = \\left\\{ \\begin{array}{cl} 1 &amp; \\text{si el }\\; i\\text{-esimo ensayo cae en la categoria }j,\\\\ 0 &amp; \\text{en otro caso.} \\end{array}\\right. \\] Entonces \\(y_i=(y_{i1},y_{i2},\\ldots,y_{ic})\\) representa un ensayo multinomial, done \\[ \\displaystyle{\\sum_j{y_{ij}}}=1. \\] Notemos que \\(y_{ic}=1-(y_{i1}+\\cdots+y_{i,c-1})\\) es redundante. Sea \\(n_j=\\displaystyle{\\sum_i{y_{ij}}}\\) el número de ensayos que caen en la categoría \\(j\\). Los conteos \\((n_1,n_2,\\ldots,n_c)\\) tienen una distribución multinomial. Sea \\(\\pi_{j}=P(Y_{ij}=1)\\), la probabilidad de éxito en la categoría \\(j\\). La función de masa de probabilidad de \\((n_1,n_2,\\ldots,n_c)\\) es \\[ p(n_1,n_2,\\ldots,n_c) = \\dfrac{n!}{n_1!n_2!\\cdots n_c!}\\pi_1^{n_1}\\pi_2^{n_2}\\cdots \\pi_c^{n_c}. \\] Sea \\(n=\\displaystyle{\\sum_j{n_j}}\\). Recordemos que esta ecuación es de dimensión \\(c-1\\) porque \\[ n_c = n - (n_1 + n_2 + \\cdots + n_{c-1}). \\] Se puede ver que \\[ \\begin{eqnarray*} E(n_j) =n\\pi_j, \\quad &amp;&amp; V(n_j)=n\\pi_j(1-\\pi_j),\\\\ C(n_i,n_j)=-n\\pi_i \\pi_j&amp;\\quad&amp;\\mbox{si } i\\neq j \\end{eqnarray*} \\] Modelo Poisson Sean \\((Y_1,Y_2,\\ldots,Y_c)\\) variables aleatorias Poisson independientes con parámetros \\((\\mu_1,\\mu_2,\\ldots,\\mu_c)\\). La función de masa de probabilidad conjunta es \\[ P(Y_1=n_1, Y_2=n_2, \\ldots, Y_c=n_c) = \\prod_i{\\mbox{exp}(-\\mu_i)\\dfrac{\\mu_i^{n_i}}{n_i!}}. \\] El total \\(n=\\displaystyle{\\sum_i{Y_i}}\\) también tiene una distribución Poisson con media \\(\\displaystyle{\\sum_i{\\mu_i}}\\). Como \\(n\\) también es una variable aleatoria, al condicionar en \\(n\\), \\(\\{Y_i\\}\\) ya no tienen una distribución Poisson, pues cada \\(Y_i\\) no puede exceder \\(n\\). La distribución condicional es \\[ \\begin{eqnarray*} P\\left(Y_1=n_1,\\ldots,Y_c=n_c \\,\\middle|\\, \\sum_j{Y_j}=n\\right) &amp;=&amp; \\dfrac{P(Y_1=n_1,\\ldots,Y_c=n_c)}{P\\left(\\sum_j{Y_j}\\right)} \\\\ &amp;=&amp; \\dfrac{\\prod_i \\mbox{exp}(-\\mu_i)\\mu_i^{n_i}/n_i!}{\\mbox{exp}\\left(-\\sum_j{\\mu_j}\\right)\\left(\\sum_j {\\mu_j}\\right)^n/n!} \\\\ &amp;=&amp; \\dfrac{n!}{\\prod_i n_i!} \\prod_i{\\pi_i^{n_i}} \\end{eqnarray*} \\] con \\(\\pi_i = \\dfrac{\\mu_i}{\\sum_i \\mu_i}\\), es decir, se trata de una distribución multinomial con parámetros \\((n, \\{\\pi_i\\})\\). Muchos análisis de datos categóricos suponen una distribución multinomial. Tales análisis usualmente tineen resultados similres a aquellos análisis que suponen una distribución Poisson, por las similitudes en sus funciones de verosimilitud. En la estimación de parámetros a menudo se utilizan dos métodos para obtener intervalos de confianza: Método de Wald En el caso univariado se utiliza como estimador de la varianza \\(-E\\left(\\dfrac{d^2 L(\\theta)}{d\\theta^2}\\right)\\) y el estadístico es \\[z=(\\hat{\\theta} - \\theta_0)/\\mbox{SE} \\sim N(0,1)\\] o en el caso multivariado, \\[ W = \\left(\\hat{\\theta}- \\theta_0\\right)^T\\left[\\mbox{Cov}\\left(\\hat{\\theta}\\right)\\right]^{-1}\\left(\\hat{\\theta}- \\theta_0\\right), \\] y como \\(\\hat{\\theta}\\) se distribuye normal asintóticamente, entonces la distribución de \\(W\\) es \\(\\chi^2\\) con grados de libertad igual al rango de \\(\\mbox{Cov}\\left(\\hat{\\theta}\\right)\\), el número de parámetros no redundantes. Método de cociente de verosimilitud Si \\(l_0\\) es el máximo valor de la función de verosimilitud bajo \\(H_0\\) y \\(l_1\\) es el valor máximo sobre el espacio de parámetros (que contiene también el valor bajo \\(H_0\\)), entonces \\(l_0 \\leq l_1\\) y el estadístico es \\[-2\\, \\mbox{log}(\\Lambda) = -2\\, \\mbox{log}(l_0/l_1)=-2(L_0-L_1) \\sim \\chi^2_n\\] donde los grados de libertad equivalen a la diferencia de dimensiones de los espacios de parámetros. 5.1.1 Caso binomial Definimos la función de verosimilitud de una variable aleatoria binomial con \\(n\\) realizaciones y \\(x\\) éxitos: \\[ L(\\theta) = \\mbox{log}(\\theta^x(1-\\theta)^{n-x}) = x\\mbox{log}(\\theta) + (n-x)\\mbox{log}(1-\\theta) \\] likelihood &lt;- function(x, n){ function(theta){x*log(theta) + (n-x)*log(1-theta)} } Creamos nuestra función de verosimilitud para \\(x=3\\) y \\(n=10\\): mi_likelihood &lt;- likelihood(3, 10) Graficamos la función: ggplot(data = data.frame(x = 0), mapping = aes(x = x)) + stat_function(fun = mi_likelihood) + xlim(0.001,0.999) El estadístico de Wald da como resultado el invervalo \\[ \\hat{\\theta} \\pm z_{\\alpha/2}\\sqrt{\\dfrac{\\hat{\\theta}(1-\\hat{\\theta})}{n}} \\] El estadístico del cociente de verosimilitud es: \\[ 2x\\left[x\\mbox{log}\\left(\\dfrac{\\hat{\\theta}}{\\theta_0}\\right)+(n-x)\\mbox{log}\\left(\\dfrac{1-\\hat{\\theta}}{1-\\theta_0}\\right)\\right] = \\chi^2_{1,\\alpha} \\] Se puede expresar como \\[ 2\\sum{\\mbox{observado} \\,\\left[\\,\\mbox{log}\\left(\\dfrac{\\mbox{observado}}{\\mbox{ajustado}}\\right)\\right]} \\] Existen varios métodos para obtener intervalos de confianza. Utilizando la función ciAllx del paquete proportion podemos obtener intervalos de confianza para \\(\\hat{\\theta}\\) a partir de 6 métodos: library(proportion) intervalos &lt;- ciAllx(x = 3, n = 10, alp = 0.05) intervalos %&gt;% knitr::kable() method x LowerLimit UpperLimit LowerAbb UpperAbb ZWI Wald 3 0.016 0.584 NO NO NO ArcSine 3 0.071 0.603 NO NO NO Likelihood 3 0.085 0.607 NO NO NO Score 3 0.108 0.603 NO NO NO Logit-Wald 3 0.100 0.624 NO NO NO Wald-T 3 0.002 0.598 NO NO NO Los intervalos de confianza para todos los métodos son realmente muy similares. Si el tamaño de muestra \\(n\\) es grande, los 6 métodos dan como resultado intervalos de confianza prácticamente idénticos. ggplot(intervalos, aes(y = method)) + geom_segment(aes(x = LowerLimit, xend = UpperLimit, y = method, yend = method)) + geom_vline(xintercept = 0.3, color = &#39;red&#39;) + xlab(&#39;Intervalo de confianza para cada método&#39;) Estimación de parámetros multinomiales Definimos la función de verosimilitud \\[ l(n_1,n_2,\\ldots,n_c | \\pi_1, \\pi_2, \\ldots, \\pi_c) = c \\prod_j \\pi_j^{n_j} \\] donde \\(\\pi_j \\geq 0\\) y \\(\\sum_j{\\pi_j}=1\\). Para estimar \\(\\{\\pi_j\\}\\) maximizamos la log-verosimilitud \\[ L(\\pi) = \\sum_j{n_j \\mbox{log}(\\pi_j)}. \\] Para no tener redundancias vemos \\(L\\) como función de \\(\\pi_1,\\pi_2,\\ldots,\\pi_{c-1}\\) pues \\(\\pi_c=1-(\\pi_1+ \\pi_2+\\cdots+\\pi_{c-1})\\). Por lo tanto, \\[ \\dfrac{d \\pi_c}{d \\pi_j} = -1 \\qquad \\mbox{para }\\; j=1,2,\\ldots,c-1. \\] Por la regla de la cadena, \\[ \\dfrac{d\\,\\mbox{log}(\\pi_c)}{d\\,\\pi_j}=\\dfrac{1}{\\pi_c} \\cdot \\dfrac{d\\, \\pi_c}{d\\, \\pi_j}=-\\dfrac{1}{\\pi_c}. \\] Ahora diferenciamos \\(L\\) con respecto a \\(\\pi_j\\) \\[ \\dfrac{d\\, L(\\pi)}{d\\, \\pi_j}=\\dfrac{n_j}{\\pi_j} - \\dfrac{n_c}{\\pi_c} = 0. \\] Por lo que los estimadores de máxima verosimilitud satisfacen que \\[ \\dfrac{\\hat{\\pi}_j}{\\hat{\\pi}_c} = \\dfrac{n_j}{n_c}. \\] Ahora bien, \\[ 1 = \\sum_j{\\pi_j}= \\dfrac{\\hat{\\pi}_c\\left(\\sum_j n_j\\right)}{n_c}=\\dfrac{\\hat{\\pi}_c n}{n_c}, \\] y se tiene que \\(\\hat{\\pi_c}=n_c/n\\) y \\(\\hat{\\pi_j}=n_j/n\\) para \\(j=1,2,\\ldots,c-1\\). Se puede verificar que estos estimadores efectivamente maximizan la verosimilitud. Notemos que \\(\\hat{\\pi_j}=n_j/n\\) son las proporciones muestrales. 5.2 La \\(\\chi^2\\) de Pearson de una multinomial En 1900 el estadístico Karl Pearson definió una prueba de hipótesis para la multinomial. Su motivación inicial fue analizar las probabilidades de ocurrencias de varias realizaciones en el juego de la ruleta. Consideramos para \\(j=1,2,\\ldots,c\\) \\[ H_0:\\pi_j =\\pi_{j0} \\qquad H_1:\\pi_j \\neq \\pi_{j0}. \\] Bajo \\(H_0\\), los valores esperados de \\(\\{n_j\\}\\), llamadas frecuencias esperadas son \\(\\mu_j=n\\pi_{j0}\\), \\(j=1,\\ldots,c\\). El estadístico propueto es \\[ X^2 = \\sum_j{\\dfrac{(n_j - \\mu_j)^2}{\\mu_j}} \\sim \\chi^2_{(c-1)}. \\] Si las diferencias \\(\\{n_j - \\mu_j\\}\\) son más grandes, esto produce valores \\(X^2\\) más grandes para una \\(n\\) fija. Si \\(X_o^2\\) es el valor observado de \\(X^2\\) entonces el valor p es \\(P(X^2 \\geq X_o^2)\\). Si \\(n\\) es grande, \\(X^2\\) tiene una distribución \\(\\chi^2_{c-1}\\). 5.2.1 Cociente de verosimilitud de una multinomial Bajo \\(H_0\\) la verosimilitud se maximiza cuando \\(\\hat{\\pi}_j=\\pi_{j0}\\) y en el caso general cuando \\(\\hat{pi}_j=\\frac{n_j}{n}\\). El cociente de verosimilitud es \\[ \\Lambda = \\dfrac{\\prod_j{\\pi_{j0}^{n_j}}}{\\prod_j{(n_j/n)^{n_j}}}. \\] Por lo tanto, el estadístico del cociente de verosimilitud es \\[ G^2 = -2\\,\\mbox{log}(\\Lambda) = 2\\, \\sum_j{n_j \\mbox{log}\\left(\\dfrac{n_j}{n\\pi_{j0}}\\right)}. \\] A este estadístico se le llama estadístico \\(\\chi^2\\) de verosimilitud. Entre más grande sea el valor de \\(G^2\\) hay mayor evidencia en contra de \\(H_0\\). En el caso general, el espacio de parámetros consiste de \\(\\{\\pi_j\\}\\) sujeto a que \\(\\sum_j{\\pi_j}=1\\), por lo que la dimensión es \\(c-1\\). Bajo \\(H_0\\), se especifica por completo \\(\\{\\pi_j\\}\\), por lo que la dimensión es \\(0\\). La diferencia entre estas dimensiones es \\((c-1)\\). Si \\(n\\) es grande, entonces \\(G^2\\) tiene una distribución \\(\\chi^2\\) con \\((c-1)\\) grados de libertad. 5.3 Definiciones Supongamos que se tiene una tabla de contingencias. A continuación introduciremos una notación y algunas definiciones. 5.3.1 Notación Sea \\(\\pi_{ij}\\) la probabilidad de que una observación \\((X,Y)\\) esté en la celdilla (\\(i\\),\\(j\\)). Las densidades marginales las denotamos por: \\[ \\pi_{i+}=\\sum_j{\\pi_{ij}},\\qquad \\pi_{+j} = \\sum_i{\\pi_{ij}} \\] Cuando ambas variables son aleatorias, se pueden definir las densidades marginales: \\[ \\pi_{j|i} = \\pi_{ij}/\\pi_{i+}, \\qquad \\mbox{para toda }i\\mbox{ y }j \\] Se dice que las variables son independientes si \\[ \\pi_{ij} = \\pi_{i+}\\pi_{+j} \\quad \\mbox{para }\\; i=1,\\ldots,I\\; \\mbox{ y para }\\; j=1,\\ldots,J. \\] Cuando son independientes se cumple que \\[ \\pi_{j|i}=\\pi_{ij}/\\pi_{i+}=(\\pi_{i+}\\pi_{+j})/\\pi_{i+}=\\pi_{+j} \\quad \\mbox{para }i=1,\\ldots,I. \\] 5.3.2 Razón de momios Para una probabilidad de éxito \\(\\pi\\) se definen los momios (o chances) como \\[ \\Omega = \\dfrac{\\pi}{1-\\pi} \\] Los momios siempre son no negativos. Ejemplo Un sitio de apuestas escribe: Momio 7/1: Ganas $7 por cada $1 apostado. Si apuestas $10, cobras $70 más tu apuesta, es decir, $80. Momio 5/2: Ganas $5 por cada $2 apostados. Si apuestas $10, cobras $25 más tu apuesta, es decir, $35. Momio 3/5: Ganas $3 por cada $5 apostados. Si apuestas $10, cobras $6 más tu apuesta, es decir, $16. Si el momio es menor que 1 entonces… La probabilidad de éxito es cero. La probabilidad de éxito es menor que \\(1/2\\). El éxito es más probable que el fracaso. Todas la anteriores. Si \\(\\Omega &gt; 1\\), entonces es más probable el éxito que el fracaso. Por ejemplo, cuando \\(\\pi=0.75\\) entonces \\(\\Omega = 0.75/0.25 =3\\), un éxito es 3 veces más probable que un fracaso, y esperaríamos 3 éxitos por cada fracaso. Cuando \\(\\Omega = \\frac{1}{3}\\) un fracaso es tres veces más verosímil que un éxito. Inversamente, \\[ \\pi = \\dfrac{\\Omega}{\\Omega + 1}. \\] Pensemos nuevamente en una tabla de contingencias de \\(2\\times 2\\), en la \\(i\\)-ésima fila los momios de éxito en vez de fracaso son \\(\\Omega_i=\\pi_i/(1-\\pi_i)\\). La razón de momios de \\(\\Omega_1\\) y \\(\\Omega_2\\) en ambas filas es: \\[ \\theta = \\dfrac{\\Omega_1}{\\Omega_2}=\\dfrac{\\pi_1/(1-\\pi_1)}{\\pi_2/(1-\\pi_2)} \\] Si se tiene una tabla con probabilidades conjuntas \\(\\{\\pi_{ij}\\}\\) la definición equivalente de momio para cada fila es \\(\\Omega_i=\\pi_{i1}/\\pi_{i2}\\), \\(i=1,2\\). Entonces la razón de momios es: \\[ \\theta = \\dfrac{\\pi_{11}/\\pi_{12}}{\\pi_{21}/\\pi_{22}}=\\dfrac{\\pi_{11}\\pi_{22}}{\\pi_{12}\\pi_{21}} \\] A \\(\\theta\\) se le conoce también como la razón del producto cruzado. ¿Cómo interpretamos este número? Si \\(\\theta=1\\) (o \\(\\Omega_1=\\Omega_2\\)), entonces las variables son independientes. Si \\(\\theta &gt; 1\\), entonces las observaciones en el renglón 1 tienen más probabilidad de éxito que observaciones en en renglón 2, es decir, \\(\\pi_1 &gt; \\pi_2\\). Si \\(\\theta &lt; 1\\), entonces \\(\\pi_1 &lt; \\pi_2\\). Para conteos en una tabla de contingencia, la razón de momios muestral es: \\[ \\hat{\\theta} = \\dfrac{n_{11}n_{22}}{n_{12}n_{21}} \\] Regresemos a los datos de billboard: billboard &lt;- read_csv(&quot;datos/billboard_alltime.csv&quot;) OR &lt;- function(var1, var2){ n &lt;- table(var1, var2) (n[1,1] / n[1,2])*(n[2,2] / n[2,1]) } OR(billboard$gains_performance, billboard$rising) #&gt; [1] 2.75 Los chances de éxito (subir una o más posiciones en el chart) cuando no hubo una presentación en vivo (rengón 1) son equivalentes a 2.75 veces los chances de éxito (incremento en el chart) que cuando no hubo presentación en vivo (renglón 2). Con la función odds.ratio del paquete questionr se puede calcular la razón de momios y el paquete hace una prueba de hipótesis conocida como prueba exacta de Fisher: library(questionr) odds.ratio(table(billboard$gains_performance, billboard$rising)) #&gt; OR 2.5 % 97.5 % p #&gt; Fisher&#39;s test 2.75 2.70 2.8 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Valores de \\(\\theta\\) más alejados de \\(1\\) reflejan un mayor grado de asociación entre las variables. Dos valores representan la misma asociación pero en direcciones opuestas, cuando uno es el recíproco del otro. Por ejemplo, cuando \\(\\theta=0.25\\) los chances de éxito en el renglón 1 son 0.25 veces los chances en el renglón 2, o equivalentemente, los chances de éxito en el renglón 2 son 1/0.25 = 4 veces los chances en el renglón 1. Si se invierte el orden de los renglones o de las columnas, entonces \\(\\theta\\) no cambia. debe ser necesariamente \\(1\\). es el recíproco de su valor original. puede tomar cualquier valor. Para hacer inferencia es conveniente usar \\(\\mbox{log}(\\theta)\\). Este tiene las siguientes propiedades: El caso de independencia corresponde a \\(\\mbox{log}(\\theta) = 0\\). El logaritmo de la razón de momios es simétrico alrededor de \\(0\\). Si se invierten los renglones o las columnas, entonces \\(\\mbox{log}(\\theta)\\) cambia de signo pero tiene la misma magnitud. Por ejemplo, dos valores de \\(\\mbox{log}(\\theta)\\) que tienen misma magnitud pero signos contrarios, como \\(\\mbox{log}(4)=1.39\\) y \\(\\mbox{log}(0.25)=-1.39\\), representan el mismo grado de asociación. 5.4 Asociación en tablas de tamaño \\(I\\times J\\) En tablas de \\(2\\times 2\\) un sólo número como la razón de momios puede ser suficiente para resumir la asociación. En tablas \\(I\\times J\\) usualmente no es posible resumir la asociación entre las dos variables con un sólo número sin alguna pérdida de información. Sin embargo, un conjunto de razones de momios, o bien, algun otro estadístico de resumen pueden ser útil para describir la asociación entre las variables. 5.4.1 Razones de momios en tablas \\(I\\times J\\) Se puede utiliar los \\(\\dbinom{I}{2}\\) pares de renglones en combinación con los \\(\\dbinom{J}{2}\\) pares de columnas. Para renglones \\(a\\) y \\(b\\) y columnas \\(c\\) y \\(d\\) la razón de momios utiliza 4 valores en casillas en un patrón rectangular: \\[ \\dfrac{\\pi_{ab}\\pi_{bd}}{\\pi_{bc}\\pi_{ad}} \\] Consideremos el subconjunto de \\((I-1)(J-1)\\) razones de momios locales: \\[ \\theta_{ij} = \\dfrac{\\pi_{ij}\\pi_{i+1,j+1}}{\\pi_{i,j+1}\\pi_{i+1,j}}, \\qquad i=1,\\ldots, I-1,\\;\\;\\; j=1,\\ldots,J-1. \\] Estos \\((I-1)(J-1)\\) razones de momios determinan las razones de momios entre pares de renglones y pares de columnas. 5.4.2 Ejemplo: mushrooms Este conjunto de datos incluye descripciones de muestras correspondientes a 23 especies de setas de las familias Agaricus y Lepiota. Cada especie está identificada como definitivamente comestible, definitivamente venenosa, o de comestibilidad desconocida y no recomendada su ingesta. Las otras variables se presentan en la siguiente tabla: Variable Categorías cap-shape bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s cap-surface fibrous=f,grooves=g,scaly=y,smooth=s cap-color brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y bruises bruises=t,no=f odor almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s gill-attachment attached=a,descending=d,free=f,notched=n gill-spacing close=c,crowded=w,distant=d gill-size broad=b,narrow=n gill-color black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y stalk-shape enlarging=e,tapering=t stalk-root bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? stalk-surface-above-ring fibrous=f,scaly=y,silky=k,smooth=s stalk-surface-below-ring fibrous=f,scaly=y,silky=k,smooth=s stalk-color-above-ring brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y stalk-color-below-ring brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y veil-type partial=p,universal=u veil-color brown=n,orange=o,white=w,yellow=y ring-number none=n,one=o,two=t ring-type cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z spore-print-color black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y population abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y habitat grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d mushrooms &lt;- read_csv(&quot;datos/mushrooms.csv&quot;) glimpse(mushrooms) #&gt; Observations: 8,124 #&gt; Variables: 23 #&gt; $ edibility &lt;chr&gt; &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, ... #&gt; $ `cap-shape` &lt;chr&gt; &quot;x&quot;, &quot;x&quot;, &quot;b&quot;, &quot;x&quot;, &quot;x&quot;, &quot;x&quot;, &quot;b&quot;, ... #&gt; $ `cap-surface` &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;y&quot;, &quot;s&quot;, &quot;y&quot;, &quot;s&quot;, ... #&gt; $ `cap-color` &lt;chr&gt; &quot;n&quot;, &quot;y&quot;, &quot;w&quot;, &quot;w&quot;, &quot;g&quot;, &quot;y&quot;, &quot;w&quot;, ... #&gt; $ bruises &lt;chr&gt; &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;f&quot;, &quot;t&quot;, &quot;t&quot;, ... #&gt; $ odor &lt;chr&gt; &quot;p&quot;, &quot;a&quot;, &quot;l&quot;, &quot;p&quot;, &quot;n&quot;, &quot;a&quot;, &quot;a&quot;, ... #&gt; $ `gill-attachment` &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, ... #&gt; $ `gill-spacing` &lt;chr&gt; &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;w&quot;, &quot;c&quot;, &quot;c&quot;, ... #&gt; $ `gill-size` &lt;chr&gt; &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, ... #&gt; $ `gill-color` &lt;chr&gt; &quot;k&quot;, &quot;k&quot;, &quot;n&quot;, &quot;n&quot;, &quot;k&quot;, &quot;n&quot;, &quot;g&quot;, ... #&gt; $ `stalk-shape` &lt;chr&gt; &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;t&quot;, &quot;e&quot;, &quot;e&quot;, ... #&gt; $ `stalk-root` &lt;chr&gt; &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, &quot;e&quot;, &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, ... #&gt; $ `stalk-surface-above-ring` &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, ... #&gt; $ `stalk-surface-below-ring` &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, ... #&gt; $ `stalk-color-above-ring` &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, ... #&gt; $ `stalk-color-below-ring` &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, ... #&gt; $ `veil-type` &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, ... #&gt; $ `veil-color` &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, ... #&gt; $ `ring-number` &lt;chr&gt; &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, ... #&gt; $ `ring-type` &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;e&quot;, &quot;p&quot;, &quot;p&quot;, ... #&gt; $ `spore-print-color` &lt;chr&gt; &quot;k&quot;, &quot;n&quot;, &quot;n&quot;, &quot;k&quot;, &quot;n&quot;, &quot;k&quot;, &quot;k&quot;, ... #&gt; $ population &lt;chr&gt; &quot;s&quot;, &quot;n&quot;, &quot;n&quot;, &quot;s&quot;, &quot;a&quot;, &quot;n&quot;, &quot;n&quot;, ... #&gt; $ habitat &lt;chr&gt; &quot;u&quot;, &quot;g&quot;, &quot;m&quot;, &quot;u&quot;, &quot;g&quot;, &quot;g&quot;, &quot;m&quot;, ... library(oddsratio) mushrooms_1 &lt;- mushrooms %&gt;% mutate(edibility = 1*(edibility == &#39;e&#39;)) fit_glm &lt;- glm(edibility ~ `cap-color`, data=mushrooms_1, family=&#39;binomial&#39;) or_mushrooms &lt;- or_glm(data = mushrooms_1, model = fit_glm) or_mushrooms %&gt;% knitr::kable() predictor oddsratio CI_low (2.5 %) CI_high (97.5 %) increment cap-colorc 6.67e+00 3.247 14.50 Indicator variable cap-colore 1.78e+00 1.263 2.55 Indicator variable cap-colorg 3.19e+00 2.272 4.56 Indicator variable cap-colorn 3.10e+00 2.210 4.41 Indicator variable cap-colorp 1.59e+00 0.992 2.56 Indicator variable cap-colorr 5.30e+06 4.411 NA Indicator variable cap-coloru 5.30e+06 4.411 NA Indicator variable cap-colorw 5.62e+00 3.951 8.12 Indicator variable cap-colory 1.49e+00 1.048 2.14 Indicator variable En esta tabla se tienen dos columnas donde “e” siginifica que la seta es comestible y “p” que la seta es venenosa. En los renglones están los colores de las setas codificados de acuerdo con la tabla anterior. En las columnas están las razones de momio para cada color para aquellas setas que son comestibles. Veamos la tabla de color y comestibilidad: table(mushrooms$edibility, mushrooms$`cap-color`) #&gt; #&gt; b c e g n p r u w y #&gt; e 48 32 624 1032 1264 56 16 16 720 400 #&gt; p 120 12 876 808 1020 88 0 0 320 672 Podemos ver que la razón de momios para p=pink y u=purple es “infinita” porque hay muy pocas observaciones para setas de esos colores. De cualquier forma, con los momios podemos concluir que aquellas setas de colores c=cinnamon y w=white aumentan los chances de que sean comestibles en 6.7 y 5.6, respectivamente. 5.5 Intervalos de confianza para los parámetros de asociación La precisión de los esitmadores de asociación está caracterizada por las distribuciones muestrales de los errores estándar. Para tablas de \\(2\\times 2\\) recordemos que \\[ \\hat{\\theta} = \\dfrac{n_{11}n_{22}}{n_{12}n_{21}} \\] Se puede demostrar que \\(\\hat{\\theta}\\) tiene una distribución normal asinotóticamente alrededor de \\(\\theta\\). A menos que \\(n\\) sea grande, la distribución muestral generalmente es sesgada. 5.5.1 Error estándar de la razón de momios Utilizando la transformación de logaritmo, la estructura multiplicativa converge muy rápidamente a la normalidad. Una aproximación del error estándar para \\(\\mbox{log}(\\hat{\\theta})\\) es \\[ \\hat{\\sigma}(\\mbox{log}\\hat{\\theta}) = \\sqrt{\\dfrac{1}{n_{11}}+\\dfrac{1}{n_{12}}+\\dfrac{1}{n_{21}}+\\dfrac{1}{n_{22}}}. \\] Como consecuencia de la normalidad de la distribución de \\(\\mbox{log}(\\hat{\\theta})\\), \\[ \\mbox{log}(\\hat{\\theta}) \\pm z_{\\alpha/2}\\hat{\\sigma}(\\mbox{log}\\hat{\\theta}) \\] 5.6 Prueba de independencia Suponemos que se tiene resultados obtenidos de una distribución multinomial y probabilidades conjuntas \\(\\{\\pi_{ij}\\}\\) en una tabla de contingencia de dimensiones \\(I\\times J\\). La hipótesis nula de independencia estadística es: \\[ H_0:\\pi_{ij}=\\pi_{i+}\\pi_{+j},\\qquad \\text{ para toda }\\;\\; i \\;\\;\\text{ y }\\;\\; j. \\] 5.6.1 La prueba \\(\\chi^2\\) de Pearson Ya estudiamos la prueba para valores específicos de probabilidades multinomiales. Una prueba de \\(H_0:\\mbox{independencia}\\) utiliza la \\(\\chi^2\\) con \\(n_{ij}\\) en lugar de \\(n_i\\) y con \\(\\mu_{ij}=n\\pi_{i+}\\pi_{+j}\\) en lugar de \\(\\mu_i\\). Bajo \\(H_0\\): \\[E(n_{ij}) = \\mu_{ij}\\] Usualmente \\(\\{\\pi_{i+}\\}\\) y \\(\\{\\pi_{+j}\\}\\) son conocidas. Sus estimadores de máxima verosimilitud son \\(\\hat{\\pi}_{i+}=n_{i+}/n\\) y \\(\\hat{\\pi}_{+j}=n_{+j}/n\\). Las frecuencias esperadas estimadas son \\[ \\{\\hat{\\mu}_{ij} = n\\hat{\\pi}_{i+}\\hat{\\pi}_{+j}=n_{i+}n_{+j}/n^2\\} \\] Por lo tanto, el estadístico de Pearson es: \\[ X^2 = \\displaystyle{\\sum_{i}\\sum_{j}{\\dfrac{(n_{ij}-\\hat{\\mu}_{ij})^2}{\\hat{\\mu}_{ij}}}}. \\] En 1900, el mismo Karl Pearson argumento que reemplazar las \\(\\{\\mu_{ij}\\}\\) por sus estimadores \\(\\{\\hat{\\mu}_{ij}\\}\\) no afectaría la distribución muestral cuando se tiene una muestra grande. Como la tabla de contingencia tiene \\(IJ\\) categorías, Pearson argumentó que la \\(X^2\\) se distribuye como chi cuadrada asintóticamente con grados de libertad \\(IJ-1\\). Sin embargo, años después (en 1922) Fisher publicó un artículo corrigiendo el error de Pearson. Lo que sucede es lo siguiente: estimar \\(\\{\\hat{\\mu}_{ij}\\}\\) requiere de estimar \\(\\{\\pi_{i+}\\}\\) y \\(\\{\\pi_{+j}\\}\\), por lo que los grados de libertad son: \\[ (IJ - 1) - (I-1) - (J-1) = (I-1)(J-1). \\] El estadístico de cociente de verosimilitud Para una muestra multinomial, el kernel de la verosimilitud es \\[ \\prod_i \\prod_j{\\pi_{ij}^{n_{ij}}},\\qquad \\;\\text{donde todas }\\;\\; \\pi_{ij}\\geq0\\;\\; \\mbox{y}\\;\\;\\sum_i \\sum_j{\\pi_{ij}}=1. \\] Bajo \\(H_0:\\text{independencia}\\), \\(\\hat{\\pi}_{ij}=\\hat{\\pi}_{i+}\\hat{\\pi}_{+j}=n_{i+}n_{+j}/n^2\\). En el caso general, \\(\\hat{\\pi}_{ij}=n_{ij}/n\\). El cociente de verosimilitud es igual a \\[ \\Lambda = \\dfrac{\\prod_i \\prod_j (n_{i+}n_{+j})^{n_{ij}}}{n^n\\prod_i\\prod_j{n_{ij}^{n_{ij}}}}. \\] El estadístico del cociente de verosimilitud es \\(-2\\mbox{log}(\\Lambda)\\). Denotado por \\(G^2\\), es igual a: \\[ G^2 = -2\\mbox{log}(\\Lambda) = 2\\sum_i\\sum_j{n_{ij}\\mbox{log}(n_{ij}/\\hat{\\mu}_{ij})} \\] Entre más grandes sean los valores de \\(G^2\\) y \\(X^2\\), mayor evidencia de independencia. En el caso general el espacio consiste de \\(\\{\\pi_{ij}\\}\\) sujeto a la restricción lineal de que deben sumar \\(1\\). El espacio de parámetros tiene dimensión \\(IJ-1\\). Bajo \\(H_0\\) el espacio está determinado por \\(\\{\\pi_{i+}\\}\\) y \\(\\{\\pi_{+j}\\}\\), por lo que su dimensión es de \\((I-1) + (J-1)\\). La diferencia entre estas dimensiones es \\((I-1)(J-1)\\). Para muestras grandes, \\(G^2\\) tiene una distribución nula \\(\\chi^2\\) con grados de libertad \\((I-1)(J-1)\\). Por lo que \\(G^2\\) y \\(X^2\\) tienen la misma distribución límite. De hecho, son asintóticamente equivalentes: \\(X^2 - G^2\\) converge en probabilidad a \\(0\\). 5.6.2 Ejemplo: brecha de género gendergap &lt;- matrix(c(279,73,225,165,47,191), byrow = T, ncol = 3) dimnames(gendergap) &lt;- list(Gender=c(&quot;Female&quot;,&quot;Male&quot;), PartyID=c(&quot;Democrat&quot;,&quot;Independent&quot;,&quot;Republican&quot;)) gendergap %&gt;% knitr::kable() Democrat Independent Republican Female 279 73 225 Male 165 47 191 La prueba de \\(\\chi^2\\) de Pearson se puede realizar con la función chisq.test que ya está en R base: chisq.test(gendergap) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: gendergap #&gt; X-squared = 7, df = 2, p-value = 0.03 5.7 General Social Survey 1972 - 2016 La Encuesta Social General (GSS) es una encuesta sociológica creada y recopilada desde 1972 por el Centro Nacional de Investigación de Opinión de la Universidad de Chicago. La GSS recopila información y mantiene un registro histórico de las preocupaciones, experiencias, actitudes y prácticas de los residentes de los Estados Unidos. Consideremos las variables de interés de la siguiente tabla: Variable Descripción Niveles YEAR Año del encuestado 1972 - 2016 GOD Grado de creencia en Dios No cree, no hay forma de saberlo, cree un un ser superior, cree en ocasiones, cree pero con dudas, sabe que existe. PARTYID Identificación con un partido político. Fuertemente democrático, algo democrático, independiente, algo republicano, fuertemente republicano, otro partido EDUC Educación del encuestado Años de educación desde 1ro de primaria hasta 8 años de educación universitaria HAPPY Grado de felicidad del encuestado Muy feliz, algo feliz, no muy feliz, POSTLIFE Creencia de la vida después de la muerte Sí, No WRKSTAT Tipo de situación laboral Tiempo completo, tiempo parcial, con trabajo pero de vacaciones o incapacidad, desempleado, retirado, en la escuela, al cuidado de la casa, otra FINRELA Ingreso con respecto a la media nacional Muy por debajo, debajo, promedio, encima, muy por encima del promedio Más información en: http://gss.norc.org/. gss &lt;- read_csv(&quot;datos/gss.csv&quot;) Veamos los datos de la encuesta del 2006 al 2016. Creamos una variable categórica para analizar la variable de educación más facilamente: gss_0616 &lt;- gss %&gt;% filter(YEAR &gt;= 2006 &amp; YEAR &lt;= 2016) %&gt;% mutate(EDUC1 = ifelse(EDUC &lt; 12, &quot;Some HS&quot;, ifelse(EDUC == 12, &quot;HS&quot;, ifelse(EDUC &lt; 16, &quot;Some College&quot;, ifelse(EDUC == 16, &quot;College&quot;, &quot;Graduate&quot;)))), EDUC1 = ordered(EDUC1, levels=c(&quot;Some HS&quot;,&quot;HS&quot;, &quot;Some College&quot;, &quot;College&quot;, &quot;Graduate&quot;)), GOD1 = ordered(GOD, levels=c(&quot;DONT BELIEVE&quot;,&quot;NO WAY TO FIND OUT&quot;,&quot;SOME HIGHER POWER&quot;, &quot;BELIEVE SOMETIMES&quot;,&quot;BELIEVE BUT DOUBTS&quot;,&quot;KNOW GOD EXISTS&quot;))) %&gt;% select(EDUC1, GOD1) %&gt;% drop_na() Veamos la tabla de contingencias: table(gss_0616$EDUC1, gss_0616$GOD1) %&gt;% knitr::kable() DONT BELIEVE NO WAY TO FIND OUT SOME HIGHER POWER BELIEVE SOMETIMES BELIEVE BUT DOUBTS KNOW GOD EXISTS Some HS 53 60 134 98 288 1607 HS 105 146 342 140 631 2551 Some College 113 216 461 155 620 2221 College 93 163 345 98 413 1235 Graduate 106 200 332 77 377 910 A esta gráfica le llamamos gráfica de mosaicos: ggplot(gss_0616, aes(x=EDUC1, fill=GOD1)) + geom_bar(position=&#39;fill&#39;) + coord_flip() + theme(aspect.ratio = 1,legend.position=&quot;bottom&quot;, axis.text.y=element_text(color=&#39;black&#39;,size=10), axis.text.x=element_text(color=&#39;black&#39;,size=10), axis.title.x=element_text(size=12), axis.title.y=element_text(size=12), legend.text=element_text(size=11)) + scale_fill_discrete(&quot;&quot;) + ylab(&#39;Proporción&#39;) Comparamos pruebas de hipótesis con los estadísticos \\(G^2\\) y \\(X^2\\): library(Deducer) likelihood.test(gss_0616$EDUC1, gss_0616$GOD1) #&gt; #&gt; Log likelihood ratio (G-test) test of independence without #&gt; correction #&gt; #&gt; data: gss_0616$EDUC1 and gss_0616$GOD1 #&gt; Log likelihood ratio statistic (G) = 500, X-squared df = 20, #&gt; p-value &lt;2e-16 chisq.test(gss_0616$EDUC1, gss_0616$GOD1) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: gss_0616$EDUC1 and gss_0616$GOD1 #&gt; X-squared = 500, df = 20, p-value &lt;2e-16 Estas estadísticas pueden ser evidencia de una asociación estadística fuerte. Para entender mejor la naturaleza de la evidencia en contra de \\(H_0\\) es necesario comparar casilla por casilla las frecuencias estimadas y observadas. Definimos los residuales ajustados como \\[ r_{ij} = \\dfrac{n_{ij} - \\hat{\\mu}_{ij}}{\\sqrt{\\hat{\\mu}_{ij}(1-p_{i+})(1-p_{+j})}}. \\] Bajo \\(H_0\\) cada \\(r_{ij}\\) tiene una distribución muestral aproximada normal estándar. Si en una casilla \\(r_{ij}\\) excede 2 en valor absoluto, entonces esto significa que en esa casilla el modelo de independencia (\\(H_0\\)) no es muy apropiado. El signo describe la naturaleza de la asociación. Veamos el cálculo de los residuales: tabla &lt;- table(gss_0616$EDUC1, gss_0616$GOD1) rowsum &lt;- apply(tabla,1,sum) colsum &lt;- apply(tabla,2,sum) n &lt;- sum(tabla) gd &lt;- outer(rowsum, colsum/n) rowp &lt;- rowsum/n #Prob. margimales renglón colp &lt;- colsum/n #Prob. marginales columna pd &lt;- outer(1-rowp,1-colp) resid &lt;- (tabla-gd)/sqrt(gd*pd) resid %&gt;% knitr::kable() DONT BELIEVE NO WAY TO FIND OUT SOME HIGHER POWER BELIEVE SOMETIMES BELIEVE BUT DOUBTS KNOW GOD EXISTS Some HS -2.67 -6.367 -8.65 1.056 -4.802 12.70 HS -2.50 -5.685 -5.94 -1.499 -0.359 8.25 Some College -1.23 0.667 2.00 0.438 0.152 -1.44 College 2.00 3.376 5.70 0.544 1.863 -7.59 Graduate 5.43 9.522 8.06 -0.318 3.309 -13.96 En la columna “God Know Exists” observamos residuales particularmente grandes: con signo positivo: 12.70 para “Some HS” y 8.25 para “HS”, significa que el número observado de personas que “saben que Dios existe” entre los encuestados con secundario o menor grado de estudios es mayor que el esperado. con signo negativo: -7.59 para “Collge” y -13.96 “Graduate”, significa que el número observado de personas que “saben que Dios existe” entre las que fueron a la universidad (licenciatura o posgrado) es menor que el esperado. De la tabla se podría concluir que el grado de estudios tiene una asociación negativ con el gardo de creencia en la existencia de Dios. 5.8 La catadora de té Una tarde del verano de 1920 en Cambridge, Inglaterra, Ronald Fisher tomaba el té en la terraza con sus colegas y amigos. La reunión había progresado complacientemente y en una ocasión cuando las tazas de té las volvían a llenar, Lady Muriel Bristol abruptadmente le dijo al mesero que parara de rellenar su taza. Lady Bristol indicó con desdeño que el mesero había puesto en la taza primero la leche y luego el té, en lugar de cumplir con una preferencia ampliamente conocida de té primero y luego leche. Miradas de reojo fueron intercambiadas por numerosos miembros del grupo, preguntándose qué diferencia podría haber con que se añadiera a la taza primero la leche o primero el té. Hacía toda la diferencia, según Lady Bristol, afirmando que fácilmente ella podía decir si se había vertido primero en la taza la leche o el té. Muy a salvo de la vista de Lady Bristol, se prepararon 8 tazas de té, en las cuales en 4 se virtió primero la leche y luego el té, y en las 4 restantes se virtió primero la leche y luego el té, siempre en las mismas proporciones. Muy amablemente, Lady Bristol cató las 8 tazas de té y dio su veredicto sobre cuáles de las 4 tazas eran aquellas en las q se sirvió primero la leche y luego el té. Los resultados obtenidos a partir de esta cata de té se muestran en la siguiente tabla: Poured &lt;- c(&quot;Milk&quot;,&quot;Milk&quot;,&quot;Tea&quot;,&quot;Tea&quot;) Guess &lt;- c(&quot;Milk&quot;,&quot;Tea&quot;,&quot;Milk&quot;,&quot;Tea&quot;) count &lt;- c(3,1,1,3) teadata &lt;- tibble(Poured, Guess, count) tea &lt;- xtabs(count ~Poured + Guess, data = teadata) tea %&gt;% knitr::kable() Milk Tea Milk 3 1 Tea 1 3 Utilizando el método de la \\(\\chi^2\\) de Pearson hacemos la prueba de independencia \\(H_0:\\theta=1\\): chisq.test(tea, correct = FALSE) #&gt; Warning in chisq.test(tea, correct = FALSE): Chi-squared approximation may #&gt; be incorrect #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: tea #&gt; X-squared = 2, df = 1, p-value = 0.2 Obtenemos el mensaje de que la prueba podría ser incorrecta. Esto no nos sorprende ya que la prueba no tiene mucha potencia debido al poco número de observaciones. ¡Fisher no le daría a probar 30 tazas a Lady Bristol sin convertirse en el objeto de su desdeño! El método de R implementa la prueba de \\(\\chi^2\\) utilizando la corrección de Yates cuando el tamaño de muestra es pequeño: \\[ X^2_Y = \\sum_{i}\\sum_j{\\dfrac{(|n_{ij}-\\hat{\\mu}_{ij}|-\\frac{1}{2})^2}{\\hat{\\mu}_{ij}}}\\sim\\chi^2_{(I-1)(J-1)} \\] En el libro The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century escrito por David Salsburg, se describe con detalle este suceso y los experimentos diseñados por Ronald Fisher para probar las afirmaciones de Lady Bristol. 5.9 Modelos multinomiales para conteos Condicional a la suma \\(n\\) de conteos de casillas en tablas de contingencia, los modelos log-lineales Poisson para \\(\\{\\mu_{ij}\\}\\) se convierten en modelos multinomiales para las probabilidades de las casillas \\[ \\left\\{\\pi_{ij} = \\mu_{ij}/\\sum_a\\sum_b{\\mu_{ab}}\\right\\} \\] Por ejemplo, para el modelo saturado: \\[ \\pi_{ij} = \\dfrac{\\mbox{exp}(\\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_{ij}^{XY})}{\\sum_a\\sum_b{\\mbox{exp}(\\lambda+\\lambda_a^X+\\lambda_b^Y+\\lambda_{ab}^{XY})}}. \\] El parámetro de intercepto \\(\\lambda\\) se cancela en este modelo multinomial. Este parámetro está relacionado con el tamaño total de la muestra, que es aleatorio en el modelo Poisson, pero no lo es en el modelo multinomial. Por esta razón, el modelo multinomial saturado tiene \\(IJ-1\\) parámetros, que representa la restricción usual de las probabilidades, o sea, \\(\\sum_i\\sum_j{\\pi_{ij}}=1\\). Por esta razón ajustar un modelo lineal Poisson con función liga \\(\\eta = log(\\mu)\\). Veremos esto con más detalle en las siguientes clases. Recordemos el ejemplo de las admisiones de posgrado de Berkeley. library(MASS) UCBAdmissions %&gt;% as.data.frame() %&gt;% knitr::kable() Admit Gender Dept Freq Admitted Male A 512 Rejected Male A 313 Admitted Female A 89 Rejected Female A 19 Admitted Male B 353 Rejected Male B 207 Admitted Female B 17 Rejected Female B 8 Admitted Male C 120 Rejected Male C 205 Admitted Female C 202 Rejected Female C 391 Admitted Male D 138 Rejected Male D 279 Admitted Female D 131 Rejected Female D 244 Admitted Male E 53 Rejected Male E 138 Admitted Female E 94 Rejected Female E 299 Admitted Male F 22 Rejected Male F 351 Admitted Female F 24 Rejected Female F 317 Ajustamos los parámetros para el modelo llmFit &lt;- loglm(~ Admit + Gender + Dept, data = UCBAdmissions) coef(llmFit) #&gt; $`(Intercept)` #&gt; [1] 5.18 #&gt; #&gt; $Admit #&gt; Admitted Rejected #&gt; -0.228 0.228 #&gt; #&gt; $Gender #&gt; Male Female #&gt; 0.191 -0.191 #&gt; #&gt; $Dept #&gt; A B C D E F #&gt; 0.2305 -0.2363 0.2143 0.0666 -0.2380 -0.0370 Comparamos utilizando el modelo Poisson: UCBdf &lt;- as.data.frame(UCBAdmissions) glmFit &lt;- glm(Freq ~ Admit + Gender + Dept, family=poisson(link = &quot;log&quot;), contrasts=list(Admit=contr.sum, Gender=contr.sum, Dept=contr.sum), data=UCBdf) coef(glmFit) #&gt; (Intercept) Admit1 Gender1 Dept1 Dept2 Dept3 #&gt; 5.1776 -0.2284 0.1914 0.2305 -0.2363 0.2143 #&gt; Dept4 Dept5 #&gt; 0.0666 -0.2380 5.10 Modelos log lineales con tres variables categóricas En la primera clase vimos ejemplos de modelos log lineales con tres variables categóricas. Sabemos construir las tablas de contingencia de tres variables con la función xtabs() y ajustar los modelos con las funciones loglin(), loglm(), y glm(). Vimos que se podía ajustar modelos de independencia y en general, de diferentes tipos de asociaciones. Una tabla de contingencia de \\(I\\times J \\times K\\) con variables de respuesta \\(X, Y\\) y \\(Z\\) potencialmente tiene varios tipos de independencias. Los modelos se aplican a la distribución multinomial con probabilidades de celdillas \\(\\{\\pi_{ijk}\\}\\) en las cuales \\(\\sum_i\\sum_j\\sum_k\\pi_{ijk}=1\\) y también a muestreo con Poisson con medias \\(\\mu_{ijk}\\). 5.10.1 Tipos de independencia Independencia mutua Las tres variables son mutuamente independientes cuando \\[ \\pi_{ijk}=\\pi_{i++}\\pi_{+j+}\\pi_{++k}\\qquad\\;\\text{para toda}\\;i,j,\\,\\mbox{ y }\\, k. \\] Para frecuencias esperadas \\(\\mu_{ijk}\\), la independencia mutua tiene un modelo loglineal de la forma \\[ \\mbox{log}\\,\\mu_{ijk} = \\lambda+\\lambda_{i}^X+\\lambda_j^Y+\\lambda_k^Z. \\] Independencia conjunta La variable \\(Y\\) es conjuntamente independiente de \\(X\\) y \\(Z\\) cuando \\[ \\pi_{ijk}=\\pi_{i+k}\\pi_{+j+}\\qquad\\;\\text{para toda}\\;i,j,\\,\\mbox{ y }\\, k. \\] Esto es equivalente a tener independencia entre la variable \\(Y\\) y una variable con las \\(IK\\) combinaciones de los niveles de \\(X\\) y \\(Z\\). El modelo loglineal es: \\[ \\mbox{log}\\,\\mu_{ijk} = \\lambda+\\lambda_{i}^X+\\lambda_j^Y+\\lambda_k^Z + \\lambda_{ik}^{XZ}. \\] Similarmente, \\(X\\) podría ser conjuntamente independiente de \\(Y\\) y \\(Z\\), o bien, \\(Z\\) podría ser conjuntamente independiente de \\(X\\) y \\(Y\\). La independencia mutua implica independencia conjunta de cualquier variable con las otras dos. Independencia condicional Las variables categóricas \\(X\\) y \\(Y\\) son condicionalmente independientes dado \\(Z\\) cuando se cumple la independencia para cada tabla parcial cuando cada \\(Z\\) permanece fija. Esto es, si \\(\\pi_{ij|k}=P(X=i,Y=j|Z=k)\\), entonces \\[ \\pi_{ij|k}=\\pi_{i+|k}\\pi_{+j|k}\\qquad\\;\\text{para toda}\\;i,j,\\,\\mbox{ y }\\, k. \\] Para probabilidades conjuntas en toda la tabla esto es equivalente: \\[ \\pi_{ijk}=\\pi_{i+k}\\pi_{+jk}/\\pi_{++k}\\qquad\\;\\text{para toda}\\;i,j,\\,\\mbox{ y }\\, k. \\] La independencia condicional de \\(X\\) y \\(Y\\), dado \\(Z\\), tiene un modelo loglineal de la forma \\[ \\mbox{log}\\,\\mu_{ijk} = \\lambda+\\lambda_i^X+\\lambda_j^Y + \\lambda_k^Z+ \\lambda_{ik}^{XZ} + \\lambda_{jk}^{YZ}. \\] Esta es una condición más débil de independencia conjunta. La independencia mutua implica que \\(Y\\) es conjuntamente independiente de \\(X\\) y \\(Z\\), la cual implica que \\(X\\) y \\(Y\\) son condicionalmente independientes. En la siguiente tabla resumimos los tres tipos de independencia: Modelo Forma probabilística para \\(\\pi_{ijk}\\) Términos de asociación en el modelo loglineal Interpretación 1 \\(\\pi_{i++}\\pi_{+j+}\\pi_{++k}\\) Ninguno Independencia mutua de \\(X,Y,Z\\) 2 \\(\\pi_{i+k}\\pi_{+j+}\\) \\(\\lambda_{ik}^{XZ}\\) Independencia conjunta de \\(Y\\) con \\(X,Z\\) 3 \\(\\pi_{i+k}\\pi_{+jk}/\\pi_{++k}\\) \\(\\lambda_{ik}^{XZ}+\\lambda_{jk}^{YZ}\\) Independencia condicional de \\(X\\) y \\(Y\\) dado \\(Z\\) 5.10.2 Asociación homogénea e interacciones de 3 factores Los modelos (1), (2), y (3) tienen tres, dos, y un par de variables condicionalmente independientes, respectivamente. En los modelos (2) y (3) los parámetros con doble subíndice (tales como \\(\\lambda_{ij}^{XY}\\)) representan dependencias condicionales entre las variables. Un modelo con las tres dependencias condicionales es \\[ \\mbox{log}\\,\\mu_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda_{ij}^{XY} + \\lambda_{ik}^{XZ} + \\lambda_{jk}^{YZ}. \\] Si exponenciamos ambas partes de la ecuación, las probabilidades en cada casilla tienen la forma \\[ \\pi_{ijk} = \\psi_{ij}\\phi_{jk}\\omega_{ik}. \\] No existe una expresión cerrada para los tres factores en términos de los márgenes de las \\(\\{\\pi_{ijk}\\}\\) en el caso general. Se puede demostrar que las razones de momios entre caulesquiera dos pares de variables son idénticos en cada categoría de la tercera variable. A este modelo se le llama el modelo loglineal de asociación homogénea, o bien, el modelo de no interacción entre los 3 factores. El modelo loglineal general es \\[ \\mbox{log}\\, \\mu_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda_{ij}^{XY} + \\lambda_{jk}^{YZ} + \\lambda_{jk}^{YZ} + \\lambda_{ijk}^{XYZ} \\] Con variables indicadoras, \\(\\lambda_{ijk}^{XYZ}\\) es el coeficiente del producto de la \\(i\\)-ésima variable indicadora de \\(X\\), la \\(j\\)-ésima de \\(Y\\), y la \\(k\\)-ésima de \\(Z\\). El número total de parámetros no redundates es \\[\\begin{eqnarray*} 1 &amp;+&amp; (I-1) + (J-1) + (K-1) + (I-1)(J-1) + (I-1)(K-1) \\\\ &amp;+&amp; (J-1)(I-1) + (I-1)(J-1)(K-1) = IJK, \\end{eqnarray*}\\] que es el número total de conteos de casillas. Este modelo tiene tantos parámetros como observaciones y es saturado. Describe todas las posibles \\(\\{\\mu_{ijk}\\}\\). Cada par de variables pueden ser condicionalmente dependientes, y las razones de momios para cada par pueden variar a lo largo de todas las categorías de la tercera variable. Poniendo algunos términos como cero obtenemos cualquiera de los modelos anteriores. En la siguiente tabla resumimos los modelos. Para facilitar cuando nos referimos a ellos en la tabla le asignamos un símbolo que pone en la lista únicamente el (los) términos de mayor orden para cada variable. Por ejemplo, el modelo (3) de independencia condicional se codifica como (XZ, YZ), porque sus términos de mayor orden son \\(\\lambda_{ik}^{XZ}\\) y \\(\\lambda_{jk}^{YZ}\\). Fórmula del modelo loglineal Símbolo \\(\\mbox{log}\\, \\mu_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y,+,\\lambda_k^Z\\) (X,Y,Z) \\(\\mbox{log}\\, \\mu_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y,+,\\lambda_k^Z + \\lambda_{ij}^{XY}\\) (XY,Z) \\(\\mbox{log}\\, \\mu_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y,+,\\lambda_k^Z + \\lambda_{ij}^{XY} + \\lambda_{jk}^{YZ}\\) (XY,YZ) \\(\\mbox{log}\\, \\mu_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y +,\\lambda_k^Z + \\lambda_{ij}^{XY} + \\lambda_{jk}^{YZ} + \\lambda_{jk}^{YZ}\\) (XY,YZ,XZ) \\(\\mbox{log}\\, \\mu_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda_{ij}^{XY} + \\lambda_{jk}^{YZ} + \\lambda_{jk}^{YZ} + \\lambda_{ijk}^{XYZ}\\) (XYZ) Finalmente, en la siguiente tabla resumímos el número de grados de libertad de los estadísticos \\(G^2\\) y \\(X^2\\) que tienen una distribución muestral aproximada a \\(\\chi^2\\). El número de grados de libertad es igual a la diferencia entre el número de parámetros en el caso general y cuando el modelo se cumple. En el caso general hay \\(IJK-1\\) parámetros. Modelo Grados de libertad \\((X,Y,Z)\\) \\(IJK - I - J - K + 2\\) \\((XY,Z)\\) \\((K-1)(IJ-1)\\) \\((XZ,Y)\\) \\((J-1)(IK-1)\\) \\((YZ,X)\\) \\((I-1)(JK-1)\\) \\((XY,YZ)\\) \\(J(I-1)(K-1)\\) \\((XZ,YZ)\\) \\(K(I-1)(J-1)\\) \\((XY,XZ)\\) \\(I(J-1)(K-1)\\) \\((XY,XZ,YZ)\\) \\((I-1)(J-1)(K-1)\\) \\((XYZ)\\) \\(0\\) 5.11 Ejemplo: sensitividad y especificidad En la siguiente tabla se muestran resultados de un artículo reciente sobre varios métodos para tratar de diagnosticar el VIH. Delaney, K. P., Branson, B. M., Uniyal, A., Phillips, S., Candal, D., Owen, S. M., &amp; Kerndt, P. R. (2011). Evaluation of the performance characteristics of 6 rapid HIV antibody tests. Clinical Infectious Diseases, 52(2), 257-263. El paciente tiene VIH Diagnóstico positivo Diagnóstico negativo Total Sí 0.99 0.01 1.0 No 0.05 0.95 1.0 Con base en una revisión de la literatura, los autores concluyen que, como el rendimiento fue similar para todas las pruebas rápidas y todos los tipos de especímenes, entonces otras características, como la conveniencia, el tiempo para obtener el resultado, la vida útil y el costo serán factores determinantes para la selección de una prueba rápida de detección del VIH para una aplicación específica. Sea \\(X=\\) verdadero valor de la enfermedad (si el paciente tiene VIH o no) y sea \\(Y=\\) diagnóstico (positivo o negativo), donde un diagnóstico positivo predice que el individuo tiene VIH. Las probabilidades mostradas en la tabla son probabilidades condicionales de \\(Y\\) dado \\(X\\). Con una prueba de diagnóstico de una enfermedad, los dos diagnósticos correctos son: un resultado positivo de la prueba cuando el individuo tiene la enfermedad, y un resultado negativo cuando el individuo no la tiene. Dado que el individuo tiene la enfermedad, la probabilidad condicional de que la prueba de diagnóstico sea positiva se denomina sensibilidad. Dado que el individuo no tiene la enfermedad, la probabilidad condicional de que la prueba sea negativa se llama especificidad. Idealmente, estos son altos. En una tabla de \\(2\\times 2\\) como la que se muestra arriba, la sensibilidad es \\(\\pi_{1|1}\\) y la especificidad es \\(\\pi_{2|2}\\). En la tabla de de arriba, la sensibilidad estimada de las pruebas rápidas del VIH es \\(0.99\\). Entre los individuos con VIH, el 99% es diagnosticado correctamente. La especificidad estimada es 0.95. Entre los individuos que no tienen VIH, el 95% es diagnosticado correctamente. 5.12 Ejemplo: horóscopos Supongamos que deseamo saber si los horóscopos son realmente sólo un figmento de la imaginación de las personas. Se hizo una encuesta con 2201 personas. Se les pidió su signo zodiacal (esta variable, obviamente, tiene 12 categorías: Capricornio, Acuario, Piscis, Aries, Tauro, Géminis, Cáncer, Leo, Virgo, Libra, Escorpio y Sagitario) y que contestaran si creían en los horóscopos (estas dos variables tienen dos categorías: creen o no creen). Posteriormente a todos los sujetos se les envió exactamente el mismo horóscopo sobre cómo sería el siguiente mes y al finalizar el mes se les preguntó si su horóscopo se cumplió o no. Buscamos realizar un análisis loglineal para ver si existe una relación entre el signo zodiacal de la persona, si cree en los horóscopos y si el horóscopo se hizo realidad. Comenzamos leyendo los datos: horoscopos &lt;- read_csv(&quot;datos/horoscopos.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; Star_Sign = col_character(), #&gt; Believe = col_character(), #&gt; True = col_character(), #&gt; Frequency = col_integer() #&gt; ) horoscopos %&gt;% head() %&gt;% knitr::kable() Star_Sign Believe True Frequency Capricorn Unbeliever Horoscope Didn’t Come True 56 Capricorn Unbeliever Horoscope Came True 46 Capricorn Believer Horoscope Didn’t Come True 50 Capricorn Believer Horoscope Came True 60 Aquarius Unbeliever Horoscope Didn’t Come True 26 Aquarius Unbeliever Horoscope Came True 20 Hacemos la tabla de contingencia con la función xtabs() y tabla_horoscopos &lt;- xtabs(Frequency ~ Star_Sign + Believe + True, data = horoscopos) tabla_horoscopos #&gt; , , True = Horoscope Came True #&gt; #&gt; Believe #&gt; Star_Sign Believer Unbeliever #&gt; Aquarius 29 20 #&gt; Aries 54 36 #&gt; Cancer 83 76 #&gt; Capricorn 60 46 #&gt; Gemini 48 53 #&gt; Leo 20 23 #&gt; Libra 36 26 #&gt; Pisces 70 51 #&gt; Sagittarius 50 41 #&gt; Scorpio 32 20 #&gt; Taurus 50 42 #&gt; Virgo 66 55 #&gt; #&gt; , , True = Horoscope Didn&#39;t Come True #&gt; #&gt; Believe #&gt; Star_Sign Believer Unbeliever #&gt; Aquarius 22 26 #&gt; Aries 70 42 #&gt; Cancer 96 84 #&gt; Capricorn 50 56 #&gt; Gemini 40 65 #&gt; Leo 12 14 #&gt; Libra 22 27 #&gt; Pisces 64 55 #&gt; Sagittarius 42 56 #&gt; Scorpio 24 32 #&gt; Taurus 41 56 #&gt; Virgo 49 69 Comenzamos el análisis con el modelo saturado: horoscopo_saturado &lt;- loglm(Frequency ~ Star_Sign*Believe*True, data = tabla_horoscopos) horoscopo_saturado #&gt; Call: #&gt; loglm(formula = Frequency ~ Star_Sign * Believe * True, data = tabla_horoscopos) #&gt; #&gt; Statistics: #&gt; X^2 df P(&gt; X^2) #&gt; Likelihood Ratio 0 0 1 #&gt; Pearson 0 0 1 Podemos ver que el número de grados de libertad es cero, esto es, el número de parámetros es igual al número de observaciones. Quitamos la interacción de los 3 factores y vemos la diferencia entre ambos modelos utilizando la función anova(): horoscopo_homogeneo &lt;- update(horoscopo_saturado, .~. -Star_Sign:Believe:True) anova(horoscopo_saturado, horoscopo_homogeneo) #&gt; LR tests for hierarchical log-linear models #&gt; #&gt; Model 1: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Star_Sign:True + Believe:True #&gt; Model 2: #&gt; Frequency ~ Star_Sign * Believe * True #&gt; #&gt; Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev) #&gt; Model 1 8.84 11 #&gt; Model 2 0.00 0 8.84 11 0.637 #&gt; Saturated 0.00 0 0.00 0 1.000 Ahora podemos quitar las interacciones una a la vez: BelieveTrue &lt;- update(horoscopo_homogeneo, .~. -Believe:True) Star_SignTrue &lt;- update(horoscopo_homogeneo, .~. -Star_Sign:True) Star_SignBelieve &lt;- update(horoscopo_homogeneo, .~. -Star_Sign:Believe) Podemos ver nuevamente las diferencias de cada uno con respecto al modelo de asociación homogénea: anova(horoscopo_homogeneo, Star_SignTrue) #&gt; LR tests for hierarchical log-linear models #&gt; #&gt; Model 1: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Believe:True #&gt; Model 2: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Star_Sign:True + Believe:True #&gt; #&gt; Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev) #&gt; Model 1 19.58 22 #&gt; Model 2 8.84 11 10.74 11 0.465 #&gt; Saturated 0.00 0 8.84 11 0.637 anova(horoscopo_homogeneo, BelieveTrue) #&gt; LR tests for hierarchical log-linear models #&gt; #&gt; Model 1: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Star_Sign:True #&gt; Model 2: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Star_Sign:True + Believe:True #&gt; #&gt; Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev) #&gt; Model 1 21.38 12 #&gt; Model 2 8.84 11 12.54 1 0.0004 #&gt; Saturated 0.00 0 8.84 11 0.6365 anova(horoscopo_homogeneo, Star_SignBelieve) #&gt; LR tests for hierarchical log-linear models #&gt; #&gt; Model 1: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:True + Believe:True #&gt; Model 2: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Star_Sign:True + Believe:True #&gt; #&gt; Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev) #&gt; Model 1 29.51 22 #&gt; Model 2 8.84 11 20.67 11 0.037 #&gt; Saturated 0.00 0 8.84 11 0.637 El modelo que consideraríamos más apropiado sería el que no incluye la interacción de Star_Sign y True. Comparamos con todos los modelos: anova(horoscopo_homogeneo, BelieveTrue, Star_SignTrue, Star_SignBelieve) #&gt; LR tests for hierarchical log-linear models #&gt; #&gt; Model 1: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Believe:True #&gt; Model 2: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:True + Believe:True #&gt; Model 3: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Star_Sign:True #&gt; Model 4: #&gt; Frequency ~ Star_Sign + Believe + True + Star_Sign:Believe + Star_Sign:True + Believe:True #&gt; #&gt; Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev) #&gt; Model 1 19.58 22 #&gt; Model 2 29.51 22 -9.93 0 1.0000 #&gt; Model 3 21.38 12 8.12 10 0.6166 #&gt; Model 4 8.84 11 12.54 1 0.0004 #&gt; Saturated 0.00 0 8.84 11 0.6365 Elegimos el modelo 1 (Star_Sign:True) porque tiene menor devianza para un número de parámetros aceptable (el número de grados de libertad es mayor). Esto implica que no hay asociación entre “si es verdad el horóscopo” y signo zodiacal, sin embargo sí existe una relación entre el signo zodiacal y si creen que el horóscopo es verdad. Por ejemplo, los Aries tienden a creer mucho en su horóscopo, pero los Géminis no. Star_SignTrue$param$Star_Sign.Believe #&gt; Believe #&gt; Star_Sign Believer Unbeliever #&gt; Aquarius 0.0302 -0.0302 #&gt; Aries 0.2104 -0.2104 #&gt; Cancer 0.0348 -0.0348 #&gt; Capricorn 0.0164 -0.0164 #&gt; Gemini -0.1680 0.1680 #&gt; Leo -0.0939 0.0939 #&gt; Libra 0.0237 -0.0237 #&gt; Pisces 0.0959 -0.0959 #&gt; Sagittarius -0.0478 0.0478 #&gt; Scorpio 0.0157 -0.0157 #&gt; Taurus -0.0584 0.0584 #&gt; Virgo -0.0590 0.0590 5.13 Tarea (opcional) Demuestra que si \\((n_1,n_2,\\ldots,n_c)\\) sigue una distribución multinomial, entonces \\(\\mbox{Cov}(n_i,n_j)=-n\\pi_i \\pi_j\\) para toda \\(i\\neq j\\). Para esto, define \\(Y_{ij}\\) como \\[ Y_{ij} = \\left\\{ \\begin{array}{cl} 1 &amp; \\text{si la }\\; i\\text{-esima observacion es la categoria }j,\\\\ 0 &amp; \\text{en otro caso.} \\end{array}\\right. \\] Sea \\(Y_i=(Y_{i1},\\ldots,Y_{ic})\\) el vector aleatorio correspondiente a la observación \\(i\\), tal que cada \\(Y_i\\) tiene parámetros \\[ \\pi = E(Y_i), \\qquad \\Sigma=\\mbox{Cov}(Y_i),\\quad i=1,\\ldots,n. \\] \\(Y_1,Y_2,\\ldots,Y_n\\) son independientes e idénticamente distribuidos. Calcula \\(\\sigma_{jk}\\) para \\(j\\neq k\\) y \\(\\sigma_{jj}\\). Escribe \\(\\mbox{Cov}(Y_i) = \\sigma\\) en forma matricial. Define \\(p=(n_1,\\ldots,n_c)\\) en términos de \\((Y_1, \\ldots, Y_n)\\). Calcula \\(\\mbox{Cov}(p)\\) y concluye que \\(\\mbox{Cov}(n_i,n_j)=-n\\pi_i\\pi_j\\) si \\(i\\neq j\\). Para probar \\(H_0:\\pi_j = \\pi_{j0}\\), \\(j=1,\\ldots,c\\) con proporciones multinomiales muestrales \\(\\{\\hat{\\pi}_j\\}\\) el estadístico del cociente de verosimilitud es \\[ G^2 = -2n \\sum_j{\\hat{\\pi}_j}\\,\\mbox{log}\\left(\\pi_{j0}/\\hat{\\pi}_j\\right). \\] Demuestra que \\(G^2 \\neq 0\\) y que la igualdad se da si y sólo si \\(\\hat{\\pi}_j=\\pi_{j0}\\) para toda \\(j\\). Para demostrar esto aplica la desigualdad de Jensen a \\(E(-2n\\,\\mbox{log}(X))\\), donde \\(X\\) es igual a \\(\\pi_{j0}/\\hat{\\pi_j}\\) con probabilidad \\(\\hat{\\pi}_j\\). Muestra que \\(X^2 \\leq n \\,\\cdot\\,\\mbox{min}(I-1,J-1)\\). Por lo cual, \\[ V^2=\\dfrac{X^2}{n \\,\\cdot\\,\\mbox{min}(I-1,J-1)} \\] está entre \\(0\\) y \\(1\\). A \\(V^2\\) se le llama la V de Cramér. Con los datos de la encuesta GSS realiza un análisis para responder las siguientes preguntas. ¿Las personas que creen en la vida después de la muerte son más felices? ¿Está asociada la creencia en la existencia de Dios con la afiliación política? Para esto combina las categorías de PARTYID de la siguiente forma: las categorías \\(0,1\\) para Demócratas, \\(2,3,4\\) para Independientes, y \\(5,6\\) para Republicanos. "],
["regresion-logistica-1.html", "Clase 6 Regresión logística 1 6.1 Regresión logística con un solo predictor 6.2 El modelo de regresión logística 6.3 Tarea", " Clase 6 Regresión logística 1 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } La regresión logística es la forma estándar de modelar los resultados binarios (es decir, los datos \\(y_i\\) que toman los valores \\(0\\) o \\(1\\)). 6.1 Regresión logística con un solo predictor library(arm) Los científicos políticos han estado interesados en la identificación de partidos y sus cambios a lo largo del tiempo. Ilustramos aquí con una serie de regresiones transversales que modelan la identificación del partido dada la ideología política y las variables demográficas. Usamos una encuesta llamada National Election Study. brdata &lt;- read_csv(&quot;datos/nes.csv&quot;) Contamos con los siguientes predictores: identificación con un partido en una escala de 1-7 (1 = demócrata fuerte, 2 = demócrata, 3 = demócrata débil, 4 = independiente, …, 7 = republicano fuerte), que tratamos como una variable continua. ideología política (1 = fuerte liberal, 2 = liberal,., 7 = fuerte conservador), etnia (0 = blanco, 1 = negro, 0.5 = otro), edad (como categorías: 18- 29, 30-44, 45-64 y más de 65 años, con la categoría de edad más baja como referencia), educación (1 = sin escuela secundaria, 2 = graduado de la escuela secundaria, 3 = algunos estudios universitarios, 4 = graduado de la universidad), sexo (0 = hombre, 1 = mujer), ingreso (1 = percentil 0-16, 2 = percentil 17-33, 3 = percentil 34-67, 4 = percentil 68-95, 5 = percentil 96-100). Los partidos conservadores generalmente reciben más apoyo entre los votantes con mayores ingresos. Veamos cómo usar la regresión logística clásica con un análisis simple de este patrón de las elecciones de Estados Unidos en 1992. datos &lt;- brdata %&gt;% filter(!is.na(black) &amp; !is.na(female) &amp; !is.na(educ1) &amp; !is.na(age) &amp; !is.na(income) &amp; !is.na(state)) %&gt;% filter(!is.na(year)) %&gt;% filter(year %in% 1952:2000) %&gt;% mutate(year.new = match(year, unique(year)), income.new = income - 3, age.new = (age - mean(age))/10, y = rep_pres_intent, age.discrete = as.numeric(cut(age, c(0,29.5, 44.5, 64.5, 200))), race.adj = ifelse(race &gt;= 3, 1.5, race) - 1, gender = gender - 1) Para cada encuestado ponemos \\(y_i=1\\) si el encuestado prefería a George Bush (el candidato Republicano a presidente) o \\(y_i=0\\) si prefería a Bill Clinton (el candidato Demócrata). Por ahora excluímos a los encuestados que preferían a Ross Perot u otros candidatos, o que no tenían opinión. datos_2 &lt;- datos %&gt;% filter(year == 1992 &amp; presvote &lt; 3) %&gt;% mutate(vote = presvote - 1) %&gt;% dplyr::select(year, age, age.new, gender, race.adj, educ1, vote, income, income.new) datos_2 %&gt;% sample_n(10) %&gt;% knitr::kable() year age age.new gender race.adj educ1 vote income income.new 1992 46 0.055 1 1.0 3 0 4 1 1992 40 -0.545 1 0.0 4 0 3 0 1992 61 1.555 0 0.0 4 1 4 1 1992 45 -0.045 1 1.0 3 0 3 0 1992 20 -2.545 0 0.5 3 0 1 -2 1992 79 3.355 1 1.0 2 0 1 -2 1992 77 3.155 1 0.0 2 0 3 0 1992 63 1.755 1 0.0 3 1 2 -1 1992 53 0.755 1 0.0 2 1 2 -1 1992 25 -2.045 1 0.0 4 0 3 0 Dado que nuestra variable de ingreso es categórica y la preferencia de voto que analizamos es binaria podríamos comenzar haciendo un análisis de datos categóricos: tab &lt;- round(prop.table(table(datos_2$income, datos_2$vote), 1)*100,2) %&gt;% as_tibble() %&gt;% spread(Var2, n) %&gt;% dplyr::select(Income=Var1, Clinton=`0`, Bush=`1`) tab %&gt;% knitr::kable() Income Clinton Bush 1 76.6 23.4 2 66.7 33.3 3 60.7 39.3 4 51.5 48.5 5 48.6 51.4 En una gráfica de mosaico vemos los siguiente: aux &lt;- datos_2 %&gt;% dplyr::select(c(&quot;vote&quot;,&quot;income&quot;)) %&gt;% mutate(vote1 = ordered(vote), income1 = ordered(income)) ggplot(aux, aes(x=income1, fill=vote1)) + geom_bar(position=&#39;fill&#39;) + coord_flip() + theme(aspect.ratio = 1,legend.position=&quot;bottom&quot;, axis.text.y=element_text(color=&#39;black&#39;,size=10), axis.text.x=element_text(color=&#39;black&#39;,size=10), axis.title.x=element_text(size=12), axis.title.y=element_text(size=12), legend.text=element_text(size=11)) + scale_fill_discrete(&quot;&quot;) + ylab(&#39;Proporción&#39;) Para podríamos introducir perfiles de variables de ingreso para obtener una mejor intuición. Utilizando el método de regresión logística obtendríamos lo siguiente: # Estimacion fit.1 &lt;- glm(vote ~ income, data = datos_2, family=binomial(link=&quot;logit&quot;)) display(fit.1) #&gt; glm(formula = vote ~ income, family = binomial(link = &quot;logit&quot;), #&gt; data = datos_2) #&gt; coef.est coef.se #&gt; (Intercept) -1.40 0.19 #&gt; income 0.33 0.06 #&gt; --- #&gt; n = 1179, k = 2 #&gt; residual deviance = 1556.9, null deviance = 1591.2 (difference = 34.3) Con este modelo buscamos predecir las preferencias dado el nivel de ingresos del encuestado, que se caracteriza por una escala de cinco puntos. El modelo ajustado es \\[ P(y_i=1) = \\mbox{logit}^{-1}(−1.40 + 0.33\\,\\cdot\\,\\mbox{income}) \\] Definimos así una función que llamamos logit inversa y de la cuál hablaremos más adelante: invlogit &lt;- function(x){ exp(x)/(1+exp(x)) } En la siguiente gráfica podemos ver los valores de la variable categórica de ingreso con un jitter y la función ajustada por el modelo: ggplot(datos_2, aes(x = income, y = vote)) + geom_jitter(width = 0.3, height = 0.08, size = 0.1) + stat_function(fun = function(x){invlogit(fit.1$coef[1] + fit.1$coef[2]*x)}, xlim = c(1,5), size=2) + stat_function(fun = function(x){invlogit(fit.1$coef[1] + fit.1$coef[2]*x)}, xlim = c(-3,10)) + scale_x_continuous(limits = c(-3, 10), breaks = 1:5) + scale_y_continuous(breaks = c(0,1), labels=c(&quot;Clinton&quot;,&quot;Bush&quot;)) En este ejemplo, estos puntos revelan muy poca información, pero queremos enfatizar que los datos y el modelo ajustado se pueden poner en una escala común. Definiremos matemáticamente este modelo y luego regresaremos para analizar su interpretación. 6.2 El modelo de regresión logística No tendría sentido ajustar el modelo de regresión lineal continuo, \\(X\\beta + \\mbox{error}\\), a los datos de \\(y\\), que toman los valores de \\(0\\) y \\(1\\). Notemos que intentar estimar las probabilidades de clase \\(p_1(x)\\) de forma lineal con \\[p_1(x)=\\beta_0+\\beta_1 x_1\\] tiene el defecto de que el lado derecho puede producir valores fuera de \\([0,1]\\). La idea es entonces aplicar una función \\(h\\) simple que transforme la recta real al intervalo \\([0,1]:\\) \\[p_1(x) = h(\\beta_0+\\beta_1 x_1),\\] donde \\(h\\) es una función que toma valores en \\([0,1]\\). ¿Cúal es la función más simple que hace esto? 6.2.1 Función logística Comenzamos con el caso más simple, poniendo \\(\\beta_0=0\\) y \\(\\beta_1=1\\), de modo que \\[p_1(x)=h(x).\\] ¿Cómo debe ser \\(h\\) para garantizar que \\(h(x)\\) está entre 0 y 1 para toda \\(x\\)? No van a funcionar polinomios, por ejemplo, porque para un polinomio cuando \\(x\\) tiende a infinito, el polinomio tiende a \\(\\infty\\) o a \\(-\\infty\\). Hay varias posibilidades, pero una de las más simples es la función logística. La función logística está dada por \\[h(x)=\\frac{e^x}{1+e^x}\\] h &lt;- function(x){exp(x)/(1+exp(x)) } curve(h, from=-6, to =6) Esta función comprime adecuadamente (para nuestros propósitos) el rango de todos los reales dentro del intervalo \\([0,1]\\). El modelo de regresión logística simple está dado por \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1)= \\frac{e^{\\beta_0+\\beta_1x_1}}{1+ e^{\\beta_0+\\beta_1x_1}},\\] y \\[p_0(x)=p_0(x;\\beta)=1-p_1(x;\\beta),\\] donde \\(\\beta=(\\beta_0,\\beta_1)\\). Este es un modelo paramétrico con 2 parámetros. Ejemplo (Impago de tarjetas de crédito) Supongamos que \\(X=\\) porcentaje del crédito máximo usado, y \\(G\\in\\{1, 2\\}\\), donde \\(1\\) corresponde al corriente y \\(2\\) representa impago. Las probabilidades condicionales de clase para la clase al corriente podrían ser, por ejemplo: \\(p_1(x) = P(G=1|X = x) =0.95\\) si \\(x &lt; 15\\%\\) \\(p_1(x) = P(G=1|X = x) = 0.95 - 0.007(x-15)\\) si \\(x&gt;=15\\%\\) Estas son probabilidades, pues hay otras variables que influyen en que un cliente permanezca al corriente o no en sus pagos más allá de información contenida en el porcentaje de crédito usado. Nótese que estas probabilidades son diferentes a las no condicionadas, por ejempo, podríamos tener que a total \\(P(G=1)=0.83\\). p_1 &lt;- function(x){ ifelse(x &lt; 15, 0.95, 0.95 - 0.007 * (x - 15)) } curve(p_1, 0,100, xlab = &#39;Porcentaje de crédito máximo&#39;, ylab = &#39;p_1(x)&#39;, ylim = c(0,1)) Vamos a generar unos datos con un modelo simple: set.seed(1933) x &lt;- pmin(rexp(500,1/30),100) probs &lt;- p_1(x) g &lt;- ifelse(rbinom(length(x), 1, probs)==1 ,1, 2) dat_ent &lt;- data_frame(x = x, p_1 = probs, g = factor(g)) dat_ent %&gt;% dplyr::select(x, g) #&gt; # A tibble: 500 x 2 #&gt; x g #&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 0.532 1 #&gt; 2 25.4 1 #&gt; 3 37.5 1 #&gt; 4 20.9 1 #&gt; 5 70.9 2 #&gt; 6 14.8 1 #&gt; # ... with 494 more rows Como este problema es de dos clases, podemos graficar como sigue: graf_1 &lt;- ggplot(dat_ent, aes(x = x)) + geom_jitter(aes(colour = g, y = as.numeric(g==&#39;1&#39;)), width=0, height=0.1) graf_1 Ahora intentaremos ajustar a mano (intenta cambiar las betas para p_mod_1 y p_mod_2 en el ejemplo de abajo) algunos modelos logísticos para las probabilidades de clase: graf_data &lt;- data_frame(x = seq(0,100, 1)) h &lt;- function(z) exp(z)/(1+exp(z)) p_logistico &lt;- function(beta_0, beta_1){ p &lt;- function(x){ z &lt;- beta_0 + beta_1*x h(z) } } p_mod_1 &lt;- p_logistico(-20, 1) p_mod_2 &lt;- p_logistico(3.2, -0.05) graf_data &lt;- graf_data %&gt;% mutate(p_mod_1 = p_mod_1(x), p_mod_2 = p_mod_2(x)) graf_verdadero &lt;- data_frame(x = 0:100, p_1 = p_1(x)) graf_1 + geom_line(data = graf_data, aes(y = p_mod_2), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_data, aes(y = p_mod_1), colour = &#39;orange&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Podemos usar también la función glm de R para ajustar los coeficientes: mod_1 &lt;- glm(g==1 ~ x, data = dat_ent, family = &#39;binomial&#39;) coef(mod_1) #&gt; (Intercept) x #&gt; 3.2447 -0.0435 p_mod_final &lt;- p_logistico(coef(mod_1)[1], coef(mod_1)[2]) graf_data &lt;- graf_data %&gt;% mutate(p_mod_f = p_mod_final(x)) graf_1 + geom_line(data = graf_data, aes(y = p_mod_f), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_data, aes(y = p_mod_1), colour = &#39;orange&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Por lo tanto, modelamos la probabilidad de que \\(y = 1\\), como \\[ P(y_i=1)=\\mbox{logit}^{-1}(X_i\\beta), \\] bajo el supuesto de que las respuestas \\(y_i\\) son independientes dadas estas probabilidades. Nos referimos a \\(X\\beta\\) como el predictor lineal. La función logística definida anteriormente \\(h(x)=\\dfrac{e^x}{1+e^x}\\) es \\(\\mbox{logit}^{-1}\\), la función inversa de logit. 6.3 Tarea Demostrar que, si \\(p_1(x)\\) está dado como \\[ p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1)= \\frac{e^{\\beta_0+\\beta_1x_1}}{1+ e^{\\beta_0+\\beta_1x_1}} \\] entonces también podemos escribir: \\[p_0(x)=\\frac{1}{1+e^{\\beta_0+\\beta_1x_1}}.\\] Graficar la función \\(p_1(x;\\beta)\\) para distintos valores de \\(\\beta_0\\) y \\(\\beta_1\\). "],
["regresion-logistica-2.html", "Clase 7 Regresión logística 2 7.1 Incertidumbre en la estimación 7.2 Función logística 7.3 Interpretación de los coeficientes 7.4 Ejemplo: pozos en Bangladesh 7.5 Ajuste de coeficientes para regresión logística (binomial). 7.6 Descenso en gradiente 7.7 Ejemplo: diabetes 7.8 Observaciones adicionales 7.9 Regresión logística para problemas de más de 2 clases 7.10 Regresión logística multinomial 7.11 Identificabilidad y separación 7.12 Tarea", " Clase 7 Regresión logística 2 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } 7.1 Incertidumbre en la estimación Podemos ajustar varios modelos para mostrar que hay incertidumbre en el ajuste del modelo. En el rango de los datos, la línea sólida muestra el mejor ajuste para una regresión logística, y las líneas de color gris clarito muestran la incertidumbre en el ajuste. invlogit &lt;- function(x){ exp(x)/(1+exp(x)) } M &lt;- 50 N &lt;- nrow(datos_2) fit.1 &lt;- glm(vote ~ income, data = datos_2, family=binomial(link=&quot;logit&quot;)) modelos &lt;- 1:M %&gt;% map(~glm(vote ~ income, data = sample_n(tbl = datos_2, size = N, replace = T), family=binomial(link=&quot;logit&quot;))) %&gt;% map(summary) %&gt;% map(&quot;coefficients&quot;) %&gt;% map_df(function(x){data_frame(intercept=x[1,1], income=x[2,1])}) x &lt;- seq(0.5,5.5, length.out = 100) graf_data &lt;- lapply(1:M, function(i){ invlogit(modelos$intercept[i] + modelos$income[i]*x) }) graf_data &lt;- as.data.frame(Reduce(f = cbind, x = graf_data)) colnames(graf_data) &lt;- paste0(&#39;V&#39;,1:M) graf_data$x &lt;- x graf_data$y &lt;- invlogit(fit.1$coef[1] + fit.1$coef[2]*x) g &lt;- ggplot(graf_data, aes(x=x)) for(i in 1:M){ g &lt;- g + geom_line(data = graf_data, aes_string(y = paste0(&#39;V&#39;,i)), colour = &#39;grey&#39;, size=0.2) } g &lt;- g + geom_line(data = graf_data, aes(y = y), colour = &#39;black&#39;, size=1) g + xlab(&quot;x&quot;) + ylab(&quot;p(y=1|x)&quot;) A este método en estadística se le conoce como bootstrap porque consiste en tomar muestras con reemplazo del mismo tamaño de los datos. Es muy útil para estimar errores estándar. 7.2 Función logística Recordemos: \\(\\mbox{logit}^{-1}\\) es la función de transformación de los predictores lineales a las probabilidades que se utilizan en la regresión logística. \\[ \\mbox{logit}^{-1}(x) = \\dfrac{e^x}{1+e^x} \\] ggplot(data.frame(x=seq(-5,5,by=0.05)), aes(x=x)) + stat_function(fun = invlogit, xlim = c(-5,5), size=1) Regresemos al ejemplo del ajuste de regresión logística y los coeficientes: fit.1 &lt;- glm(vote ~ income, data = datos_2, family=binomial(link=&quot;logit&quot;)) fit.1$coefficients #&gt; (Intercept) income #&gt; -1.402 0.326 Podemos ver la probabilidades que predice el modelo graficando la función con sus respectivos coeficientes: ggplot(datos_2, aes(x = income, y = vote)) + geom_jitter(width = 0.3, height = 0.08, size = 0.1) + stat_function(fun=function(x){invlogit(fit.1$coef[1]+fit.1$coef[2]*x)}, xlim=c(0.1,5.5), size=0.5) + geom_segment(aes(x=-0.01, y=0.5, xend=4.31, yend=0.5), linetype = 2, color = &#39;lightpink&#39;) + geom_segment(aes(x=4.31, y=-0.01, xend=4.31, yend=0.5), linetype=2, color = &#39;lightpink&#39;) + geom_point(aes(x=4.31, y=0.5), color = &#39;red&#39;) + scale_x_continuous(breaks=c(1,2,3,4,4.31,5), labels=c(&quot;(menor ingreso)&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;4.31&quot;, &quot;(mayor ingreso)&quot;)) + scale_y_continuous(breaks=c(0,0.5,1), labels=c(&quot;Clinton (0)&quot;, &quot;0.5&quot;, &quot;Bush (1)&quot;)) + xlab(&quot;Categoría de ingreso&quot;) + ylab(&quot;Voto&quot;) \\[ \\mbox{logistic regression model: }\\; y = \\mbox{logit}^{−1} (−1.40 + 0.33x) \\] La probabilidad de predicción es \\(0.5\\) cuando \\(−1.40 + 0.33x = 0\\), que es \\(x = 1.40/0.33 = 4.31\\). La pendiente de la curva de regresión logística es mayor en este punto intermedio. La función \\(\\mbox{logit}^{-1}(x)=\\dfrac{e^x}{1+e^x}\\) transforma valores continuos en \\((0,1)\\), lo cual es necesario, ya que las probabilidades deben estar entre \\(0\\) y \\(1\\). El modelo \\[ P(y_i=1)=\\mbox{logit}^{-1}(X_i\\beta), \\] se puede expresar como \\[ \\begin{eqnarray*} P(y_i=1) &amp;=&amp; p_i\\\\ \\mbox{logit}(p_i) &amp;=&amp; X_i\\beta. \\end{eqnarray*} \\] Vamos a invertir la función \\(\\mbox{logit}^{-1}\\): \\[ \\begin{eqnarray*} f(x) = \\dfrac{e^x}{1+e^x} &amp;=&amp; y\\\\ e^x &amp;=&amp; y (1+e^x)\\\\ e^x (1-y)&amp;=&amp;y\\\\ e^x &amp;=&amp; \\dfrac{y}{1-y}\\\\ x &amp;=&amp; \\mbox{log}\\left(\\dfrac{y}{1-y}\\right) \\end{eqnarray*} \\] Preferimos trabajar con \\(\\mbox{logit}^{-1}\\) porque es más natural pensar en la transformación del predictor lineal a las probabilidades, que al revés. Como la función logística inversa no es lineal, entonces la diferencia esperada en \\(y\\) correspondiente a una diferencia fija en \\(x\\) no es constante: \\(\\mbox{logit}(0.5) = 0\\), y \\(\\mbox{logit}(0.6) = 0.4\\). Agregar \\(0.4\\) en la escala de logit corresponde a un cambio de 50% a 60% en la escala de probabilidad. \\(\\mbox{logit}(0.9) =2.2\\), y \\(\\mbox{logit}(0.93) = 2.6\\). Agregar \\(0.4\\) en la escala logit corresponde a un cambio de sólo 90% a 93%. \\(\\mbox{logit}(0.953) = 3\\). Agregar \\(0.4\\) más corresponde a un incremento en la probabilida de 93% a 95.3%. En general, los cambios de probabilidad se comprimen en los extremos de la escala de logit, y esto es necesario para mantener las probabilidades entre 0 y 1. 7.3 Interpretación de los coeficientes Debido a esta no linealidad, los coeficientes de regresión logística pueden ser difíciles de interpretar. Vamos a utilizar resúmenes numéricos para hacer las interpretaciones. 7.3.1 Evaluar en (o alrededor de) la media La curva de la función logística requiere que elijamos dónde evaluar los cambios, si queremos interpretar en la escala de probabilidad. Podemos comenzar evaluando en la media de los datos de entrada. Como en regresión lineal, el intercepto se puede interpretar suponiendo valores de \\(0\\) para los otros predictores. Cuando la interpretación de \\(0\\) en los demás valores no es interesante, o bien, \\(0\\) no está en el rango de las variables (como en el ejemplo de votaciones, donde el ingreso está en una escala del 1-5), se puede evaluar el intercepto en otro punto. Por ejemplo, podemos evaluar la probabilidad de voto por Bush en la categoría central de del ingreso y obtener \\[\\mbox{logit}^{−1}(−1.40 + 0.33 \\cdot 3) = 0.40.\\] O podemos evaluar la probabilidad del voto por Bush \\(P(y_i=1)\\) en la media del ingreso de los encuestados, \\[\\mbox{logit}^{-1}(-1.4+0.33\\cdot \\bar{x}).\\] invlogit(coef(fit.1)[1] + coef(fit.1)[2]*mean(datos_2$income)) #&gt; (Intercept) #&gt; 0.401 En este ejemplo, \\(\\bar{x} = 3.1\\), que da como resultado \\(P(\\mbox{vota Bush}) = 0.40\\) en la media de \\(x\\). Una diferencia de \\(1\\) (1 más en la escala de ingreso de \\(1\\) a \\(5\\)) corresponde a una diferencia positiva de \\(0.33\\) en la probabilidad (logit) de voto por Bush. Hay dos maneras convenientes de resumir esto directamente en términos de probabilidades: Podemos evaluar cómo cambia la probabilidad (logit) ante un cambio unitario con respecto a la media de \\(x\\). Como \\(\\bar{x}=3.1\\), entonces podemos evaluar la función de regresión logísta en \\(x=3\\) y \\(x=2\\). La difrerencia en \\(P(y=1)\\) que corresponde a agregar \\(1\\) a \\(x\\) es: \\[ \\mbox{logit}^{−1}(−1.40+0.33·3)−\\mbox{logit}^{−1}(−1.40+0.33·2) = 0.08.\\] Una diferencia de 1 en la categoría de ingresos corresponde a una diferencia positiva del 8% en la probabilidad de apoyar a Bush. En vez de considerar un cambio discreto en \\(x\\) podemos calcular la derivada de la curva logística en algún valor central, en este caso la media \\(\\bar{x}=3.1\\). Diferenciando la función \\[\\mbox{logit}^{−1}(\\alpha + \\beta x)\\] con respecto a \\(x\\) resulta en \\[\\beta e^{\\alpha+\\beta x}/(1 + e^{\\alpha +\\beta x})^2\\]. El valor del predictor lineal en el valor central de \\(\\bar{x}=3.1\\) es \\[−1.40+0.33·3.1 = −0.39,\\] y la pendiente de la curva, el “cambio” en \\(P(y = 1)\\) por unidad pequeña de “cambio” en x, en este punto es \\[0.33\\cdot e^{-0.39}/(1 + e^{-0.39})^2 = 0.13.\\] Para este ejemplo, la diferencia en la escala de probabilidad es el mismo valor de 0.13 (con un lugar decimal); esto es típico, pero en algunos casos donde una diferencia de unidad es grande, la diferenciación y la derivada pueden dar respuestas ligeramente diferentes. Sin embargo, siempre serán el mismo signo. Podemos comparar la diferencia en la escala de probabilidad (0.08) con la derivada (0.13). Estas generalmente son similares, pero pueden no serlo cuando la diferencia de una unidad es grande. 7.3.2 La regla de “dividir entre 4” La curva logística tiene mayor inclinación en el centro, en el punto en el cual \\[\\alpha + \\beta x = 0,\\] de tal forma que \\[\\mbox{logit}^{-1}(\\alpha + \\beta x) = 0.5.\\] La pendiente de la curva, o sea la derivada de la función logística, es máxima en este punto y su valor máximo es \\(\\beta/4\\). Como una regla general, se puede tomar cualquier coeficiente de la regresión logística (que no sea el constante o intercepto) y dividirlos entre 4 para obtener una cota superior de la diferencia en probabilidad cuando se varía \\(x\\) en una unidad. Este límite superior es una aproximación razonable alrededor del punto medio de la curva logística, es decir, donde las probabilidades son cercanas a 0.5. En el ejemplo anterior, el modelo \\[ P(\\mbox{vota por Bush}) = \\mbox{logit}^{-1}(-1.4 + 0.33\\;\\cdot \\;\\mbox{ingreso}), \\] y podemos dividir \\(\\beta/4\\): \\[ \\dfrac{\\beta}{4}=\\dfrac{0.33}{4}\\approx 0.0825. \\] Este número ya lo habíamos obtenido antes analizando diferencias. Una diferencia de \\(1\\) en la categoría de ingreso corresponde a no más de un 8% de diferencia en la probabilidad de voto por Bush. Como los datos en este caso están cerca del punto del 50%, esta aproximación de 0.08 es cercana a 0.13, el valor de la derivada evaluada en la media (el punto medio en los datos), que puede no ser el punto medio en la curva. 7.3.3 Interpretación de los coeficientes como cocientes de momios Otra forma de interpretar los coeficientes de la regresión logística es en términos de cocientes de momios. Recordemos: Si el resultado de un experimento tiene probabilidad \\(p\\), entonces \\(p/(1-p)\\) se llaman los momios. Un momio de 1 es equivalente a una probabilidad de \\(1/2\\), es decir, ambos resultados (éxito y fracaso) son equiprobables. Momios de 0.5 y 2 representan probabilidades de 1/3 y 2/3, respectivamente. La razón de momios es un cociente de momios: \\[\\dfrac{p_1/(1-p_1)}{p_2/(1-p_2)}.\\] Una razón de momios de 2 corresponde a un cambio de \\(p= 0.33\\) a \\(p = 0.5\\) o un cambio de \\(p = 0.5\\) a \\(p = 0.67\\). Una ventaja de trabajar con razones de momios (en lugar de probabilidades) es que es posible escalar cocientes de momios indefinidamente sin los límites de (0,1) de las probabilidades. Por ejemplo, el cociente de momios de 2 a 4 incrementa la probabilidad de \\(2/3\\) a \\(4/5\\), si se duplican de nuevo los momios a 8, la probabilidad ahora es \\(8/9\\), y así sucesivamente. Los coeficientes de regresión logística (exponenciados) se pueden interpretar como cocientes de momios. Por simplicidad, vamos a verlo con un modelo de un predictor, pero esta técnica (igual que las anteriores son útiles para cualquier predictor cuando se tienen varias variables). El modelo es \\[ \\begin{eqnarray*} P(y_i=1|x) &amp;=&amp; \\mbox{logit}^{-1}(\\alpha+\\beta x)\\\\ &amp;=&amp; \\dfrac{e^{\\alpha+\\beta x}}{1+e^{\\alpha+\\beta x}}. \\end{eqnarray*} \\] Además tenemos que \\[ P(y_i=0|x) = \\dfrac{1}{e^{\\alpha+\\beta x}}. \\] Por lo tanto, \\[ \\begin{eqnarray*} \\dfrac{P(y_i=1|x)}{P(y_i=0|x)} &amp;=&amp; e^{\\alpha + \\beta x},\\\\ \\mbox{log}\\left[\\dfrac{P(y_i=1|x)}{P(y_i=0|x)} \\right] &amp;=&amp; \\alpha + \\beta x. \\end{eqnarray*} \\] Sumar 1 a la variable \\(x\\) es equivalente a sumar \\(\\beta\\) en ambos lados de la ecuación. Exponenciando nuevamente ambos lados, el cociente de momios se multiplica por \\(e^\\beta\\). Por ejemplo, si \\(\\beta=0.2\\), entonces una diferencia unitaria en \\(x\\) corresponde a un cambio multiplicativo de \\(e^{0.2}=1.22\\) en los momios de éxito (con respecto a los chances de un fracaso). Nota: El concepto de los momios puede ser un poco difícil de entender y comunicar, y razones de momios aún más. Sin embargo, los momios son útiles para conferirle al modelo una interpretación, para explicar cómo ciertos valores pueden aumentar los chances de éxito utilizando la exponencial de algun coeficiente \\(e^{\\beta_i}\\). 7.4 Ejemplo: pozos en Bangladesh Vamos a ver cómo utilizar un modelo logístico para poder tomar la decisión a nivel hogar en Bangladesh de si cambiar o no su fuente de agua potable. 7.4.1 Descripción del problema Muchos de los pozos utilizados para el agua potable en Bangladesh y otros países del sur de Asia están contaminados con arsénico natural, afectando a aproximadamente 100 millones de personas. El arsénico es un veneno acumulativo y la exposición aumenta el riesgo de cáncer y otras enfermedades, y se estima que los riesgos son proporcionales a la exposición. wells_all &lt;- read_csv(&quot;datos/wells_all.csv&quot;) ggplot(wells_all, aes(x = lon, y = lat)) + geom_point(size = 0.05) En esta gráfica podemos ver los pozos en un área de Araihazar upazila, Bangladesh. Los puntos representan pozos con arsénico mayor o menor que el estándar de seguridad de 0.5 (en unidades de cientos de microgramos por litro). Los pozos están ubicados donde viven las personas. Las áreas vacías entre los pozos son principalmente tierras de cultivo. Tanto pozos seguros como inseguros están mezclados en la mayor parte del área, lo que sugiere que los usuarios de pozos inseguros pueden recurrir a algún pozo seguro cercano. library(ggmap) left &lt;- min(wells_all$lon) bottom &lt;- min(wells_all$lat) right &lt;- max(wells_all$lon) top &lt;- max(wells_all$lat) araihazar &lt;- get_map(location = c(left,bottom,right,top), zoom = 13) ggmap(araihazar) + geom_point(data = wells_all, aes(x = lon, y = lat, color = switch), size = 0.03, alpha = 0.2) + scale_color_manual(values = c(&quot;navyblue&quot;, &quot;red&quot;)) En este artículo reciente se discuten posibles soluciones que hagan uso de tecnologías desarrolladas recientemente: Win, T. L. (2017, August 28). Can technology help Bangladesh end mass arsenic poisoning? Pueden leer también este boletín de la Organización Mundial de la Salud WHO. 7.4.2 Antecedentes del problema Muchos de los pozos utilizados para el agua potable en Bangladesh y otros países del sur de Asia están contaminados con arsénico natural, afectando a aproximadamente 100 millones de personas. El arsénico es un veneno acumulativo y la exposición aumenta el riesgo de cáncer y otras enfermedades, y se estima que los riesgos son proporcionales a la exposición. Causa del problema La crisis de arsénico de Bangladesh se remonta a la década de 1970 cuando, en un esfuerzo por mejorar la calidad del agua potable y la lucha contra la diarrea, que era uno de los mayores asesinos de niños en el país, hubo inversiones internacionales a gran escala en la construcción de pozos tubulares. Se creía que los pozos proporcionarían suministros seguros para las familias, de lo contrario dependían del agua superficial sucia que mataba hasta 250,000 niños al año. El agua superficial puede contener microbios, es por esto que era preferible el consumo de agua de pozos profundos. Cualquier localidad puede incluir pozos con arsénico, como se puede ver en la gráfica de arriba. La mala noticia es que incluso si el pozo de tu vecino es seguro, eso no significa que el tuyo esté a salvo. Sin embargo, la buena noticia es que si tu pozo tiene un nivel alto de arsénico, entonces probablemente puedas encontrar un pozo seguro cerca (si es que estás dispuesto a caminar y tu vecino está dispuesto a compartir). La cantidad de agua necesaria para beber es lo suficientemente baja como para suponer que más personas pueden ocupar el pozo sin agotar su capacidad. 7.4.3 Metodología para abordar el problema Un equipo de investigación de los Estados Unidos y Bangladesh midió todos los pozos y los etiquetó con su nivel de arsénico, así como una caracterización: “seguro” (por debajo de 0.5 en unidades de cientos de microgramos por litro, un estándar para el arsénico en el agua potable), o “inseguro” (por encima de 0.5). Las personas con pozos inseguros fueron alentados a cambiar a pozos privados o comunitarios cercanos o a nuevos pozos de su propia construcción. Unos años más tarde, los investigadores volvieron para averiguar qué vecinos habían cambiado de pozo. Hagamos un análisis de regresión logística para comprender los factores predictivos del cambio de pozo entre los usuarios de pozos no seguros. Nuestra variable de respuesta es \\[ y_{i} = \\left\\{ \\begin{array}{cl} 1 &amp; \\text{si la }\\; i\\text{-esima casa cambió}\\;\\; \\text{de pozo},\\\\ 0 &amp; \\text{en otro caso.} \\end{array}\\right. \\] Consideramos las siguientes entradas: Un término constante La distancia (en metros) al pozo seguro conocido más cercano El nivel de arsénico del pozo del encuestado Si algún miembro del hogar está activo en organizaciones comunitarias. El nivel de educación del jefe del hogar. Primero ajustaremos el modelo usando la distancia al pozo más cercano y luego colocaremos la concentración de arsénico, la membresía organizacional y la educación. 7.4.4 Ajuste y resultados del modelo Ajustamos la regresión logística con sólo un predictor: wells &lt;- read_csv(&quot;datos/wells.csv&quot;) fit.1 &lt;- glm(switch ~ dist, data = wells, family=binomial(link=&quot;logit&quot;)) fit.1 #&gt; #&gt; Call: glm(formula = switch ~ dist, family = binomial(link = &quot;logit&quot;), #&gt; data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist #&gt; 0.60596 -0.00622 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3018 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 4080 AIC: 4080 El coeficiente para dist es -0.0062, que parece bajo, pero esto es engañoso ya que la distancia se mide en metros, por lo que este coeficiente corresponde a la diferencia entre, por ejemplo, una casa que está a 90 metros del pozo seguro más cercano y una casa que está a 91 metros de distancia. Veamos la distribución de la distancia en los datos: ggplot(wells, aes(x=dist)) + geom_histogram(binwidth = 5) Parece razonable escalar la distancia en unidades de 100 metros: wells &lt;- wells %&gt;% mutate(dist_100 = dist/100) Volvemos a ajustar el modelo: fit.2 &lt;- glm(switch ~ dist_100, data = wells, family=binomial(link=&quot;logit&quot;)) fit.2 #&gt; #&gt; Call: glm(formula = switch ~ dist_100, family = binomial(link = &quot;logit&quot;), #&gt; data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist_100 #&gt; 0.606 -0.622 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3018 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 4080 AIC: 4080 Podemos ver gráficamente la regresión logística ajustada, \\[ P(\\mbox{cambie pozo}) = \\mbox{logit}^{-1}(0.61 - 0.62 \\cdot \\mbox{dist_100}), \\] con los datos (jitter) superpuestos. ggplot(wells, aes(x = dist_100, y = switch)) + geom_jitter(width = 0.308, height = 0.1, size = 0.1) + stat_function(fun = function(x){invlogit(fit.2$coef[1] + fit.2$coef[2]*x)}, xlim = c(-0.3,3.5)) + scale_y_continuous(breaks = c(0,1), labels=c(&quot;No switch&quot;,&quot;Switch&quot;)) + ylab(&quot;P(switch)&quot;) La probabilidad de cambio es aproximadamente del 60% para las personas que viven cerca de un pozo seguro, disminuyendo a un 20% para las personas que viven a más de 300 metros de cualquier pozo seguro. Esto tiene sentido: la probabilidad de cambio es mayor para las personas que viven más cerca de un pozo seguro. 7.4.5 Interpretación de los coeficientes El término constante se puede interpretar cuando \\(\\mbox{dist_100 = 0}\\), en cuyo caso la probabilidad de cambio es \\(\\mbox{logit}^{-1}(0.61) = 0.65\\). Por lo tanto, el modelo estima un 65% de probabilidades de cambio si vives junto a un pozo seguro existente. Podemos evaluar la “diferencia predictiva” con respecto a \\(\\mbox{dist_100}\\) calculando la derivada en la media de \\(\\mbox{dist_100}\\), que es 0.48 (es decir, 48 metros). El valor del predictor lineal aquí es \\[0.61 - 0.62 \\cdot 0.48 = 0.31,\\] y entonces la pendiente de la curva en este punto es \\[\\dfrac{-0.62 \\cdot e^{0.31}}{(1 + e^{0.31})^2} = -0.15.\\] Por lo tanto, agregar 1 a \\(\\mbox{dist_100}\\), es decir, sumar 100 metros a la distancia al pozo seguro más cercano, representa una diferencia en la probabilidad de 15% menos. La “regla de dividir entre 4” nos da \\[\\dfrac{\\beta}{4}=\\dfrac{-0.62}{4} = -0.15.\\] El resultado es el mismo al que se calculó usando la derivada porque la curva pasa aproximadamente por el punto del 50% de probabilidad (en realidad es 57%). Además de interpretar su magnitud, podemos estudiar la distribución del error estándar del coeficiente de distancia. Utilizamos el procedimiento visto al principio: set.seed(110265) M &lt;- 500 N &lt;- nrow(wells) modelos_wells &lt;- 1:M %&gt;% map(~glm(switch ~ dist_100, data = sample_n(tbl = wells, size = N, replace = T), family=binomial(link=&quot;logit&quot;))) %&gt;% map(summary) %&gt;% map(&quot;coefficients&quot;) %&gt;% map_df(function(x){data_frame(intercept=x[1,1], dist_100=x[2,1])}) Vemos el resumen para la distancia: summary(modelos_wells$dist_100) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.928 -0.687 -0.621 -0.624 -0.556 -0.333 Con cuantiles: quantile(modelos_wells$dist_100, 0.025) #&gt; 2.5% #&gt; -0.839 quantile(modelos_wells$dist_100, 0.975) #&gt; 97.5% #&gt; -0.448 La pendiente se estima bien, con un error estándar de sólo \\(0.10\\), que es muy pequeño en comparación con la estimación del coeficiente de \\(-0.62\\). El intervalo aproximado de 95% es \\([-0.83, -0.44]\\), que es claramente estadísticamente diferente de cero. 7.4.6 Agregamos una segunda variable de entrada Ahora ampliamos el ejemplo de cambio de pozo al agregar el nivel de arsénico del pozo existente como una entrada de regresión. Con los niveles presentes de arsénico en el agua potable de Bangladesh, los riesgos para la salud son proporcionales a su exposición, por lo que podemos esperar que el cambio de pozo sea más probable cuando el nivel de arsénico es alto. Veamos un histograma de niveles de arnésico: ggplot(wells, aes(x=arsenic)) + geom_histogram(binwidth = 0.1) Hacemos la regresión logística añadiendo esta variable: fit.3 &lt;- glm(switch ~ dist_100 + arsenic, data = wells, family=binomial(link=&quot;logit&quot;)) fit.3 #&gt; #&gt; Call: glm(formula = switch ~ dist_100 + arsenic, family = binomial(link = &quot;logit&quot;), #&gt; data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist_100 arsenic #&gt; 0.00275 -0.89664 0.46077 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3017 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 3930 AIC: 3940 Observaciones: Por lo tanto, comparando dos pozos con el mismo nivel de arsénico cada 100 metros de distancia al pozo más cercano que sea seguro, corresponde a una diferencia negativa de aproximadamente \\(90\\%\\) en la probabilidad logit de cambio. Utilizando la interpretación de momios, comparando dos pozos con el mismo nivel de arsénico si incrementamos la distancia del pozo seguro más cercano en 100 metros, entonces los chances de cambio de pozo son de \\(e^{-0.9}=0.407\\) a favor de no cambiar de pozo. De manera similar una diferencia de 1 en niveles de arsénico corresponden a una diferencia positiva de 0.46 en la probabilidad logit de cambiar de pozo. Ambos coeficientes son estadísticamente significativos, cada uno con errores estándar que justifican ser significativamente distintos de cero. Una interpretación rápida la podemos obtener dividiendo los coeficientes entre 4: 100 metros más de distancia corresponden aproximadamente a \\(0.89/4=22\\%\\) menos probabilidad de cambio, y 1 unidad más de concentración de arsénico corresponden a aproximadamente \\(0.46/4 = 11\\%\\) de una diferencia positiva de probabilidad de cambio. Si comparamos estos dos coeficientes podría parecer que la distancia es un factor más importante que el nivel de arsénico para determinar la probabilidad de cambio. Este argumento es engañoso porque en los datos la distancia tienen menor variación que los niveles de arsénico: la desviación estándar de la distancia es 0.38 (en unidades de 100 metros), mientras que la desviación estándar de los niveles de arsénico es de 1.10. Comparando con los coeficientes de los predictores: \\(-0.90\\cdot 0.38 = -0.342\\) para distancia y \\(0.46*1.10=0.51\\) para niveles de arsénico. Usando la regla de la división por 4, las diferencias en la probabilidad de cambio entre arsénico y distancia son de 13% y 8% respectivamente. 7.4.7 Comparación de coeficientes cuando añades un predictor El coeficiente de distancia cambio de \\(-0.62\\) en el modelo original a \\(0.90\\) cuando el nivel de arsénico se añade al modelo. Este cambio ocurre porque los pozos que están lejos del pozo seguro más cercano probablemente tienen niveles altos de arsénico. 7.4.8 Graficar el modelo ajustado con dos predictores La forma de ver esta relación entre los predictores y \\(P(y=1)\\) como una función en una superficie de 3 dimensiones donde los dos predictores se ponen en los ejes horizontales. Estas gráficas suelen ser difíciles de leer por lo cual se hacen generalmente gráficas separadas: ggplot(wells, aes(x = dist_100, y = switch)) + geom_jitter(width = 0.308, height = 0.1, size = 0.1) + stat_function(fun = function(x){invlogit(fit.3$coef[1] + fit.3$coef[2]*x + 0.5)}, xlim = c(-0.3,3.5)) + stat_function(fun = function(x){invlogit(fit.3$coef[1] + fit.3$coef[2]*x + 1)}, xlim = c(-0.3,3.5)) + annotate(&quot;text&quot;, x = 0.50, y = 0.45, label = &quot;As=0.5&quot;, size = 4) + annotate(&quot;text&quot;, x = 0.75, y = 0.65, label = &quot;As=1.0&quot;, size = 4) Escogemos niveles de arsénico de 0.5 y 1.0 porque 0.5 es el valor mínimo de la concentración de arsénico (porque estamos estudiando únicamente pozos peligrosos), y una diferencia de 0.5 representa una comparación razonable, dada la distribución de los niveles de arsénico. ggplot(wells, aes(x = arsenic, y = switch)) + geom_jitter(width = 0.308, height = 0.1, size = 0.1) + stat_function(fun = function(x){invlogit(fit.3$coef[1] + 0 + fit.3$coef[3]*x)}, xlim = c(-0.3,10)) + stat_function(fun = function(x){invlogit(fit.3$coef[1] + 0.5 + fit.3$coef[3]*x)}, xlim = c(-0.3,10)) + annotate(&quot;text&quot;, x = 0.7, y = 0.80, label = &quot;dist=0&quot;, size = 4) + annotate(&quot;text&quot;, x = 2.0, y = 0.65, label = &quot;dist=50&quot;, size = 4) La distancia de 0 representa la distancia mínima de un pozo seguro a un pozo posiblemente seguro. Dada la distribución de la distancia, una diferencia de 50 metros representa en si misma un diferencia sustancial en la probabilidad de cambiar de pozo. 7.5 Ajuste de coeficientes para regresión logística (binomial). Ahora escribimos el modelo cuando tenemos más de una entrada. La idea es la misma: primero combinamos las variables linealmente usando pesos \\(\\beta\\), y despúes comprimimos a \\([0,1]\\) usando la función logística: El modelo de regresión logística está dado por \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1 + \\beta_2x_2 +\\cdots + \\beta_p x_p),\\] y \\[p_0(x)=p_0(x;\\beta)=1-p_1(x;\\beta),\\] donde \\(\\beta=(\\beta_0,\\beta_1, \\ldots, \\beta_p)\\). Escribimos \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1 + \\beta_2x_2 +\\cdots + \\beta_p x_p),\\] y definimos la devianza como \\[D(\\beta) = -2\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})).\\] En regresión logística y otros modelos de datos discretos, no tiene sentido calcular la desviación estándar residual y \\(R^2\\), por más o menos la misma razón por la que los modelos no se ajustan simplemente por mínimos cuadrados: el error cuadrado no es la medida matemáticamente óptima del error del modelo. En cambio, es estándar usar la devianza, un resumen estadístico del ajuste del modelo, definido para la regresión logística y otros modelos lineales generalizados para ser una analogía con la desviación estándar residual. Por ahora, consideremos las siguientes propiedades de la desvianza: La devianza es una medida del error, menor devianza significa mejor ajuste de los datos. Si un predictor que es únicamente ruido aleatorio se agrega al modelo, esperaríamos que la devianza disminuyera en 1, en promedio. Si se añade al modelo un predictor informativo, se espera que la devianza disminuya en más de 1 en promedio. Cuando \\(k\\) predictores se añaden al modelo, esperamos que la devianza disminuya en más de \\(k\\). Los coeficientes estimados por regresión logística están dados por \\[\\hat{\\beta} = \\arg\\min_\\beta D(\\beta).\\] El nombre de devianza se utiliza de manera diferente en distintos lugares (pero para cosas similares). Usamos el factor 2 por razones históricas (la medida de devianza definida en estadística tiene un 2, para usar más fácilmente en pruebas de hipótesis relacionadas con comparaciones de modelos). No es fácil interpretar la devianza, pero es útil para comparar modelos. 7.6 Descenso en gradiente Aunque el problema de mínimos cuadrados se puede resolver analíticamente, proponemos un método numérico básico que es efectivo y puede escalarse a problemas grandes de manera relativamente simple: descenso en gradiente, o descenso máximo. Supongamos que una función \\(h(x)\\) es convexa y tiene un mínimo. La idea de descenso en gradiente es comenzar con un candidato inicial \\(z_0\\) y calcular la derivada en \\(z^{(0)}\\). Si \\(h&#39;(z^{(0)})&gt;0\\), la función es creciente en \\(z^{(0)}\\) y nos movemos ligeramente a la izquierda para obtener un nuevo candidato \\(z^{(1)}\\). si \\(h&#39;(z^{(0)})&lt;0\\), la función es decreciente en \\(z^{(0)}\\) y nos movemos ligeramente a la derecha para obtener un nuevo candidato \\(z^{(1)}\\). Iteramos este proceso hasta que la derivada es cercana a cero (estamos cerca del óptimo). Si \\(\\eta&gt;0\\) es una cantidad chica, podemos escribir \\[z^{(1)} = z^{(0)} - \\eta \\,h&#39;(z^{(0)}).\\] Nótese que cuando la derivada tiene magnitud alta, el movimiento de \\(z^{(0)}\\) a \\(z^{(1)}\\) es más grande, y siempre nos movemos una fracción de la derivada. En general hacemos \\[z^{(j+1)} = z^{(j)} - \\eta\\,h&#39;(z^{(j)})\\] para obtener una sucesión \\(z^{(0)},z^{(1)},\\ldots\\). Esperamos a que \\(z^{(j)}\\) converja para terminar la iteración. 7.6.0.1 Ejemplo Si tenemos h &lt;- function(x) x^2 + (x - 2)^2 - log(x^2 + 1) Calculamos (a mano): h_deriv &lt;- function(x) 2 * x + 2 * (x - 2) - 2*x/(x^2 + 1) Ahora iteramos con \\(\\eta = 0.4\\) y valor inicial \\(z_0=5\\) z_0 &lt;- 5 eta &lt;- 0.4 descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } z &lt;- descenso(20, 5, 0.1, h_deriv) z #&gt; [,1] #&gt; [1,] 5.00 #&gt; [2,] 3.44 #&gt; [3,] 2.52 #&gt; [4,] 1.98 #&gt; [5,] 1.67 #&gt; [6,] 1.49 #&gt; [7,] 1.39 #&gt; [8,] 1.33 #&gt; [9,] 1.29 #&gt; [10,] 1.27 #&gt; [11,] 1.26 #&gt; [12,] 1.25 #&gt; [13,] 1.25 #&gt; [14,] 1.25 #&gt; [15,] 1.25 #&gt; [16,] 1.25 #&gt; [17,] 1.24 #&gt; [18,] 1.24 #&gt; [19,] 1.24 #&gt; [20,] 1.24 Y vemos que estamos cerca de la convergencia. curve(h, -3, 6) points(z[,1], h(z)) text(z[1:6], h(z[1:6]), pos = 3) 7.6.1 Selección de tamaño de paso \\(\\eta\\) Si hacemos \\(\\eta\\) muy chico, el algoritmo puede tardar mucho en converger: z &lt;- descenso(20, 5, 0.01, h_deriv) curve(h, -3, 6) points(z, h(z)) text(z[1:6], h(z[1:6]), pos = 3) Si hacemos \\(\\eta\\) muy grande, el algoritmo puede divergir: z &lt;- descenso(20, 5, 1.5, h_deriv) z #&gt; [,1] #&gt; [1,] 5.00e+00 #&gt; [2,] -1.84e+01 #&gt; [3,] 9.80e+01 #&gt; [4,] -4.84e+02 #&gt; [5,] 2.42e+03 #&gt; [6,] -1.21e+04 #&gt; [7,] 6.06e+04 #&gt; [8,] -3.03e+05 #&gt; [9,] 1.51e+06 #&gt; [10,] -7.57e+06 #&gt; [11,] 3.79e+07 #&gt; [12,] -1.89e+08 #&gt; [13,] 9.47e+08 #&gt; [14,] -4.73e+09 #&gt; [15,] 2.37e+10 #&gt; [16,] -1.18e+11 #&gt; [17,] 5.92e+11 #&gt; [18,] -2.96e+12 #&gt; [19,] 1.48e+13 #&gt; [20,] -7.40e+13 Es necesario ajustar el tamaño de paso para cada problema particular. Si la convergencia es muy lenta, podemos incrementarlo. Si las iteraciones divergen, podemos disminuirlo. 7.6.2 Funciones de varias variables Si ahora \\(h(z)\\) es una función de \\(p\\) variables, podemos intentar la misma idea usando el gradiente. Por cálculo sabemos que el gradiente apunta en la dirección de máximo crecimiento local. El gradiente es el vector columna con las derivadas parciales de \\(h\\): \\[\\nabla h(z) = \\left( \\frac{\\partial h}{\\partial z_1}, \\frac{\\partial h}{\\partial z_2}, \\ldots, \\frac{\\partial h}{\\partial z_p} \\right)^t\\] Y el paso de iteración, dado un valor inicial \\(z_0\\) y un tamaño de paso \\(\\eta &gt;0\\) es \\[z^{(i+1)} = z^{(i)} - \\eta \\nabla h(z^{(i)})\\] Las mismas consideraciones acerca del tamaño de paso \\(\\eta\\) aplican en el problema multivariado. h &lt;- function(z) { z[1]^2 + z[2]^2 - z[1] * z[2] } h_gr &lt;- function(z_1,z_2) apply(cbind(z_1, z_2), 1, h) grid_graf &lt;- expand.grid(z_1 = seq(-3, 3, 0.1), z_2 = seq(-3, 3, 0.1)) grid_graf &lt;- grid_graf %&gt;% mutate( val = apply(cbind(z_1,z_2), 1, h)) gr_contour &lt;- ggplot(grid_graf, aes(x = z_1, y = z_2, z = val)) + geom_contour(binwidth = 1.5, aes(colour = ..level..)) gr_contour El gradiente está dado por h_grad &lt;- function(z){ c(2*z[1] - z[2], 2*z[2] - z[1]) } Podemos graficar la dirección de máximo descenso para diversos puntos. Estas direcciones son ortogonales a la curva de nivel que pasa por cada uno de los puntos: grad_1 &lt;- h_grad(c(0,-2)) grad_2 &lt;- h_grad(c(1,1)) eta &lt;- 0.2 gr_contour + geom_segment(aes(x=0.0, xend=0.0-eta*grad_1[1], y=-2, yend=-2-eta*grad_1[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ geom_segment(aes(x=1, xend=1-eta*grad_2[1], y=1, yend=1-eta*grad_2[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ coord_fixed(ratio = 1) Y aplicamos descenso en gradiente: inicial &lt;- c(3, 1) iteraciones &lt;- descenso(20, inicial , 0.1, h_grad) iteraciones #&gt; [,1] [,2] #&gt; [1,] 3.000 1.000 #&gt; [2,] 2.500 1.100 #&gt; [3,] 2.110 1.130 #&gt; [4,] 1.801 1.115 #&gt; [5,] 1.552 1.072 #&gt; [6,] 1.349 1.013 #&gt; [7,] 1.181 0.945 #&gt; [8,] 1.039 0.874 #&gt; [9,] 0.919 0.803 #&gt; [10,] 0.815 0.734 #&gt; [11,] 0.726 0.669 #&gt; [12,] 0.647 0.608 #&gt; [13,] 0.579 0.551 #&gt; [14,] 0.518 0.499 #&gt; [15,] 0.464 0.451 #&gt; [16,] 0.417 0.407 #&gt; [17,] 0.374 0.367 #&gt; [18,] 0.336 0.331 #&gt; [19,] 0.302 0.299 #&gt; [20,] 0.271 0.269 ggplot(data= grid_graf) + geom_contour(binwidth = 1.5, aes(x = z_1, y = z_2, z = val, colour = ..level..)) + geom_point(data = data.frame(iteraciones), aes(x=X1, y=X2), colour = &#39;red&#39;) Para minimizar utilizaremos descenso en gradiente (aunque hay más opciones). La última expresión para \\(D(\\beta)\\) puede ser difícil de operar, pero podemos reescribir como: \\[D(\\beta) = -2\\sum_{i=1}^N y^{(i)} \\log(p_{1} (x^{(i)})) + (1-y^{(i)}) \\log(p_{0} (x^{(i)})).\\] Para hacer descenso en gradiente, necesitamos encontrar \\(\\frac{\\partial D}{\\beta_j}\\) para \\(j=1,2,\\ldots,p\\). Comenzamos por calcular la derivada de un término: \\[D^{(i)} (\\beta) = y^{(i)} \\log(p_{1} (x^{(i)})) + (1-y^{(i)}) \\log(1-p_{1} (x^{(i)}))\\] Calculamos primero las derivadas de \\(p_1 (x^{(i)};\\beta)\\) (demostrar la siguiente ecuación): \\[\\frac{\\partial p_1}{\\partial \\beta_0} = {p_1(x^{(i)})(1-p_1(x^{(i)}))},\\] y \\[\\frac{\\partial p_1}{\\partial \\beta_j} = p_1(x^{(i)})(1-p_1(x^{(i)}))x_j^{(i)},\\] Así que \\[\\begin{align*} \\frac{\\partial D^{(i)}}{\\partial \\beta_j} &amp;= \\frac{y^{(i)}}{(p_1(x^{(i)}))}\\frac{\\partial p_1}{\\partial \\beta_j} - \\frac{1- y^{(i)}}{(1-p_1(x^{(i)}))}\\frac{\\partial p_1}{\\partial \\beta_j} \\\\ &amp;= \\left( \\frac{y^{(i)} - p_1(x^{(i)})}{(p_1(x^{(i)}))(1-p_1(x^{(i)}))} \\right )\\frac{\\partial p_1}{\\partial \\beta_j} \\\\ &amp; = \\left ( y^{(i)} - p_1(x^{(i)}) \\right ) x_j^{(i)} \\\\ \\end{align*}\\] para \\(j=0,1,\\ldots,p\\), usando la convención de \\(x_0^{(i)}=1\\). Podemos sumar ahora sobre los datos para obtener: \\[ \\frac{\\partial D}{\\partial\\beta_j} = - 2\\sum_{i=1}^N (y^{(i)}-p(x^{(i)}))x_j^{(i)}\\] De modo que, Para un paso \\(\\eta&gt;0\\) fijo, la iteración de descenso para regresión logística para el coeficiente \\(\\beta_j\\) es: \\[\\beta_{j}^{(k+1)} = \\beta_j^{(k)} + {2\\eta} \\sum_{i=1}^N (y^{(i)}-p(x^{(i)}))x_j^{(i)}\\] para \\(j=0,1,\\ldots, p\\), donde fijamos \\(x_0^{(i)}=1\\). Podríamos usar las siguientes implementaciones: devianza_calc &lt;- function(x, y){ dev_fun &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x)) %*% beta) -2*sum(y*log(p_beta) + (1-y)*log(1-p_beta)) } dev_fun } grad_calc &lt;- function(x, y){ salida_grad &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x)) %*% beta) e &lt;- y - p_beta grad_out &lt;- -2*as.numeric(t(cbind(1,x)) %*% e) names(grad_out) &lt;- c(&#39;Intercept&#39;, colnames(x)) grad_out } salida_grad } descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } Ejemplo: crédito máximo Probemos nuestros cálculos con el ejemplo de 1 entrada de tarjetas de crédito. p_1 &lt;- function(x){ ifelse(x &lt; 15, 0.95, 0.95 - 0.007 * (x - 15)) } curve(p_1, 0,100, xlab = &#39;Porcentaje de crédito máximo&#39;, ylab = &#39;p_1(x)&#39;, ylim = c(0,1)) x &lt;- pmin(rexp(500,1/30),100) probs &lt;- p_1(x) y &lt;- ifelse(rbinom(length(x), 1, probs)==1 ,1, 2) datos &lt;- data_frame(x = x, p_1 = probs, y = factor(y)) datos %&gt;% select(x, y) #&gt; # A tibble: 500 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 4.74 1 #&gt; 2 60.1 2 #&gt; 3 20.6 1 #&gt; 4 62.6 1 #&gt; 5 15.0 1 #&gt; 6 63.7 1 #&gt; # ... with 494 more rows datos$y &lt;- as.numeric(datos$y==1) datos &lt;- datos %&gt;% ungroup %&gt;% mutate(x_s = (x - mean(x))/sd(x)) devianza &lt;- devianza_calc(datos[, &#39;x_s&#39;, drop = FALSE], datos$y) grad &lt;- grad_calc(datos[, &#39;x_s&#39;, drop = FALSE], datos$y) h &lt;- function(x){exp(x)/(1+exp(x)) } grad(c(0,1)) #&gt; Intercept x_s #&gt; -376 359 grad(c(0.5,-0.1)) #&gt; Intercept x_s #&gt; -238 137 Verificamos cálculo de gradiente: (devianza(c(0.5+0.0001,-0.1)) - devianza(c(0.5,-0.1)))/0.0001 #&gt; [1] -238 (devianza(c(0.5,-0.1+0.0001)) - devianza(c(0.5,-0.1)))/0.0001 #&gt; [1] 137 Y hacemos descenso: iteraciones &lt;- descenso(200, z_0=c(0,0), eta = 0.001, h_deriv = grad) tail(iteraciones, 20) #&gt; [,1] [,2] #&gt; [181,] 2.25 -1.12 #&gt; [182,] 2.25 -1.12 #&gt; [183,] 2.25 -1.12 #&gt; [184,] 2.25 -1.12 #&gt; [185,] 2.25 -1.12 #&gt; [186,] 2.25 -1.12 #&gt; [187,] 2.25 -1.12 #&gt; [188,] 2.25 -1.12 #&gt; [189,] 2.25 -1.12 #&gt; [190,] 2.25 -1.12 #&gt; [191,] 2.25 -1.12 #&gt; [192,] 2.25 -1.12 #&gt; [193,] 2.25 -1.12 #&gt; [194,] 2.25 -1.12 #&gt; [195,] 2.25 -1.12 #&gt; [196,] 2.25 -1.12 #&gt; [197,] 2.25 -1.12 #&gt; [198,] 2.25 -1.12 #&gt; [199,] 2.25 -1.12 #&gt; [200,] 2.25 -1.12 plot(apply(iteraciones, 1, devianza)) matplot(iteraciones) Comparamos con glm: mod_1 &lt;- glm(y~x_s, data=datos, family = &#39;binomial&#39;) coef(mod_1) #&gt; (Intercept) x_s #&gt; 2.25 -1.12 mod_1$deviance #&gt; [1] 315 devianza(iteraciones[200,]) #&gt; [1] 315 Nótese que esta devianza está calculada sin dividir intre entre el número de casos. Podemos calcular la devianza promedio haciendo: devianza(iteraciones[200,])/nrow(datos) #&gt; [1] 0.629 7.7 Ejemplo: diabetes Consideremos los datos Pima.tr del paqute MASS: diabetes &lt;- MASS::Pima.tr diabetes %&gt;% sample_n(10) %&gt;% knitr::kable() npreg glu bp skin bmi ped age type 133 6 80 66 30 26.2 0.313 41 No 16 4 99 76 15 23.2 0.223 21 No 41 8 176 90 34 33.7 0.467 58 Yes 175 0 95 85 25 37.4 0.247 24 Yes 37 1 79 80 25 25.4 0.583 22 No 102 4 117 62 12 29.7 0.380 30 Yes 26 9 164 84 21 30.8 0.831 32 Yes 145 3 120 70 30 42.9 0.452 30 No 69 0 131 66 40 34.3 0.196 22 Yes 51 3 116 74 15 26.3 0.107 24 No Se trata de una población de 200 mujeres de al menos 21 años de edad, con un ancestro de los indios Pima, que viven alrededor de Phoenix, Arizona. A los individuos se les midió el nivel de glucosa y otras características para investigar si tenían diabetes de acuerdo con el US National Institute of Diabetes and Digestive and Kidney Diseases. Estos datos tiene las siguientes columnas: npreg: número de embarazos. glu: concentración de glucosa medida mediante un test oral de tolerancia bp: presión arterial diastólica (mmHg) skin: grosor de piel en triceps (mm) bmi: índice de masa corporal (peso en kg/(estatura en m)^2) ped: función diabetes pedigree age: edad en años type: Yes o No, diagnóstico de diabetes Normalizamos: diabetes$id &lt;- 1:nrow(diabetes) datos_norm &lt;- diabetes %&gt;% gather(variable, valor, npreg:age) %&gt;% group_by(variable) %&gt;% summarise(media = mean(valor), de = sd(valor)) normalizar &lt;- function(datos, datos_norm){ datos %&gt;% gather(variable, valor, npreg:age) %&gt;% left_join(datos_norm) %&gt;% mutate(valor_s = (valor - media)/de) %&gt;% select(id, type, variable, valor_s) %&gt;% spread(variable, valor_s) } diabetes_s &lt;- normalizar(diabetes, datos_norm) x &lt;- diabetes_s %&gt;% select(age:skin) %&gt;% as.matrix p &lt;- ncol(x) y &lt;- diabetes_s$type == &#39;Yes&#39; grad &lt;- grad_calc(x, y) iteraciones &lt;- descenso(1000, rep(0,p+1), 0.001, h_deriv = grad) diabetes_coef &lt;- data_frame(variable = c(&#39;Intercept&#39;,colnames(x)), coef = iteraciones[1000,]) diabetes_coef %&gt;% knitr::kable() variable coef Intercept -0.956 age 0.452 bmi 0.513 bp -0.055 glu 1.017 npreg 0.347 ped 0.559 skin -0.022 7.8 Observaciones adicionales Máxima verosimilitud Es fácil ver que este método de estimación de los coeficientes (minimizando la devianza) es el método de máxima verosimilitud. La verosimilitud está dada por: \\[L(\\beta) =\\prod_{i=1}^N p_{y^{(i)}} (x^{(i)})\\] Y la log verosimilitud es \\[l(\\beta) =\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})).\\] Así que ajustar el modelo minimizando la devianza es lo mismo que hacer máxima verosimilitud (condicional a los valores de \\(x\\)). Normalización Igual que en regresión lineal, en regresión logística conviene normalizar las entradas antes de ajustar el modelo. 7.9 Regresión logística para problemas de más de 2 clases Consideramos ahora un problema con más de dos clases, de manera que \\(Y ∈ {1,2,...,K}\\) (\\(K\\) clases), y tenemos \\(X = (X_1,\\ldots,X_p)\\) entradas. ¿Cómo generalizar el modelo de regresión logística a este problema? Una estrategia es la de uno contra todos: En clasificación uno contra todos, hacemos Para cada clase \\(y\\in\\{1,\\ldots,K\\}\\) ajustamos un modelo de regresión logística (binaria) \\(\\hat{p}^{(y)}(x)\\), tomando como positivos a los casos de 1 clase \\(g\\), y como negativos a todo el resto. Esto lo hacemos como en las secciones anteriores, y de manera independiente para cada clase. Para clasificar un nuevo caso \\(x\\), calculamos \\[\\hat{p}^{(1)}, \\hat{p}^{(2)},\\ldots, \\hat{p}^{(K)}\\] y clasificamos a la clase de máxima probabilidad \\[\\hat{Y}(x) = \\arg\\max_y \\hat{p}^{(y)}(x)\\] Nótese que no hay ninguna garantía de que las probabilidades de clase sumen 1, pues se trata de estimaciones independientes de cada clase. En este sentido, produce estimaciones que en realidad no satisfacen las propiedades del modelo de probabilidad establecido. Sin embargo, esta estrategia es simple y en muchos casos funciona bien. 7.10 Regresión logística multinomial Si queremos obtener estimaciones de las probabilidades de clase que sumen uno, entonces tenemos que contruir las estimaciones de cada clase de manera conjunta. Tenemos que estimar, para cada \\(x\\) y \\(y\\in\\{1,\\ldots, K\\}\\), las probabilidades condicionales de clase: \\[p_y(x) = P(Y = y|X = x).\\] Consideremos primero cómo funciona el modelo de regresión logística (2 clases) Tenemos que \\[p_1(x) = h(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p) = \\exp(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p)/Z \\] y \\[p_2 (x) = 1/Z\\] donde \\(Z = 1 + \\exp(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p)\\). Podemos generalizar para más de 2 clases usando una idea similar: \\[p_1(x) = \\exp(\\beta_{0,1} + \\beta_{1,1}x_1 + \\ldots + \\beta_{p,1} x_p)/Z\\] \\[p_2(x) = \\exp(\\beta_{0,2} + \\beta_{1,2}x_2 + \\ldots + \\beta_{p.2} x_p)/Z\\] hasta \\[p_{K-1}(x) = \\exp(\\beta_{0,{K-1}} + \\beta_{1,{K-1}}x_2 + \\ldots + \\beta_{p,{K-1}} x_p)/Z\\] y \\[p_K(x) = 1/Z\\] En este caso, para que las probabilidades sumen 1, necesitamos que \\[Z = 1 + \\sum_{j=1}^{K-1}\\exp(\\beta_0^j + \\beta_1^jx_2 + \\ldots + \\beta_p^j x_p)\\] Para ajustar coeficientes, usamos el mismo criterio de maximizar la log verosimilitud. Buscamos maximizar: \\[l(\\beta)= \\sum_{i=1}^N p_{g^{(i)}}(x^{(i)}),\\] donde \\(\\beta\\) contiene todos los coeficientes organizados en un vector de tamaño \\((p+1)(K+1)\\): \\[\\beta = ( \\beta_0^1, \\beta_1^1, \\ldots , \\beta_p^1, \\beta_0^2, \\beta_1^2, \\ldots , \\beta_p^2, \\ldots \\beta_0^{K-1}, \\beta_1^{K-1}, \\ldots , \\beta_p^{K-1} )\\] Y ahora podemos usar algún método númerico para maximizar la log verosimilitud (por ejemplo, descenso en gradiente). Cuando es muy importante tener probabilidades bien calibradas, el enfoque multinomial es más apropiado, pero muchas veces, especialmente si sólo nos interesa clasificar, los dos métodos dan resultados similares. Utilizando la función multinom del paquete nnet se puede hacer regresión logística multinomial. La función glm no tiene la familia multinomial. 7.11 Identificabilidad y separación Hay dos razones por las cuales una regresión logística puede estar no identificada (esto es, tener parámetros que no se pueden estimar con los datos disponibles y el modelo): Como con regresión lineal, si los predictores son colineales, entonces la estimación del predictor lineal, \\(X\\beta\\), no permite una estimación separable de los parámetros individuales \\(\\beta\\). Un problema diferente de identificabilidad, llamado separación puede surgir cuando se tienen datos discretos. Si un predictor \\(x_j\\) está completamente alineado con el resultado, de tal forma que \\(y = 1\\) para todos los casos tales que \\(x_j\\) exceda un umbral \\(T\\), y \\(y=0\\) para todos los casos donde \\(x_j&lt;T\\), entonces el mejor coeficiente para \\(\\beta_j\\) es \\(\\infty\\). A la inversa, si \\(y=1\\) para todos los casos cuando \\(x_j&lt;T\\), y \\(y=0\\) para todos los casos cuando \\(x_j &gt; T\\), entonces \\(\\hat{\\beta}_j\\) sería \\(-\\infty\\). Más generalmente, este problema ocurre si cualquier combinación lineal de predictores se alinea perfectamente con el resultado. 7.12 Tarea Los datos nes (de la encuesta National Election Study) contienen datos de la preferencia presidencial y el ingreso de los votantes durante las elecciones de 1992. Los datos, junto con otras variables que incluyen sex (gender), ethnicity (race), education (educ1), party identification (partyid7), ingreso (ìncome), y political ideology (ìdeo7). Ajusta varios modelos de regresión logística del apoyo a Bush (para el año 1992) dadas estas entradas incluyendo como predictores el ingreso y las variables antes mencionadas. La variable respuesta es vote como vimos en clase. Evalúa y compara diferentes modelos que hayas ajustado. Considera los coeficientes estimados y sus errores estándar. ¿Son significativos? Compara modelos utilizando AIC. Elige el mejor modelo e interprétalo. Compara la importancia de cada variable de entrada en la predicción. "],
["regresion-logistica-3.html", "Clase 8 Regresión logística 3 8.1 Ejemplo óscares 8.2 Repaso de regresión logística 8.3 Regresión logística con interacciones 8.4 Gráficas del modelo con interacciones 8.5 Agregar más predictores 8.6 Evaluación de modelos de regresión logística 8.7 Diferencias predictivas promedio en la escala de probabilidad 8.8 Tarea", " Clase 8 Regresión logística 3 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 8.1 Ejemplo óscares Algunas de los factores citados usualmente por las personas como importantes para que una película gane un Óscar son: Estar nominada a Mejor Director. Haber ganado un premio en los Director’s Guild Awards. Tener más nominaciones a la Academia. Ganó mejor película en Golden Globe Awards. La calificación en IMdb. El score de Metacritic El gusto de las personas por la película. El score en RT de los críticos más destacados. Las recaudaciones en taquilla dométicas. Las recaudaciones en taquilla generales. El presupuesto con el que se realizó la película. La duración de la película. El número de estrellas “conocidas” Contamos con datos que provienen de varias fuentes y se tienen las siguientes variables disponibles: Variable Descripción Fuente film Nombre de la película nominada year Año de nominación de la película release_date Fecha de lanzamiento IMdb mpaa Clasificación IMdb imdb_score Rating de IMdb IMdb metacritic_score Score de Metacritic IMdb rt_audience_score % de personas que la favorecen Rotten Tomatoes rt_critic_score Score de “Top critics” Rotten Tomatoes bo Recaudado en taquilla Box Office Mojo budget Estimated budget Wikipedia running_time Duración (en minutos) Wikipedia stars_count # de actores mostrados en el recuadro Wikipedia aabd Nominación a mejor director Óscar Wikipedia dga Ganadora del Director’s Guild Wikipedia noms Número de nominaciones al Óscar Wikipedia ggbp Ganadora en los Globos Wikipedia winner Ganó Óscar Wikipedia oscars &lt;- read_csv(&quot;datos/oscars.csv&quot;) glimpse(oscars) #&gt; Observations: 156 #&gt; Variables: 17 #&gt; $ film &lt;chr&gt; &quot;Forrest Gump&quot;, &quot;Four Weddings and a Funeral... #&gt; $ year &lt;int&gt; 1994, 1994, 1994, 1994, 1994, 1995, 1995, 19... #&gt; $ release_date &lt;date&gt; 1994-07-06, 1994-04-15, 1994-10-14, 1994-10... #&gt; $ mpaa &lt;chr&gt; &quot;PG-13&quot;, &quot;R&quot;, &quot;R&quot;, &quot;PG-13&quot;, &quot;R&quot;, &quot;PG&quot;, &quot;G&quot;, ... #&gt; $ imdb_score &lt;dbl&gt; 8.8, 7.1, 8.9, 7.5, 9.3, 7.6, 6.8, 8.4, 7.7,... #&gt; $ metacritic_score &lt;int&gt; 82, 81, 94, 88, 80, 77, 83, 68, 84, 81, 85, ... #&gt; $ rt_audience_score &lt;int&gt; 95, 74, 96, 87, 98, 87, 67, 85, 90, 94, 92, ... #&gt; $ rt_critic_score &lt;int&gt; 72, 95, 94, 96, 91, 95, 97, 77, 98, 93, 93, ... #&gt; $ bo &lt;dbl&gt; 677.90, 245.70, 213.90, 24.80, 58.30, 355.20... #&gt; $ budget &lt;dbl&gt; 55.0, 2.8, 88.5, 31.0, 25.0, 52.0, 30.0, 65.... #&gt; $ running_time &lt;int&gt; 142, 117, 154, 133, 142, 140, 92, 178, 136, ... #&gt; $ stars_count &lt;int&gt; 5, 10, 12, 5, 7, 6, 2, 4, 4, 3, 5, 7, 12, 5,... #&gt; $ aabd &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,... #&gt; $ dga &lt;int&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,... #&gt; $ noms &lt;int&gt; 19, 12, 17, 11, 7, 7, 6, 6, 17, 1, 12, 7, 12... #&gt; $ ggbp &lt;int&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,... #&gt; $ winner &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,... Ajustamos un modelo de regresión logística para “winner” utilizando como predictores “imdb_score + metacritic_score + rt_audience_score + rt_critic_score + bo + budget + running_time + stars_count + aabd + dga + noms + ggbp”: oscars_2 &lt;- oscars %&gt;% mutate(imdb_score = imdb_score/10, metacritic_score = metacritic_score/100, rt_audience_score = rt_audience_score/100, bo = log(bo), budget = log(budget), running_time = log(running_time), stars_count = log(stars_count)) %&gt;% filter(year &lt; 2017) fit.1 &lt;- glm(formula = winner ~ imdb_score + rt_critic_score + bo + budget + running_time + stars_count + aabd + dga + noms + ggbp, family = binomial(link = &quot;logit&quot;), data = oscars_2) fit.1 #&gt; #&gt; Call: glm(formula = winner ~ imdb_score + rt_critic_score + bo + budget + #&gt; running_time + stars_count + aabd + dga + noms + ggbp, family = binomial(link = &quot;logit&quot;), #&gt; data = oscars_2) #&gt; #&gt; Coefficients: #&gt; (Intercept) imdb_score rt_critic_score bo #&gt; -14.7473 11.0596 -0.0426 0.1991 #&gt; budget running_time stars_count aabd #&gt; -0.7994 0.9864 0.6808 0.6482 #&gt; dga noms ggbp #&gt; 3.6727 0.1642 0.4169 #&gt; #&gt; Degrees of Freedom: 146 Total (i.e. Null); 136 Residual #&gt; Null Deviance: 128 #&gt; Residual Deviance: 65.7 AIC: 87.7 mean(oscars$dga) #&gt; [1] 0.154 Podemos ver las probabilidades \\(p_1({x^{(i)}})\\), \\(i=1,\\ldots,N\\), que predice el modelo utilizando la función predict: predict(fit.1, type=&quot;response&quot;) #&gt; 1 2 3 4 5 6 7 8 #&gt; 0.979292 0.080864 0.210608 0.013778 0.060864 0.188686 0.002499 0.034669 #&gt; 9 10 11 12 13 14 15 16 #&gt; 0.042967 0.004579 0.095418 0.007524 0.193038 0.116403 0.930428 0.025499 #&gt; 17 18 19 20 21 22 23 24 #&gt; 0.105491 0.023082 0.042503 0.744668 0.011385 0.029657 0.773747 0.119831 #&gt; 25 26 27 28 29 30 31 32 #&gt; 0.016132 0.978409 0.062259 0.040791 0.014552 0.032914 0.044156 0.478123 #&gt; 33 34 35 36 37 38 39 40 #&gt; 0.018495 0.150354 0.029615 0.938574 0.066907 0.063561 0.021840 0.128031 #&gt; 41 42 43 44 45 46 47 48 #&gt; 0.837398 0.026380 0.324370 0.028328 0.053700 0.186569 0.005487 0.102118 #&gt; 49 50 51 52 53 54 55 56 #&gt; 0.004324 0.912908 0.032282 0.454924 0.034419 0.060990 0.059391 0.944622 #&gt; 57 58 59 60 61 62 63 64 #&gt; 0.071828 0.335235 0.054461 0.014381 0.181415 0.011564 0.034738 0.850732 #&gt; 65 66 67 68 69 70 71 72 #&gt; 0.037506 0.054856 0.046618 0.023317 0.757340 0.055775 0.042111 0.028372 #&gt; 73 74 75 76 77 78 79 80 #&gt; 0.873698 0.057019 0.087092 0.002829 0.015224 0.011532 0.014084 0.071483 #&gt; 81 82 83 84 85 86 87 88 #&gt; 0.010316 0.017203 0.598860 0.000934 0.025656 0.006078 0.118978 0.021457 #&gt; 89 90 91 92 93 94 95 96 #&gt; 0.055964 0.031121 0.923980 0.055082 0.002808 0.012944 0.016335 0.007388 #&gt; 97 98 99 100 101 102 103 104 #&gt; 0.003131 0.024420 0.006746 0.725232 0.061440 0.114100 0.002055 0.002480 #&gt; 105 106 107 108 109 110 111 112 #&gt; 0.017222 0.406895 0.019276 0.025923 0.033696 0.011340 0.060555 0.099039 #&gt; 113 114 115 116 117 118 119 120 #&gt; 0.006466 0.356416 0.075537 0.007128 0.030705 0.135007 0.007054 0.039183 #&gt; 121 122 123 124 125 126 127 128 #&gt; 0.009095 0.051619 0.002576 0.240445 0.272211 0.004463 0.142060 0.142537 #&gt; 129 130 131 132 133 134 135 136 #&gt; 0.065616 0.055508 0.005639 0.008353 0.004578 0.059580 0.042772 0.046167 #&gt; 137 138 139 140 141 142 143 144 #&gt; 0.013173 0.500113 0.015982 0.006753 0.036636 0.007289 0.010139 0.825918 #&gt; 145 146 147 #&gt; 0.047052 0.138713 0.345245 Responde las siguientes preguntas: La variable que tiene más impacto en la probabilidad de ganar un Óscar… Ganó un premio en el Director’s Guild. Número de nominaciones a los Óscares. Crítica de Rotten Tomatoes (Top critics). Presupuesto de la película. Si la película ganó un premio en los Director’s Guild, entonces la probabilidad logit de ganar un Óscar aumenta en 3.67. los momios de ganar un Óscar son \\(0.92\\). la probabilidad de ganar un Óscar aumenta en \\(0.92\\). los momios de ganar un Óscar aumentan en \\(e^{3.67}\\). Correspondientes al 2017 estas películas fueron nominadas al premio de la academia: oscars %&gt;% filter(year == 2017) %&gt;% select(film,year,release_date, mpaa, imdb_score,rt_critic_score) %&gt;% knitr::kable() film year release_date mpaa imdb_score rt_critic_score Call Me by Your Name 2017 2018-01-19 R 8.1 96 Darkest Hour 2017 2017-12-22 PG-13 7.4 86 Dunkirk 2017 2017-07-21 PG-13 8.0 93 Get Out 2017 2017-02-24 R 7.7 99 Lady Bird 2017 2017-12-01 R 7.6 99 Phantom Thread 2017 2018-01-19 R 7.8 91 The Post 2017 2018-01-12 PG-13 7.3 88 The Shape of Water 2017 2017-12-22 R 7.7 92 Three Billboards Outside Ebbing, Missouri 2017 2017-12-01 R 8.3 92 Veamos qué predicciones obtenemos para el 2017: oscars_3 &lt;- oscars %&gt;% mutate(imdb_score = imdb_score/10, metacritic_score = metacritic_score/100, rt_audience_score = rt_audience_score/100, bo = log(bo), budget = log(budget), running_time = log(running_time), stars_count = log(stars_count)) %&gt;% filter(year == 2017) %&gt;% select(imdb_score,rt_critic_score,bo,budget,running_time,stars_count,aabd,dga,noms,ggbp) predict(fit.1, type=&quot;response&quot;, newdata = oscars_3) #&gt; 1 2 3 4 5 6 7 8 9 #&gt; 0.07409 0.00672 0.01005 0.06379 0.07457 0.00597 0.00422 0.76347 0.17262 La probabilidad de predicción más alta es 0.76347 que corresponde a The Shape Of Water. 8.2 Repaso de regresión logística En un problema de clasificación donde las \\(y_i\\)’s son binarias, desearíamos tener un modelo de la forma: \\[ \\pi_i = x_i^\\prime \\beta \\] donde \\(\\beta\\) es un vector de coeficientes. El problema es que el componente lineal puede tomar cualquier valor real, mientras que \\(\\pi_i\\) sólo puede tomar valores entre \\(0\\) y \\(1\\). Una alternativa es utilizar los momios: \\[ \\Omega_i = \\dfrac{\\pi_i}{1-\\pi_i}, \\] el cociente de la probabilidad y su complemento, la razón de exitosos por fracasados. Nota: Si la probabilidad de un evento es \\(1/2\\), entonces los momios son uno a uno o justos. Si la probabilidad es \\(1/3\\), entonces los momios son uno a dos. Los momios toman valores entre \\(0\\) e \\(\\infty\\), lo cual no los hace del todo útiles para especificar nuestro modelo. Por lo tanto lo planteamos de la forma: \\[ \\mbox{logit}(\\pi_i) = x_i^\\prime\\beta \\] La trasformación logit es uno a uno. La función logit inversa \\(\\mbox{logit}^{-1}\\) nos permite regresar de probabilidades logits a probabilidades usuales. Podemos ver gráficamente la transformación logit: logit &lt;- function(x){log((x/((1-x))))} graf_data &lt;- data_frame(x = seq(0, 1, length.out = 100), logit = logit(x)) ggplot(graf_data, aes(x = x)) + geom_line(aes(y = logit), colour = &#39;lightpink&#39;, size=1.2) Despejando para \\(\\pi_i\\) obtenemos \\[ \\pi_i = \\mbox{logit}^{-1}(\\eta_i)=\\mbox{logit}^{-1}(x_i^\\prime\\beta)=\\dfrac{e^{x_i^\\prime\\beta}}{1+e^{x_i^\\prime\\beta}} \\] donde \\(\\eta_i=x_i^\\prime\\beta\\). Cada variable aleatoria \\(y_i\\) puede tomar valor de \\(0\\) o \\(1\\), fracaso o éxito, respectivamente. Por lo tanto, \\(y_i\\) tiene una distribución Bernoulli con probabilidad de éxito \\(\\pi_i\\). Y se tiene que \\[ p_1(x_i) = \\pi_i = \\mbox{logit}^{-1}(x_i^\\prime\\beta). \\] Recordemos las interpretaciones de los coeficientes: Evaluar en o alrededor de la media: \\[\\mbox{logit}^{-1}(\\beta_0+\\beta_j\\cdot \\bar{x}_j).\\] Interpretar como un cambio en la probabilidad ante un cambio unitario en \\(x\\) alrededor de la media: \\[\\mbox{logit}^{-1}(\\beta_0+\\beta_j\\cdot \\bar{x}_j) - \\mbox{logit}^{-1}(\\beta_0+\\beta_j\\cdot (\\bar{x}_j-1)).\\] Calcular la derivada de la curva logística en la media: \\[\\dfrac{\\beta_j\\, e^{\\beta_0+\\beta_j \\bar{x}_j}}{(1 + e^{\\beta_0 +\\beta_j \\bar{x}_j})^2}.\\] Dividir entre 4: \\[\\dfrac{\\beta_j}{4}.\\] Se interpreta como la diferencia en la probabilidad ante un cambio unitario en \\(x_j\\) alrededor de la media (aproximadamente). Calcular el cociente de momios: \\[\\mbox{log}\\left[\\dfrac{P(y_i=1|x)}{P(y_i=0|x)} \\right] = \\alpha + \\beta x.\\] Sumar 1 a la variable \\(x\\) es equivalente a sumar \\(\\beta\\) en ambos lados de la ecuación. Exponenciando nuevamente ambos lados, el cociente de momios se multiplica por \\(e^\\beta\\). Estimación de los parámetros \\(\\beta\\): Se utiliza descenso en gradiente para estimar los coeficientes \\(\\beta_j\\) minimizando la devianza: \\[ D(\\beta) = -2\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})). \\] Responde las siguientes preguntas: Supongamos que se ajustan los coeficientes de un modelo lineal logístico y una nueva observación \\(x\\) es tal que su predicción es \\(h(x^\\prime \\beta)=0.7\\). Esto significa que (selecciona una o más): Nuestra estimación de \\(P(y=0|x;\\beta)\\) es 0.7. Nuestra estimación de \\(P(y=1|x;\\beta)\\) es 0.7. Nuestra estimación de \\(P(y=0|x;\\beta)\\) es 0.3. Nuestra estimación de \\(P(y=1|x;\\beta)\\) es 0.3. Supongamos que se ajusta un modelo logístico \\(p_1(x_i)=h(\\beta_0+\\beta_1x_1^{(i)}+\\beta_2 x_2^{(i)})\\). Supongamos que \\(\\beta_0=6, \\;\\beta_1=-1,\\; \\beta_2=0\\). ¿Cuál de las siguientes figuras puede servir como una regla de decisión para este modelo?         8.3 Regresión logística con interacciones Recordemos el modelo de pozos en Bangladesh: wells &lt;- read_csv(&quot;datos/wells.csv&quot;) wells &lt;- wells %&gt;% mutate(dist_100 = dist/100) fit.2 &lt;- glm(switch ~ dist_100, data = wells, family=binomial(link=&quot;logit&quot;)) fit.2 Posteriormente añadimos una segunda variable: fit.3 &lt;- glm(switch ~ dist_100 + arsenic, data = wells, family=binomial(link=&quot;logit&quot;)) fit.3 #&gt; #&gt; Call: glm(formula = switch ~ dist_100 + arsenic, family = binomial(link = &quot;logit&quot;), #&gt; data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist_100 arsenic #&gt; 0.00275 -0.89664 0.46077 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3017 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 3930 AIC: 3940 Añadimos una interacción entre estos dos términos: fit.4 &lt;- glm(switch ~ dist_100 + arsenic + dist_100:arsenic, data = wells, family=binomial(link=&quot;logit&quot;)) fit.4 #&gt; #&gt; Call: glm(formula = switch ~ dist_100 + arsenic + dist_100:arsenic, #&gt; family = binomial(link = &quot;logit&quot;), data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist_100 arsenic dist_100:arsenic #&gt; -0.148 -0.577 0.556 -0.179 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3016 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 3930 AIC: 3940 Para entender los números en la tabla, usamos los siguientes trucos: Evaluar predicciones e interacciones en la media de los datos, que tienen valores promedio de 0.48 para distancia y 1.66 para arsénico (es decir, una distancia media de 48 metros al pozo seguro más cercano, y un nivel promedio de arsénico de 1.66 entre los pozos inseguros). Dividir entre 4 para obtener diferencias predictivas aproximadas en la escala de probabilidad. Intrepretamos los coeficientes: El término constante no tiene interpretación: \\(\\mbox{logit}^{-1}(-0.15) = 0.47\\) es la probabilidad estimada de cambio, si la distancia al pozo seguro más cercano es \\(0\\) y el nivel de arsénico del pozo actual es 0. Esto es imposible porque la distribución de arsénico en pozos inseguros comienza en \\(0.5\\). En cambio, podemos evaluar la predicción en los valores promedio de \\(\\mbox{dist_100} = 0.48\\) y \\(\\mbox{arsénico} = 1.66\\), la probabilidad de cambiar de pozo es \\(\\mbox{logit}^1(-0.15 - 0.58 \\cdot 0.48 + 0.56 \\cdot 1.66 - 0.18 \\cdot 0.48 \\cdot 1.66) = 0.59\\). Coeficiente de distancia: esto corresponde a la comparación de dos pozos que difieren en 1 en dist_100, si el nivel de arsénico es 0 para ambos pozos. Una vez más, no debemos tratar de interpretarlo. En cambio, podemos ver el valor promedio, arsénico = 1.66, donde la distancia tiene un coeficiente de \\(-0.58 - 0.18 · 1.66 = -0.88\\) en la escala logit. Para interpretar esto rápidamente en la escala de probabilidad, lo dividimos por 4: \\(-0.88 / 4 = -0.22\\). Por lo tanto, al nivel medio de arsénico en los datos, a cada 100 metros de distancia le corresponden a una diferencia negativa aproximada del 22% en la probabilidad de cambio. Coeficiente para arsénico: esto equivale a comparar dos pozos que difieren en 1 en arsénico, si la distancia al pozo seguro más cercano es 0 para ambos. Evaluamos la comparación en el valor promedio para la distancia, \\(\\mbox{dist_100} = 0.48\\), donde el arsénico tiene un coeficiente de \\(0.56 - 0.18 · 0.48 = 0.47\\) en la escala logit. Para interpretar esto rápidamente en la escala de probabilidad, lo dividimos entre \\(4\\): \\(0.47 / 4 = 0.12\\). Por lo tanto, en el nivel medio de distancia en los datos, cada unidad adicional de arsénico corresponde a una diferencia positiva aproximada del 12% en la probabilidad de cambio. Coeficiente para el término de interacción: se puede interpretar de dos maneras: por cada unidad adicional de arsénico, el valor \\(-0.18\\) se agrega al coeficiente de distancia. Ya hemos visto que el coeficiente de distancia es \\(-0.88\\) en el nivel promedio de arsénico, por lo que podemos entender la interacción diciendo que la importancia de la distancia como predictor aumenta para los hogares con niveles más altos de arsénico. por cada \\(100\\) metros adicionales de distancia al pozo más cercano, se agrega el valor \\(-0.18\\) al coeficiente de arsénico. Ya hemos visto que el coeficiente de distancia es \\(0.47\\) a la distancia promedio al pozo seguro más cercano, y así podemos entender la interacción diciendo que la importancia del arsénico como predictor disminuye para los hogares que están más lejos de los pozos seguros existentes. Centrando las variables Como se discutió anteriormente en el contexto de la regresión lineal, antes de ajustar las interacciones tiene sentido centrar las variables de entrada para que podamos interpretar los coeficientes más fácilmente. Las entradas centradas son: wells &lt;- wells %&gt;% mutate(dist_100_c = dist_100 - mean(dist_100), arsenic_c = arsenic - mean(arsenic)) Podemos reajustar el modelo usando las variables de entrada centradas, lo que hará que los coeficientes sean mucho más fáciles de interpretar: fit.5 &lt;- glm(switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c, data = wells, family=binomial(link=&quot;logit&quot;)) fit.5 #&gt; #&gt; Call: glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c, #&gt; family = binomial(link = &quot;logit&quot;), data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist_100_c arsenic_c #&gt; 0.351 -0.874 0.470 #&gt; dist_100_c:arsenic_c #&gt; -0.179 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3016 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 3930 AIC: 3940 Centramos las entradas, no los predictores. Por lo tanto, no centramos la interacción (\\(\\mbox{dist_100} * \\mbox{arsénico}\\)); más bien, incluimos la interacción de las dos variables de entrada centradas. Interpretamos los coeficientes en esta nueva escala: Término constante: \\(\\mbox{logit}^{-1}(0.35) = 0.59\\) es la probabilidad estimada cambiar de pozo, si \\(\\mbox{dist_100_c} = \\mbox{arsenic_c} = 0\\), es decir, en las medias de la distancia al pozo seguro más cercano y el nivel de arsénico. (Obtuvimos este mismo cálculo, pero con más esfuerzo, con nuestro modelo anterior con datos no centrados). Coeficiente de distancia: éste es el coeficiente de distancia (en la escala logit) si el nivel de arsénico está en su valor promedio. Para interpretar esto rápidamente en la escala de probabilidad, lo dividimos por 4: \\(-0.88 / 4 = -0.22\\). Por lo tanto, al nivel medio de arsénico en los datos, cada 100 metros de distancia corresponde a una diferencia negativa aproximada del 22% en la probabilidad de cambio. Coeficiente para arsénico: este es el coeficiente para el nivel de arsénico si la distancia al pozo seguro más cercano está en su valor promedio. Para interpretar esto rápidamente en la escala de probabilidad, lo dividimos por 4: \\(0.47 / 4 = 0.12\\). Por lo tanto, en el nivel medio de distancia en los datos, cada unidad adicional de arsénico corresponde a una diferencia positiva aproximada del \\(12\\%\\) en la probabilidad de cambio. Coeficiente para el término de interacción: esto no se modifica al centrarse y tiene la misma interpretación que antes. Las predicciones para nuevas observaciones no se modifican. Centrar los predictores cambia las interpretaciones de los coeficientes pero no cambia el modelo subyacente. Estima el error estándar del coeficiente de interacción usando la técnica de bootsrap. ¿Es significativo dicho coeficiente? summary(fit.5) #&gt; #&gt; Call: #&gt; glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c, #&gt; family = binomial(link = &quot;logit&quot;), data = wells) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.78 -1.20 0.77 1.08 1.85 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.3511 0.0399 8.81 &lt;2e-16 *** #&gt; dist_100_c -0.8737 0.1048 -8.34 &lt;2e-16 *** #&gt; arsenic_c 0.4695 0.0421 11.16 &lt;2e-16 *** #&gt; dist_100_c:arsenic_c -0.1789 0.1023 -1.75 0.08 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 4118.1 on 3019 degrees of freedom #&gt; Residual deviance: 3927.6 on 3016 degrees of freedom #&gt; AIC: 3936 #&gt; #&gt; Number of Fisher Scoring iterations: 4 8.4 Gráficas del modelo con interacciones La forma más clara de visualizar el modelo de interacción es graficar la función de la curva de regresión para cada posible escenario. invlogit &lt;- function(x){ exp(x)/(1+exp(x)) } ggplot(wells, aes(x = dist_100, y = switch)) + geom_jitter(width = 0.308, height = 0.1, size = 0.1) + stat_function(fun = function(x){ invlogit(fit.5$coef[1] + fit.5$coef[2]*x + 0.5 + fit.5$coef[4]*0.5*x)}, xlim = c(-0.3,3.5)) + stat_function(fun = function(x){ invlogit(fit.5$coef[1] + fit.5$coef[2]*x + 1 + fit.5$coef[4]*1*x)}, xlim = c(-0.3,3.5)) + annotate(&quot;text&quot;, x = 0.50, y = 0.45, label = &quot;As=0.5&quot;, size = 4) + annotate(&quot;text&quot;, x = 0.75, y = 0.65, label = &quot;As=1.0&quot;, size = 4) ggplot(wells, aes(x = arsenic, y = switch)) + geom_jitter(width = 0.308, height = 0.1, size = 0.1) + stat_function(fun = function(x){ invlogit(fit.5$coef[1] + 0 + fit.5$coef[3]*x + fit.5$coef[4]*0*x)}, xlim = c(-0.3,10)) + stat_function(fun = function(x){ invlogit(fit.5$coef[1] + fit.5$coef[2]*0.5 + fit.5$coef[3]*x + fit.5$coef[4]*0.5*x)}, xlim = c(-0.3,10)) + annotate(&quot;text&quot;, x = 0.7, y = 0.80, label = &quot;dist=0&quot;, size = 4) + annotate(&quot;text&quot;, x = 2.0, y = 0.65, label = &quot;dist=50&quot;, size = 4) La interacción no es grande en el rango de la mayoría de los datos. En la gráfica de arriba vemos que las líneas se empiezan a juntar a los \\(300\\) metros de distancia. Las diferencias en el cambio asociadas con las diferencias en el nivel de arsénico son grandes si se está cerca de un pozo seguro, pero el efecto disminuye si se está lejos de un pozo seguro. Esta interacción tiene algún sentido; sin embargo, hay cierta incertidumbre en el tamaño de la interacción (de la tabla de regresión anterior, una estimación de \\(-0.18\\) con un error estándar de \\(0.10\\)). Solo hay unos pocos datos en el área donde la interacción hace alguna diferencia. 8.5 Agregar más predictores ¿Son más propensos los usuarios a cambiar de pozo si pertenecen a alguna asociación en su comunidad o si tienen mayor educación? Para ver, agregamos dos entradas: assoc = 1 si un miembro del hogar pertenece a alguna organización comunitaria educ = años de educación del usuario del pozo. En realidad, trabajamos con \\(\\mbox{educ4} = \\mbox{educ} / 4\\), por las razones habituales de hacer que su coeficiente de regresión sea más interpretable: ahora representa la diferencia predictiva de agregar cuatro años de educación. wells &lt;- wells %&gt;% mutate(educ4 = educ / 4) fit.6 &lt;- glm(switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c + assoc + educ4, data = wells, family=binomial(link=&quot;logit&quot;)) fit.6 #&gt; #&gt; Call: glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c + #&gt; assoc + educ4, family = binomial(link = &quot;logit&quot;), data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist_100_c arsenic_c #&gt; 0.203 -0.875 0.475 #&gt; assoc educ4 dist_100_c:arsenic_c #&gt; -0.123 0.168 -0.161 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3014 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 3910 AIC: 3920 Nota: Para los hogares con pozos inseguros, pertenecer a una asociación comunitaria sorprendentemente no es predictivo de cambio de pozo, después de controlar los otros factores en el modelo. Sin embargo, las personas con educación superior tienen más probabilidades de cambiar: la diferencia estimada bruta es \\(0.17 / 4 = 0.04\\), o una diferencia positiva de \\(4\\%\\) en la probabilidad de cambio cuando se comparan hogares que difieren en 4 años de educación. El coeficiente para la educación tiene sentido y es estadísticamente significativo, por lo que lo mantenemos en el modelo. El coeficiente de asociación comunitaria no tiene sentido y no es estadísticamente significativo, por lo que lo eliminamos. fit.7 &lt;- glm (switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c + educ4, data = wells, family = binomial(link=&quot;logit&quot;)) fit.7 #&gt; #&gt; Call: glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c + #&gt; educ4, family = binomial(link = &quot;logit&quot;), data = wells) #&gt; #&gt; Coefficients: #&gt; (Intercept) dist_100_c arsenic_c #&gt; 0.148 -0.875 0.477 #&gt; educ4 dist_100_c:arsenic_c #&gt; 0.169 -0.163 #&gt; #&gt; Degrees of Freedom: 3019 Total (i.e. Null); 3015 Residual #&gt; Null Deviance: 4120 #&gt; Residual Deviance: 3910 AIC: 3920 Añadimos otras interacciones (centrando la variable de educación): wells &lt;- wells %&gt;% mutate(educ4_c = educ4 - mean(educ4)) fit.8 &lt;- glm(switch ~ dist_100_c + arsenic_c + educ4_c + dist_100_c:arsenic_c + dist_100_c:educ4_c + arsenic_c:educ4_c, data = wells, family = binomial(link=&quot;logit&quot;)) summary(fit.8) #&gt; #&gt; Call: #&gt; glm(formula = switch ~ dist_100_c + arsenic_c + educ4_c + dist_100_c:arsenic_c + #&gt; dist_100_c:educ4_c + arsenic_c:educ4_c, family = binomial(link = &quot;logit&quot;), #&gt; data = wells) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.571 -1.196 0.731 1.072 1.871 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.3563 0.0403 8.84 &lt; 2e-16 *** #&gt; dist_100_c -0.9029 0.1073 -8.41 &lt; 2e-16 *** #&gt; arsenic_c 0.4950 0.0431 11.50 &lt; 2e-16 *** #&gt; educ4_c 0.1850 0.0392 4.72 2.4e-06 *** #&gt; dist_100_c:arsenic_c -0.1177 0.1035 -1.14 0.2557 #&gt; dist_100_c:educ4_c 0.3227 0.1066 3.03 0.0025 ** #&gt; arsenic_c:educ4_c 0.0722 0.0439 1.65 0.0996 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 4118.1 on 3019 degrees of freedom #&gt; Residual deviance: 3891.7 on 3013 degrees of freedom #&gt; AIC: 3906 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Podemos interpretar estas nuevas interacciones entendiendo cómo la educación modifica la diferencia predictiva correspondiente a la distancia y el arsénico. Interacción de distancia y educación: una diferencia de 4 años de educación corresponde a una diferencia de \\(0.32\\) en el coeficiente para \\(\\mbox{dist_100}\\). Como ya hemos visto, \\(\\mbox{dist_100}\\) tiene un coeficiente negativo en promedio; por lo tanto, los cambios positivos en la educación reducen la asociación negativa de la distancia. Esto tiene sentido: las personas con más educación probablemente tengan otros recursos, por lo que andar una distancia extra para obtener agua no es una carga tan pesada. Interacción de arsénico y educación: una diferencia de 4 años de educación corresponde a una diferencia de \\(0.07\\) en el coeficiente de arsénico. Como ya hemos visto, el arsénico tiene un coeficiente positivo en promedio; por lo tanto, aumentar la educación aumenta la asociación positiva del arsénico. Esto tiene sentido: las personas con más educación podrían estar más informadas sobre los riesgos del arsénico y, por lo tanto, ser más sensibles al aumento de los niveles de arsénico (o, a la inversa, tener menos prisa para cambiar de pozos con niveles de arsénico relativamente bajos). Estandarizar los predictores Deberíamos considerar seriamente la posibilidad de estandarizar todos los predictores como una opción predeterminada para ajustar modelos con interacciones. Las dificultades con \\(\\mbox{dist_100}\\) y \\(\\mbox{educ4}\\) en este ejemplo sugieren que la estandarización, restar la media de cada una de las variables de entrada y dividir entre 2 desviaciones estándar. 8.6 Evaluación de modelos de regresión logística Podemos definir residuales en regresión logística como \\[ \\mbox{residual}_i = y_i − E(y_i|X_i) = y_i − \\mbox{logit}^{-1}(X_i\\beta). \\] Los datos \\(y_i\\) son discretos y también los residuales. Por ejemplo, si \\(\\mbox{logit}^{-1} (X_i\\beta) = 0.7\\), entonces \\(\\mbox{residual}_i = -0.7\\) o \\(+0.3\\), dependiendo de si \\(y_i = 0\\) o \\(1\\). Graficamos los residuales de la regresión logística: fit.8 &lt;- glm(switch ~ dist_100_c + arsenic_c + educ4_c + dist_100_c:arsenic_c + dist_100_c:educ4_c + arsenic_c:educ4_c, data = wells, family=binomial(link=&quot;logit&quot;)) # Probabilidades de predicción wells$pred.8 &lt;- fit.8$fitted.values ggplot(wells, aes(x=pred.8, y=switch-pred.8)) + geom_point(size=1) + geom_abline(slope = 0, intercept = 0) + xlab(&quot;P(switch) de predicción&quot;) + ylab(&quot;Observado - estimado&quot;) Vemos que esto no es útil. En la gráfica se ve un patrón fuerte en los residuales debido a que las observaciones de \\(y_i\\) son discretas. Esto nos sugiere hacer una gráfica de residuales agrupados. Para calcular los residuales agrupados dividimos los datos en clases (cubetas) en función de sus valores ajustados. Luego graficamos el residual promedio contra el valor promedio ajustado para cada cubeta. Calculamos la agrupación de los residuales con la siguiente función: binned_residuals &lt;- function(x, y, nclass=sqrt(length(x))){ breaks.index &lt;- floor(length(x)*(1:(nclass-1))/nclass) breaks &lt;- c (-Inf, sort(x)[breaks.index], Inf) output &lt;- NULL xbreaks &lt;- NULL x.binned &lt;- as.numeric (cut (x, breaks)) for (i in 1:nclass){ items &lt;- (1:length(x))[x.binned==i] x.range &lt;- range(x[items]) xbar &lt;- mean(x[items]) ybar &lt;- mean(y[items]) n &lt;- length(items) sdev &lt;- sd(y[items]) output &lt;- rbind(output, c(xbar, ybar, n, x.range, 2*sdev/sqrt(n))) } colnames(output) &lt;- c (&quot;xbar&quot;, &quot;ybar&quot;, &quot;n&quot;, &quot;x.lo&quot;, &quot;x.hi&quot;, &quot;2se&quot;) return (list(binned=output, xbreaks=xbreaks)) } La función binned_residuals recibe como entrada un vector \\(x\\). ¿Es significativo dicho coeficiente? Veamos la gráfica: br.8 &lt;- binned_residuals(wells$pred.8, wells$switch-wells$pred.8, nclass=40) %&gt;% .$binned %&gt;% as.data.frame() ggplot(br.8, aes(xbar, ybar)) + geom_point() + geom_line(aes(x=xbar, y=`2se`), color=&quot;grey60&quot;) + geom_line(aes(x=xbar, y=-`2se`), color=&quot;grey60&quot;) + geom_abline(intercept = 0, slope = 0) + xlab(&quot;P(switch) de predicción&quot;) + ylab(&quot;Residual promedio&quot;) Lo que observamos es los datos divididos en 40 cubetas de igual tamaño. Las líneas de color gris se calculan como \\[ \\displaystyle{2\\sqrt{\\frac{p(1 - p)}{n}}}, \\] donde \\(n\\) es el número de puntos por cubeta. En este caso, \\(n = 3020/40 = 75\\) en este caso) indican Si consideramos \\(\\pm 2\\) errores estándar esperamos que caigan adentro de estas bandas aproximadamente el 95% de los residuales agrupados (si el modelo fuera realmente verdadero). 8.6.1 Gráficas de residuales agrupados vs predictores Podemos estudiar los residuales graficándolos contra algunos predictores. Graficamos los residuales contra la distancia al pozo seguro más cercano: br.8.dist &lt;- binned_residuals(wells$dist_100, wells$switch-wells$pred.8, nclass=40) %&gt;% .$binned %&gt;% as.data.frame() ggplot(br.8.dist, aes(xbar, ybar)) + geom_point() + geom_line(aes(x=xbar, y=`2se`), color=&quot;grey60&quot;) + geom_line(aes(x=xbar, y=-`2se`), color=&quot;grey60&quot;) + geom_abline(intercept = 0, slope = 0) + xlab(&quot;P(switch) de predicción&quot;) + ylab(&quot;Distancia al pozo seguro más cercano&quot;) No observamos algún patrón en los residuales de la gráfica anterior, lo cual es consistente con el modelo. Vemos ahora la gráfica con arsénico: br.8.ars &lt;- binned_residuals(wells$arsenic, wells$switch-wells$pred.8, nclass=40) %&gt;% .$binned %&gt;% as.data.frame() ggplot(br.8.ars, aes(xbar, ybar)) + geom_point() + geom_line(aes(x=xbar, y=`2se`), color=&quot;grey60&quot;) + geom_line(aes(x=xbar, y=-`2se`), color=&quot;grey60&quot;) + geom_smooth(method = &#39;loess&#39;, se=F, color=&quot;red&quot;, size=0.5) + geom_abline(intercept = 0, slope = 0) + xlab(&quot;P(switch) de predicción&quot;) + ylab(&quot;Arsénico&quot;) Esta gráfica sí muestra un patrón en los residuales. Tiene un residual negativo extremo. Además podemos decir que: las personas en los pozos para las primeras 3 cubetas tienen probabilidad de cambio promedio de: personas_cub3 &lt;- wells %&gt;% filter(arsenic &lt;= 0.59) sum(personas_cub3$switch)/nrow(personas_cub3) #&gt; [1] 0.308 el modelo predice que la probabilidad de cambio de estas personas en las primras tres cubetas es en promedio: mean(personas_cub3$pred.8) #&gt; [1] 0.485 Esto quiere decir que la probabilidad de que realmente cambien de pozo es aproximadamente 20% menos que la que predice el modelo. Observamos un patrón en los residuales: los residuales positivos (ej promedio) están en la mitad del rango de arsénico, y los residuales están en los extremos. 8.6.2 Transformaciones Ahora consideramos transformar la variable de arsénico: vemos un patrón en los residuales en el cual estos primero aumentan y luego disminuyen. Para solucionar esto hay algunas opciones: usar una transformación logarítmica. agregar un término cuadrático al término lineal. En este caso como la variable de arsénico es no negativa es un poco más práctico utilizar la transformación de logaritmo. wells &lt;- wells %&gt;% mutate(arsenic_log = log(arsenic), arsenic_log_c = arsenic_log - mean(arsenic_log)) fit.9 &lt;- glm(switch ~ dist_100_c + arsenic_log_c + educ4_c + dist_100_c:arsenic_log_c + dist_100_c:educ4_c + arsenic_log_c:educ4_c, data = wells, family=binomial(link=&quot;logit&quot;)) summary(fit.9) #&gt; #&gt; Call: #&gt; glm(formula = switch ~ dist_100_c + arsenic_log_c + educ4_c + #&gt; dist_100_c:arsenic_log_c + dist_100_c:educ4_c + arsenic_log_c:educ4_c, #&gt; family = binomial(link = &quot;logit&quot;), data = wells) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.103 -1.162 0.718 1.040 1.923 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.3452 0.0405 8.53 &lt; 2e-16 *** #&gt; dist_100_c -0.9796 0.1112 -8.81 &lt; 2e-16 *** #&gt; arsenic_log_c 0.9036 0.0695 13.00 &lt; 2e-16 *** #&gt; educ4_c 0.1785 0.0390 4.58 4.7e-06 *** #&gt; dist_100_c:arsenic_log_c -0.1567 0.1851 -0.85 0.3974 #&gt; dist_100_c:educ4_c 0.3384 0.1078 3.14 0.0017 ** #&gt; arsenic_log_c:educ4_c 0.0601 0.0703 0.85 0.3926 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 4118.1 on 3019 degrees of freedom #&gt; Residual deviance: 3863.1 on 3013 degrees of freedom #&gt; AIC: 3877 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Esto da resultados similares al modelo anterior, los signos de las interacciones son los mismos y los signos de los efectos principales también son los mismos. Volvemos a hacer la gráfica de niveles de arsénico vs residuales: wells$pred.9 &lt;- fit.9$fitted.values br.fit.9 &lt;- binned_residuals(wells$arsenic, wells$switch-wells$pred.9, nclass=40) %&gt;% .$binned %&gt;% as.data.frame() ggplot(br.fit.9, aes(xbar, ybar)) + geom_point() + geom_line(aes(x=xbar, y=`2se`), color=&quot;grey60&quot;) + geom_line(aes(x=xbar, y=-`2se`), color=&quot;grey60&quot;) + geom_smooth(method = &#39;loess&#39;, se=F, color=&quot;red&quot;, size=0.5) + geom_abline(intercept = 0, slope = 0) + xlab(&quot;Residual promedio P(switch)&quot;) + ylab(&quot;Arsénico&quot;) Los residuales se ven mucho mejor, aunque para niveles bajos de arsénico aún hay muchos residuales negativos: los usuarios de pozos con niveles de arsénico justo por encima de \\(0.50\\) tienen menos probabilidad de cambiarse que la que predice el modelo. Posibles explicaciones: psicológicamente, las mediciones justo por encima de \\(0.50\\) podrían parecer no muy graves por error de medición, tal vez algunos de los pozos con \\(0.51\\) o \\(0.52\\) se midieron antes o después y de encontrar niveles de arsénico por debajo de \\(0.5\\) 8.6.3 Tasa de error y comparación contra el modelo nulo La tasa de error se define como la proporción de casos para los cuáles la predicción de \\(y_i\\) a partir de la probabilidad estimada \\[ \\pi_i=p_1(x_i)=\\mbox{logit}^{-1}(X_i\\beta) \\] es incorrecta. Para hacer la predicción para cada \\(y_i\\) es necesario definir un punto de corte, en principio se utiliza el \\(0.5\\) de la forma que la predicción de \\(y_i\\), denotada por \\(\\hat{y}_i\\), es \\[ \\hat{y}_{i} = \\left\\{ \\begin{array}{cl} 1 &amp; \\text{si }\\;\\mbox{logit}^{-1}(X_i\\beta) &gt; 0.5,\\\\ 0 &amp; \\text{en otro caso.} \\end{array}\\right. \\] Por lo tanto, calculamos la tasa de error para el ejemplo de los pozos: (error_rate &lt;- mean((wells$pred.9&gt;0.5 &amp; wells$switch==0) | (wells$pred.9&lt;=0.5 &amp; wells$switch==1))) #&gt; [1] 0.365 Observaciones: La tasa de error siempre debe ser menor que \\(1/2\\). Si la tasa de error fuera mayor o igual a \\(1/2\\), entonces poniendo todas las \\(\\beta^\\prime\\mbox{s}\\) igual a cero obtendríamos un mejor modelo. El modelo nulo se define como el modelo que asigna la misma probabilidad \\(p\\) a toda \\(y_i\\), y dicha probabilidad es \\[p = \\dfrac{1}{n}\\sum_{i=1}^N{y_i}.\\] ¿Cuál o cuáles de las siguientes afirmaciones son ciertas? La tasa de error del modelo nulo es igual a \\(p\\). El modelo nulo es aquel en el cual todas las observaciones son equiprobables. El modelo nulo es simplemente regresión logística con sólo un término constante. La tasa de error del modelo nulo es igual a \\(1-p\\). Por ejemplo, en el modelo de los pozos la tasa de error del modelo nulo es: mean(1-wells$switch) #&gt; [1] 0.425 Es decir, \\(58\\%\\) de los encuestados cambiaron de pozo, pero \\(42\\%\\) no cambiaron de pozo. Interpretación del modelo nulo: el modelo sin predictores le da a cada persona un \\(58\\%\\) de posibilidad de cambio (que corresponde a una predicción puntual de cambiar para cada persona), y esta predicción será incorrecta el \\(42\\%\\) de las veces. Nota: Utilizamos la devianza para evaluar el ajuste de un modelo porque la tasa de error no es un resumen perfecto del desajuste del modelo. La razón de esto es que la tasa de error no puede distinguir entre predicciones de \\(0.6\\) y \\(0.9\\), por ejemplo, porque únicamente toma en cuenta predicciones de \\(y_i\\)’s. Es importante poder predecir \\(y_i\\)’s pero generalmente es más útil trabajar con las probabilidades porque en estas subyace la incertidumbre de la predicción. La tasa de error se utiliza a menudo porque es fácil de interpretar y en la práctica puede llegar a ser muy útil. Una tasa de error igual a la tasa de error del modelo nulo es terrible, y la mejor tasa de error posible es cero. Una tasa de error alta (cercana a la del modelo nulo pero menor) no significa que el modelo no sea útil. La interpretación del modelo puede aportar a explicar el fenómeno de estudio. La razón por la cual la tasa de error del modelo de pozos es alta se resume en los siguientes puntos: La mayoría de los datos están alrededor de las medias de los predictores (la distancia menor a 100 m y arsénico en 0.5 y 1). Por el punto anterior, para la mayoría de los datos la probabilidad de cambio \\(P(\\mbox{switch})=0.58\\) funciona bien, que es simplementa la media en los datos. El modelo da información sobre lo que está pasando en los extremos, pero relativamente hay muy pocos datos, por lo que la precisión predictiva general del modelo no es muy alta. 8.7 Diferencias predictivas promedio en la escala de probabilidad Recordemos: Las regresiones logísticas son inherentemente más difíciles que las regresiones lineales para interpretar. La regresión logística es no lineal en la escala de probabilidad, es decir, a una diferencia unitaria en \\(x\\) no le corresponde una diferencia constante en \\(P(y_i=1)\\). Como resultado de esto, los coeficientes de regresión logística no se pueden interpretar directamente en la escala de probabilidad. Se puede calcular la diferencia predictiva promedio como función de los valores de entrada de los predictores para luego dar una diferencia promedio en \\(P(y=1)\\) correspondiente a una diferencia en cada una de las variables de entrada. Comenzamos utilizando el siguiente modelo sin interacciones para hacer más simple la explicación: fit.10 &lt;- glm(switch ~ dist_100 + arsenic + educ4, data = wells, family=binomial(link=&quot;logit&quot;)) summary(fit.10) #&gt; #&gt; Call: #&gt; glm(formula = switch ~ dist_100 + arsenic + educ4, family = binomial(link = &quot;logit&quot;), #&gt; data = wells) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.577 -1.197 0.755 1.063 1.699 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.2139 0.0931 -2.30 0.022 * #&gt; dist_100 -0.8956 0.1046 -8.56 &lt; 2e-16 *** #&gt; arsenic 0.4684 0.0416 11.26 &lt; 2e-16 *** #&gt; educ4 0.1713 0.0383 4.47 7.7e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 4118.1 on 3019 degrees of freedom #&gt; Residual deviance: 3910.4 on 3016 degrees of freedom #&gt; AIC: 3918 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Ejemplo: Diferencia predictiva promedio en probabilidad de cambio, comparando hogares que están a 100 metros o menos de un pozo seguro más cercano. Comparemos dos hogares: uno con \\(\\mbox{dist_100} = 0\\), y uno con \\(\\mbox{dist_100} = 1\\), pero ambos con los mismos valores en las otras variables de entrada: arsénico y educ4. La diferencia predictiva en probabilidad de cambiar de pozo entre los dos hogares: \\[ \\begin{eqnarray*} \\delta(\\mbox{arsenic}, \\mbox{educ4}) = \\mbox{logit}^{−1}(−0.21 − 0.90 \\cdot 1 + 0.47 \\cdot \\mbox{arsenic} + 0.17 \\cdot \\mbox{educ4}) &amp;−&amp;\\\\ \\mbox{logit}^{−1}(−0.21 − 0.90 \\cdot 0 + 0.47 \\cdot \\mbox{arsenic} + 0.17 \\cdot \\mbox{educ4}). \\end{eqnarray*} \\] Escribimos \\(\\delta\\) como una función de arsénico y educ4 para enfatizar que depende de los niveles de estas otras variables. Promeiamos las diferencias predictivas sobre los \\(n\\) hogares en los datos para obtener: \\[ \\mbox{diferencia predictiva promedio} = \\dfrac{1}{n}\\sum_{i=1}^n{\\delta(\\mbox{arsenic}_i, \\mbox{educ4}_i)} \\] Hacemos el cálculo: b &lt;- coef(fit.10) hi &lt;- 1 lo &lt;- 0 delta &lt;- invlogit (b[1] + b[2]*hi + b[3]*wells$arsenic + b[4]*wells$educ4) - invlogit (b[1] + b[2]*lo + b[3]*wells$arsenic + b[4]*wells$educ4) print(mean(delta)) #&gt; [1] -0.204 El resultado es \\(-0.20\\), lo que implica que, en promedio en los datos, los hogares que están a 100 metros del pozo seguro más cercano tienen un \\(20\\%\\) menos de probabilidades de cambiar, en comparación con los hogares que están justo al lado del pozo seguro más cercano, con mismos niveles de arsénico y mismos niveles de educación. Ejemplo: Diferencia predictiva promedio en probabilidad de cambio, comparando hogares con niveles de arsénico existentes de 0.5 y 1.0. Calculamos la diferencia predictiva y la diferencia predictiva promedio, comparando los hogares en dos niveles diferentes de arsénico, suponiendo que la distancia al pozo seguro más cercano y los niveles de educación son iguales. Elegimos \\(\\mbox{arsenic} = 0.5\\) y \\(1.0\\) como puntos de comparación porque \\(0.5\\) es el nivel más bajo inseguro, \\(1.0\\) es el doble, y esta comparación captura gran parte del rango de los datos: hi &lt;- 1.0 lo &lt;- 0.5 delta &lt;- invlogit (b[1] + b[2]*wells$dist_100 + b[3]*hi + b[4]*wells$educ4) - invlogit (b[1] + b[2]*wells$dist_100 + b[3]*lo + b[4]*wells$educ4) print (mean(delta)) #&gt; [1] 0.0564 El resultado es \\(0.06\\), por lo que esta comparación corresponde a una diferencia del \\(6\\%\\) en la probabilidad de cambiar de pozo. Ejemplo: Diferencia predictiva promedio en probabilidad de cambio, comparando hogares con 0 y 12 años de educación. Calcular nuevamente la diferencia predictiva promedio de la probabilidad de cambio de pozo para hogares con 0 vs 12 años de educación: hi &lt;- 3 lo &lt;- 0 delta &lt;- invlogit (b[1]+b[2]*wells$dist_100+b[3]*wells$arsenic+b[4]*hi) - invlogit (b[1]+b[2]*wells$dist_100+b[3]*wells$arsenic+b[4]*lo) print (mean(delta)) #&gt; [1] 0.117 Diferencias predictivas promedio en presencia de interacciones Por ejemplo, consideremos la diferencia predictiva promedio, comparando \\(\\mbox{dist} = 0\\) con \\(\\mbox{dist} = 100\\), para el modelo que incluye una interacción distancia \\(\\times\\) arsénico: fit.11 &lt;- glm(switch ~ dist_100 + arsenic + educ4 + dist_100:arsenic, data = wells, family=binomial(link=&quot;logit&quot;)) summary(fit.11) #&gt; #&gt; Call: #&gt; glm(formula = switch ~ dist_100 + arsenic + educ4 + dist_100:arsenic, #&gt; family = binomial(link = &quot;logit&quot;), data = wells) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.715 -1.189 0.748 1.069 1.722 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.3490 0.1264 -2.76 0.0057 ** #&gt; dist_100 -0.6047 0.2095 -2.89 0.0039 ** #&gt; arsenic 0.5554 0.0695 7.99 1.4e-15 *** #&gt; educ4 0.1692 0.0383 4.42 1.0e-05 *** #&gt; dist_100:arsenic -0.1629 0.1023 -1.59 0.1115 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 4118.1 on 3019 degrees of freedom #&gt; Residual deviance: 3907.9 on 3015 degrees of freedom #&gt; AIC: 3918 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Calculamos la diferencia predictiva promedio entre hogares a 0 y 100 metros de distancia del pozo seguro como: b &lt;- coef(fit.11) hi &lt;- 1 lo &lt;- 0 delta &lt;- invlogit(b[1] + b[2]*hi + b[3]*wells$arsenic + b[4]*wells$educ4 + b[5]*hi*wells$arsenic) - invlogit(b[1] + b[2]*lo + b[3]*wells$arsenic + b[4]*wells$educ4 + b[5]*lo*wells$arsenic) print(mean(delta)) #&gt; [1] -0.194 Notación general para diferencias predictivas Consideramos que deseamos evaluar las diferencias predictivas una a la vez y usamos la notación de: \\(u\\) para la entrada de interés, y \\(v\\) para el vector de todas las otras entradas. Supongamos que estamos considerando comparaciones entre \\(u=u^{(1)}\\) y \\(u=u^{(0)}\\) y todas las demás variables permanecen constantes. La diferencia predictiva en probabilidades entre los dos casos que difiere solo en \\(u\\) es: \\[ \\delta(u^{(\\mbox{hi})},u^{(\\mbox{lo})},v,\\beta) = P(y=1|u^{(\\mbox{hi})},v,\\beta)−P(y=1|u^{(\\mbox{lo})},v,\\beta). \\] La diferencia predictiva media es la media de las \\(n\\) diferencias predictivas correspondientes a cada observación en los datos: \\[ \\Delta(u^{(\\mbox{hi})}, u^{(\\mbox{lo})}) = \\dfrac{1}{n} \\sum_{i=1}^n{\\delta(u^{(\\mbox{hi})}, u^{(\\mbox{lo})}, v_i, β),} \\] donde \\(v_i\\) representa el vector de otras variables de entrada para la \\(i\\)-ésima observación. Consideremos el modelo de regresión logística con la interacción de educación y distancia: fit.12 &lt;- glm(switch ~ dist_100 + arsenic + educ4 + dist_100:educ4, data = wells, family=binomial(link=&quot;logit&quot;)) Los coeficientes del modelo son: coef(fit.12) #&gt; (Intercept) dist_100 arsenic educ4 dist_100:educ4 #&gt; 0.000496 -1.389852 0.480599 -0.008308 0.382545 b &lt;- coef(fit.12) hi &lt;- 3 lo &lt;- 0 delta &lt;- invlogit(b[1] + b[2]*wells$dist_100 + b[3]*wells$arsenic + b[4]*hi + b[5]*wells$educ*wells$dist_100) - invlogit(b[1] + b[2]*wells$dist_100 + b[3]*wells$arsenic + b[4]*lo + b[5]*wells$educ*wells$dist_100) print(mean(delta)) #&gt; [1] -0.00468 La diferencia predictiva promedio entre hogares con 0 años y 12 años de educación es: -0.05 0.30 0.12 0.05 En regresión logística, el gradiente de \\(D(\\beta)\\) está dado por \\[ p_1(x) = h\\left(\\beta_0+\\beta_1 x_1 + \\beta_2x_2\\right). \\] Se cuenta con los datos de la siguiente tabla y que aparecen en la gráfica de la derecha: ¿Cuál de las siguientes afirmaciones son ciertas? Selecciona una o más. \\(D(\\beta)\\) será una función convexa, por lo que descenso en gradiente debería converger al mínimo global. Para incrementar el ajuste del modelo a los datos podríamos agregar términos polinomiales o de interacción, por ejemplo, \\(p_1(x_i)=h\\left(\\beta_0+\\beta_1 x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_1x_2 +\\beta_5x_2^2\\right)\\). Los datos no se pueden separar utilizando una recta de modo que las observaciones de éxito estén de un lado de la recta y las observaciones de fracaso del otro. Por lo tanto, descenso en gradiente no podría converger. Como lo datos no se pueden separar mediante una recta, entonces el método de regresión logística produciría los mismos resultados que aplicar regresión lineal a estos datos. En regresión logística el gradiente de la devianza está dado por \\[ \\dfrac{\\partial D(\\beta)}{\\partial \\beta_j} = \\sum_{i=1}^N(h_\\beta(x^{(i)}) - y^{(i)})x_j^{(i)}. \\] ¿Cuál de las siguientes expresiones es una actualizción correcta para descenso en gradiente con parámetro \\(\\alpha\\)? Selecciona una o más. \\(\\beta_j := \\beta_j - \\alpha\\frac{1}{N}\\sum_{i=1}^N\\left(\\beta^Tx-y^{(i)}\\right)x_j^{(i)}.\\) \\(\\beta := \\beta - \\alpha\\frac{1}{N}\\sum_{i=1}^N\\left(\\dfrac{1}{1+e^{-\\beta^Tx^{(i)}}}-y^{(i)}\\right)x^{(i)}.\\) \\(\\beta := \\beta - \\alpha\\frac{1}{N}\\sum_{i=1}^N\\left(\\beta^Tx-y^{(i)}\\right)x^{(i)}.\\) \\(\\beta_j := \\beta_j - \\alpha\\frac{1}{N}\\sum_{i=1}^N\\left(\\dfrac{1}{1+e^{-\\beta^Tx^{(i)}}}-y^{(i)}\\right)x_j^{(i)}\\)   (actualiza simultáneamente para toda \\(j\\)). ¿Cuáles de las siguientes afirmaciones son ciertas? La devianza \\(D(\\beta)\\) en regresión logística con \\(N\\geq 1\\) observaciones siempre es no negativa. En regresión logística, descenso en gradiente algunas veces converge a un mínimo global (no logra encontrar el mínimo global). Esta es la razón por la cual preferimos algoritmos más avanzados de optimización, como BFGS y Región de confianza. La regresión lineal siempre funciona bien para hacer clasificación si se clasifica utilizando un umbral en la predicción hecha por regresión lineal. La función logística \\(h(x)=\\frac{1}{1+e^{-x}}\\) nunca es mayor que uno (\\(&gt;1\\)). 8.8 Tarea Utiliza los datos de cambio de pozos de arsénico del archivo wells.csv. Ajusta un modelo de regresión logística para la probabilidad de cambio de pozo utilizando una transformación logarítmica para la variable de distancia. Haz una gráfica que muestre la probabilidad de cambio de pozo como función de la distancia al pozo, y que en la gráfica se muestren también los datos. Haz una gráfica de residuales contra ajustados utilizando residuales agrupados. Calcula la tasa de error del modelo ajustado y compárala con la tasa de error del modelo nulo. Crea variables indicadores correspondientes a: \\(\\mbox{dist} &lt; 100\\), \\(100 \\leq \\mbox{dist} &lt; 200\\), y \\(\\mbox{dist} \\geq 200\\). Ajusta una regresión logística para \\(P(\\mbox{switch})\\) utilizando estas tres variables indicadoras. Con este nuevo modelo repite las gráficas de los incisos b y c, y los cálculos del inciso d. Continuamos con los datos de los pozos. Ajusta una regresión logística utilizando como predictores distancia, el logaritmo del arsénico y su interacción. Interpreta los coeficientes estimados y sus errores estándar. Haz gráficas que muestren la relación entre la probabilidad de cambio de pozo, la distancia y el nivel de arsénico. Calcula las diferencias predictivas promedio correspondientes a: la diferencia entre \\(\\mbox{dist}=0\\) y \\(\\mbox{dist=100}\\), cuando arsénico permanece constante. la diferencia entre \\(\\mbox{dist=100}\\) y \\(\\mbox{dist=200}\\), cuando arsénico permanece constante. la diferencia entre \\(\\mbox{arsenic=0.5}\\) y \\(\\mbox{arsenic=1.0}\\), cuando distancia permanece constante. la diferencia entre \\(\\mbox{arsenic=1.0}\\) y \\(\\mbox{arsenic=2.0}\\), cuando distancia permanece constante. Discute los resultados. "],
["regularizacion.html", "Clase 9 Regularización 9.1 Repaso 9.2 Otras medidas de clasificación 9.3 Análisis de error en clasificación binaria 9.4 Curvas ROC 9.5 Regularización 9.6 Regularización Ridge 9.7 Regularización Lasso 9.8 Tarea", " Clase 9 Regularización .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 9.1 Repaso La tasa de error se define como la proporción de casos para los cuáles la predicción de \\(y_i\\) a partir de la probabilidad estimada \\[ \\pi_i=p_1(x_i)=\\mbox{logit}^{-1}(X_i\\beta) \\] es incorrecta. Para hacer la predicción para cada \\(y_i\\) es necesario definir un punto de corte, en principio se utiliza el \\(0.5\\) de la forma que la predicción de \\(y_i\\), denotada por \\(\\hat{y}_i\\), es \\[ \\hat{y}_{i} = \\left\\{ \\begin{array}{cl} 1 &amp; \\text{si }\\;\\mbox{logit}^{-1}(X_i\\beta) &gt; 0.5,\\\\ 0 &amp; \\text{en otro caso.} \\end{array}\\right. \\] Definimos la devianza como \\[ D(\\beta) = -2\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})), \\] y utilizamos descenso en gradiente paera minimizar \\(D(\\beta\\) con respecto a \\(\\beta\\). Es fácil ver que este método de estimación de los coeficientes (minimizando la devianza) es el método de máxima verosimilitud. La verosimilitud está dada por: \\[ L(\\beta) =\\prod_{i=1}^N p_{y^{(i)}} (x^{(i)}), \\] y la log verosimilitud es \\[ l(\\beta) =\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})). \\] Usamos el factor \\(2\\) en la medida de devianza para usarla más fácilmente en pruebas de hipótesis relacionadas con comparaciones entre modelos. Después de estudiar la tasa de error y la tasa de error del modelo nulo, presentamos técnicas adicionales para evaluar el desempeño de un modelo. Veamos primero las ventajas y desventajas de estas dos medidas: Ventajas Desventajas Devianza - es una buena medida para ajustar y evaluar el desempeño de un modelo - permite comparar modelos - es una medida dificil de interpretar en cuanto a los errores que podemos esperar del modelo Tasa de error - puede interpretarse con facilidad - no puede representar errores de clasificación que son cualitativamente diferentes 9.2 Otras medidas de clasificación ¿Por qué son importantes otras medidas de clasificación? Diagnosticar a alguien con una enfermedad cuando no la tiene tiene consecuencias distintas a diagnosticar como libre de enfermedad a alguien que la tiene. Estas consecuencias dependen de cómo son los tratamientos y de qué tan peligrosa es la enfermedad. Cuando usamos un buscador como Google, es cualitativamente diferente que el buscador omita resultados relevantes a que nos presente resultados irrelevantes. ¿Otros ejemplos? En general, los costos de los distintos errores son distintos, y en muchos problemas quiséramos entenderlos y controlarlos individualmente. Aunque en teoría podríamos asignar costos a los errores y definir una función de pérdida apropiada, en la práctica esto muchas veces no es tan fácil o deseable. Una función de pérdida se utiliza para la estimación de parámetros, y es tal que le asocia a cada observación un valor que representa un costo asociado a la clasificación, generalmente es la diferencia entre los valores estimados y los observados para cada observación en los datos. Podemos, sin embargo, reportar el tipo de errores que ocurren: Matriz de confusión La matriz de confusión \\(C\\) está dada por \\[ C_{i,j}=\\mbox{Nú}\\;\\,\\mbox{mero de casos de la clase verdadera j que son clasificados como clase i} \\] Ejemplo En un ejemplo de tres clases, podríamos obtener la matriz de confusión: A B C A.pred 50 2 0 B.pred 20 105 10 C.pred 20 10 30 Esto quiere decir que de 90 casos de clase \\(A\\), sólo clasificamos a 50 en la clase correcta, de 117 casos de clase \\(B\\), acertamos en 105, etcétera. Podemos ver esta tabla de distintas formas, por ejemplo, usando porcentajes por columna, nos dice cómo se distribuyen los casos de cada clase: knitr::kable(round(prop.table(tabla_1, 2),2)) A B C A.pred 0.56 0.02 0.00 B.pred 0.22 0.90 0.25 C.pred 0.22 0.09 0.75 Mientras que una tabla de porcentajes por renglón nos muestra qué pasa cada vez que hacemos una predicción dada: knitr::kable(round(prop.table(tabla_1, 1),2)) A B C A.pred 0.96 0.04 0.00 B.pred 0.15 0.78 0.07 C.pred 0.33 0.17 0.50 Ahora pensemos cómo podría sernos de utilidad esta tabla. Pensemos qué implicaciones tendría esta tabla de confusión, si la clasificación fuera respecto a: la severidad de emergencias en un hospital, donde A=requiere atención inmediata B=urgente C=puede posponerse, entonces qué implicación tendría cada uno de los números de la tabla. tipos de cliente de un negocio, por ejemplo, A = cliente de gasto potencial alto, B=cliente medio, C=abandonador. Imagínate que tiene un costo intentar conservar a un abandonador, y hay una inversión alta para tratar a los clientes A. La tasa de incorrectos es la misma en los dos ejemplos, pero la adecuación del modelo es muy diferente. 9.3 Análisis de error en clasificación binaria Cuando la variable a predecir es binaria (dos clases), podemos etiquetar una clase como positivo y otra como negativo. En el fondo no importa cómo catalogemos cada clase, pero para problemas particulares una asignación puede ser más natural. Por ejemplo, en diagnóstico de enfermedades, positivo=tiene la enfermedad, en análisis de crédito, positivo=cae en impago, en sistemas de recomendacion, positivo = le gusta el producto X, en recuperación de textos, positivo=el documento es relevante a la búsqueda, etc. Hay dos tipos de errores en un modelo logístico binario (positivo - negativo): Falsos positivos (fp): clasificar como positivo a un caso negativo. Falsos negativos (fn): clasificar como negativo a un caso positivo. A los casos clasificados correctamente les llamamos positivos verdaderos (pv) y negativos verdaderos (nv). La matriz de confusion es entonces positivos negativos total predicción posiriva pv fp pred.pos predicción negativa fn nv pred.neg total pos neg Nótese que un modelo bueno, en general, es uno que tiene la mayor parte de los casos en la diagonal de la matriz de confusión. Podemos estudiar nuestro modelo en términos de las proporciones de casos que caen en cada celda, que dependen del desempeño del modelo en cuanto a casos positivos y negativos. La nomenclatura es confusa, pues en distintas áreas se usan distintos nombres para estas proporciones: Tasa de falsos positivos \\[ \\frac{\\mbox{fp}}{\\mbox{fp}+\\mbox{nv}}=\\frac{\\mbox{fp}}{\\mbox{neg}} \\] Tasa de falsos negativos \\[ \\frac{\\mbox{fn}}{\\mbox{pv}+\\mbox{fn}}=\\frac{\\mbox{fn}}{\\mbox{pos}} \\] Especificidad \\[ \\frac{\\mbox{nv}}{\\mbox{fp}+\\mbox{nv}}=\\frac{\\mbox{nv}}{\\mbox{neg}} \\] Sensibilidad \\[ \\frac{\\mbox{pv}}{\\mbox{pv}+\\mbox{fn}}=\\frac{\\mbox{pv}}{\\mbox{pos}} \\] Y también otras que tienen como base las predicciones: Valor predictivo positivo o Precisión \\[ \\frac{\\mbox{vp}}{\\mbox{vp}+\\mbox{fp}}=\\frac{\\mbox{vp}}{\\mbox{pred.pos}} \\] Valor predictivo negativo \\[ \\frac{\\mbox{vn}}{\\mbox{fn}+\\mbox{vn}}=\\frac{\\mbox{vn}}{\\mbox{pred.neg}} \\] Dependiendo de el tema y el objetivo hay medidas más naturales que otras: En pruebas clínicas, se usa típicamente sensibilidad y especificidad (proporción de positivos que detectamos y proporción de negativos que descartamos). En búsqueda y recuperación de documentos (positivo=el documento es relevante, negativo=el documento no es relevante), se usa precisión y sensibilidad (precisión=de los documentos que entregamos (predicción positiva), cuáles son realmente positivos/relevantes, y sensibilidad=de todos los documentos relevantes, cuáles devolvemos). Aquí la tasa de falsos positivos (de todos los negativos, cuáles se predicen positivos), por ejemplo, no es de ayuda pues generalmente son bajas y no discriminan el desempeño de los modelos. La razón es que típicamente hay una gran cantidad de negativos, y se devuelven relativamente pocos documentos, de forma que la tasa de falsos positivos generalmente es muy pequeña. Ejercicio ¿Qué relaciones hay entre las cantidades mostradas arriba? Escribe la tasa de clasificación incorrecta en términos de especificidad y sensibilidad. También intenta escribir valor predictivo positivo y valor predictivo negativo en términos de sensibilidad y especificidad. Cada modelo tiene un balance distinto especificidad-sensibliidad. Muchas veces no escogemos modelos por la tasa de error solamente, sino que intentamos buscar un balance adecuado entre el comportamiento de clasificación para positivos y para negativos. Ejercicio Consideremos los datos Pima.tr del paqute MASS: diabetes &lt;- MASS::Pima.tr diabetes %&gt;% sample_n(10) %&gt;% knitr::kable() npreg glu bp skin bmi ped age type 17 1 109 60 8 25.4 0.947 21 No 167 0 151 90 46 42.1 0.371 21 Yes 119 1 136 74 50 37.4 0.399 24 No 31 1 79 60 42 43.5 0.678 23 No 2 7 195 70 33 25.1 0.163 55 Yes 91 1 79 75 30 32.0 0.396 22 No 97 4 110 76 20 28.4 0.118 27 No 56 4 127 88 11 34.5 0.598 28 No 141 1 167 74 17 23.4 0.447 33 Yes 148 0 177 60 29 34.6 1.072 21 Yes Calcula la matriz de confusión para el modelo logístico de diabetes en términos de glucosa. Calcula especificidad, sensibilidad, y precisión. mod_1 &lt;- glm(type ~ glu, data = diabetes, family = &#39;binomial&#39;) preds &lt;- mod_1$fitted.values 9.3.1 Punto de corte para un clasificador binario ¿Qué sucede cuando el perfil de sensibilidad y especificidad de unmodelo logístivco no es apropiado para nuestros fines? Recordemos que una vez que hemos estimado con \\(\\hat{p}_1(x)\\), nuestra regla de clasificación es: Predecir positivo si \\(\\hat{p}_1(x) &gt; 0.5\\), Predecir negativo si \\(\\hat{p}_1(x) \\leq 0.5.\\) Esto sugiere una regla alternativa: Para \\(0 &lt; d &lt; 1\\), podemos utilizar nuestras estimaciones \\(\\hat{p}_1(x)\\) para construir un modelo alternativo poniendo: Predecir positivo si \\(\\hat{p}_1(x) &gt; d\\), Predecir negativo si \\(\\hat{p}_1(x) \\leq d\\). Distintos valores de \\(d\\) dan distintos perfiles de sensibilidad-especificidad para una misma estimación de las probabilidades condicionales de clase: Para minimizar la tasa de incorrectos conviene poner \\(d = 0.5\\). Sin embargo, en ocasiones no es esto lo que se busca de un modelo de clasificación binaria. Cuando incrementamos d, quiere decir que exigimos estar más seguros de que un caso es positivo para clasificarlo como positivo. Eso quiere decir que la especifidad va a ser más grande (entre los negativos verdaderos va a haber menos falsos positivos). Sin embargo, la sensibilidad va a ser más chica pues captamos menos de los verdaderos positivos. Ejemplo Por ejemplo, si en el caso de diabetes incrementamos el punto de corte a \\(0.7\\): table(preds &gt; 0.7, diabetes$type) #&gt; #&gt; No Yes #&gt; FALSE 128 52 #&gt; TRUE 4 16 tab &lt;- prop.table(table(preds &gt; 0.7, diabetes$type),2) tab #&gt; #&gt; No Yes #&gt; FALSE 0.9697 0.7647 #&gt; TRUE 0.0303 0.2353 La especificidad ahora es 0.97, muy alta (descartamos muy bien casos negativos), pero la sensibilidad se deteriora a 0.24. Cuando hacemos más chica \\(d\\), entonces exigimos estar más seguros de que un caso es negativo para clasificarlo como negativo. Esto aumenta la sensibilidad, pero la especificidad baja. Por ejemplo, si en el caso de diabetes ponemos el punto de corte en 0.3: table(preds &gt; 0.3, diabetes$type) #&gt; #&gt; No Yes #&gt; FALSE 94 15 #&gt; TRUE 38 53 tab &lt;- prop.table(table(preds &gt; 0.3, diabetes$type),2) tab #&gt; #&gt; No Yes #&gt; FALSE 0.712 0.221 #&gt; TRUE 0.288 0.779 9.4 Curvas ROC 9.4.1 Espacio ROC Podemos visualizar el desempeño de cada uno de estos modelos con punto de corte mapeándolos a las coordenadas de tasa de falsos positivos (1-especificidad) y sensibilidad: clasif_1 &lt;- data.frame( corte = c(&#39;0.3&#39;,&#39;0.5&#39;,&#39;0.7&#39;,&#39;perfecto&#39;,&#39;azar&#39;), tasa_falsos_pos=c(0.24,0.08,0.02,0,0.7), sensibilidad =c(0.66, 0.46,0.19,1,0.7)) ggplot(clasif_1, aes(x=tasa_falsos_pos, y=sensibilidad, label=corte)) + geom_point() + geom_abline(intercept=0, slope=1) + xlim(c(0,1)) +ylim(c(0,1)) + geom_text(hjust=-0.3, col=&#39;red&#39;)+ xlab(&#39;1-especificidad (tasa falsos pos)&#39;) Nótese que agregamos otros dos modelos de clasificación, uno perfecto, que tiene tasa de falsos positivos igual a 0 y sensibilidad igual a 1. En esta gráfica, un modelo que esté más arriba a la izquierda domina a otro que esté más abajo a la derecha porque tiene mejor especificidad y mejor sensibilidad. Entre los puntos de corte \\(0.3\\), \\(0.5\\) y \\(0.7\\) de la gráfica, no hay ninguno que domine a otro. Todos los modelos en la diagonal son equivalentes a clasificar las observaciones al azar. ¿Por qué? La razón es que si cada vez que vemos un nuevo caso lo clasificamos como positivo con probabilidad \\(p\\) fija y arbitraria. Esto implica que cuando veamos un caso positivo, la probabilidad de ‘atinarle’ es de \\(p\\) (sensibilidad), y cuando vemos un negativo, la probabilidad de equivocarnos también es de \\(p\\) (tasa de falsos positivos). De modo que este modelo al azar está en la diagonal. ¿Qué podemos decir acerca de los modelos que caen por debajo de la diagonal? Estos son particularmente malos, pues existen los modelos al azar que tienen mayor sensibilidad y especificidad. Sin embargo, se puede construir un mejor modelo volteando las predicciones, lo que cambia sensibilidad por tasa de falsos positivos. ¿Cuál de los tres modelos es el mejor? En términos de la tasa de incorrectos, el de corte 0.5. Sin embargo, para otros propósitos puede ser razonable escoger alguno de los otros. En lugar de examinar cada punto de corte por separado, podemos hacer el análisis de todos los posibles puntos de corte mediante la curva ROC (receiver operating characteristic, de ingeniería). Para un problema de clasificación binaria, dadas estimaciones \\(\\hat{p}(x)\\), la curva ROC grafica todos los pares de (1-especificidad, sensibilidad) para cada posible punto de corte \\(\\hat{p}(x) &gt; d\\). Ejemplo Recordemos los datos de diabéticos. Modelamos el tipo de diabetes tomando como predictor el nivel de glucosa. Con el paquete tabplot podemos obtener la gráfica de abajo. library(tabplot) mod_1 &lt;- glm(type ~ glu, diabetes, family = &#39;binomial&#39;) diabetes$probs_1 &lt;- predict(mod_1, newdata = diabetes, type = &quot;response&quot;) head(arrange(diabetes, desc(probs_1))) #&gt; npreg glu bp skin bmi ped age type probs_1 #&gt; 1 1 199 76 43 42.9 1.394 22 Yes 0.882 #&gt; 2 0 198 66 32 41.3 0.502 28 Yes 0.878 #&gt; 3 2 197 70 99 34.7 0.575 62 Yes 0.874 #&gt; 4 7 195 70 33 25.1 0.163 55 Yes 0.866 #&gt; 5 7 194 68 28 35.9 0.745 41 Yes 0.861 #&gt; 6 1 193 50 16 25.9 0.655 24 No 0.857 tableplot(diabetes, sortCol = probs_1) La columna de probabilidad de la derecha nos dice en qué valores podemos cortar para obtener distintos modelos. Nótese que si cortamos más arriba, se nos escapan más positivos verdaderos que clasificamos como negativos, pero clasificamos a más negativos verdaderos como negativos. Lo opuesto ocurre cuando cortamos más abajo. Vamos a graficar todos los pares (1-especificidad, sensibilidad) para cada punto de corte \\(d\\) de estas probabilidades. library(ROCR) pred_rocr &lt;- prediction(diabetes$probs_1, diabetes$type) perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) graf_roc_1 &lt;- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], d = perf@alpha.values[[1]]) ggplot(graf_roc_1, aes(x = tfp, y = sens, colour=d)) + geom_point() + xlab(&#39;1-especificidad&#39;) + ylab(&#39;Sensibilidad&#39;) En esta gráfica podemos ver todos los modelos posibles basados en las probabilidades de clase. Dejamos para más tarde la selección del punto de corte. También podemos definir una medida resumen del desempeño de un modelo según esta curva: La medida AUC (area under the curve) para un modelo es el área bajo la curva generada por los pares (sensibilidad, 1-especificidad) de la curva ROC. auc_1 &lt;- performance(pred_rocr, measure = &#39;auc&#39;)@y.values auc_1 #&gt; [[1]] #&gt; [1] 0.789 También es útil para comparar modelos. Consideremos el modelo de los datos de diabetes que incluyen todas las variables: mod_2 &lt;- glm(type ~ ., diabetes, family = &#39;binomial&#39;) diabetes$probs_2 &lt;- predict(mod_2, newdata = diabetes, type = &quot;response&quot;) head(arrange(diabetes, desc(probs_2))) #&gt; npreg glu bp skin bmi ped age type probs_1 probs_2 #&gt; 1 0 137 40 35 43.1 2.288 33 Yes 0.419 0.976 #&gt; 2 1 199 76 43 42.9 1.394 22 Yes 0.882 0.972 #&gt; 3 7 194 68 28 35.9 0.745 41 Yes 0.861 0.948 #&gt; 4 2 197 70 99 34.7 0.575 62 Yes 0.874 0.944 #&gt; 5 10 148 84 48 37.6 1.001 51 Yes 0.522 0.932 #&gt; 6 8 181 68 36 30.1 0.615 60 Yes 0.792 0.918 tableplot(diabetes, sortCol = probs_2) Y graficamos juntas: library(ROCR) pred_rocr &lt;- prediction(diabetes$probs_2, diabetes$type) perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) auc_2 &lt;- performance(pred_rocr, measure = &quot;auc&quot;)@y.values graf_roc_2 &lt;- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], d = perf@alpha.values[[1]]) graf_roc_2$modelo &lt;- &#39;Todas las variables&#39; graf_roc_1$modelo &lt;- &#39;Solo glucosa&#39; graf_roc &lt;- bind_rows(graf_roc_1, graf_roc_2) ggplot(graf_roc, aes(x = tfp, y = sens, colour = modelo)) + geom_point() + xlab(&#39;1-especificidad&#39;) + ylab(&#39;Sensibilidad&#39;) Comparación auc: auc_1 #&gt; [[1]] #&gt; [1] 0.789 auc_2 #&gt; [[1]] #&gt; [1] 0.851 En este ejemplo, vemos que casi no importa que perfil de especificidad y sensibilidad busquemos: el modelo que usa todas las variables domina casi siempre al modelo que sólo utiliza las variables de glucosa. La razón es que para cualquier punto de corte (con sensibilidad menor a 0.4) en el modelo de una variable, existe otro modelo en la curva roja (todas las variable), que domina al primero. 9.5 Regularización En primer lugar, supondremos que tenemos un problema con \\(n=400\\) y \\(p=100\\), y tomamos como modelo para los datos (sin ordenada al origen): \\[ p_1(x)=h\\left(\\sum_{j=1}^{100} \\beta_j x_j\\right ), \\] donde \\(h\\) es la función logística. Nótese que este es el verdadero modelo para los datos. Para simular datos, primero generamos las betas fijas, y después, utilizando estas betas, generamos 400 observaciones. Generamos las betas: h &lt;- function(x){ 1 / (1 + exp(-x))} set.seed(2805) beta &lt;- rnorm(100,0,0.1) names(beta) &lt;- paste0(&#39;V&#39;, 1:length(beta)) head(beta) #&gt; V1 V2 V3 V4 V5 V6 #&gt; -0.11988 0.03463 -0.08182 0.01492 0.04016 0.00204 Con esta función simulamos 400 observaciones. sim_datos &lt;- function(n, m, beta){ p &lt;- length(beta) #n = observaciones, p=num variables mat &lt;- matrix(rnorm(n*p, 0, 0.5), n, p) + rnorm(n) prob &lt;- h(mat %*% beta) y &lt;- rbinom(n, 1, prob) dat &lt;- as.data.frame(mat) dat$y &lt;- y dat } set.seed(9921) datos &lt;- sim_datos(n = 400, beta = beta) Y ahora ajustamos el modelo de regresión logística: mod_1 &lt;- glm(y ~ -1 + ., datos, family = &#39;binomial&#39;) ¿Qué tan buenas fueron nuestras estimaciones? qplot(beta, mod_1$coefficients) + xlab(&#39;Coeficientes&#39;) + ylab(&#39;Coeficientes estimados&#39;) + geom_abline(intercept=0, slope =1) + xlim(c(-1.5,1.5))+ ylim(c(-1.5,1.5)) Y notamos que las estimaciones no son muy buenas. Podemos hacer otra simulación para confirmar que el problema es que las estimaciones son muy variables. Con otra muestra, vemos que las estimaciones tienen varianza alta. datos_2 &lt;- sim_datos(n = 400, beta = beta) mod_2 &lt;- glm(y ~ -1 + ., datos_2, family = &#39;binomial&#39;) qplot(mod_1$coefficients, mod_2$coefficients) + xlab(&#39;Coeficientes mod 1&#39;) + ylab(&#39;Coeficientes mod 2&#39;) + geom_abline(intercept=0, slope =1) + xlim(c(-1.5,1.5))+ ylim(c(-1.5,1.5)) Si repetimos varias veces: dat_sim &lt;- map_df(.x = 1:50, .f = function(i, beta){ salida &lt;- sim_datos(n=400, beta=beta) mod &lt;- glm(y ~ -1 + ., salida, family = &quot;binomial&quot;) tibble(rep = i, vars = names(coef(mod)), coefs = coef(mod)) }, beta = beta) head(dat_sim) #&gt; # A tibble: 6 x 3 #&gt; rep vars coefs #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 V1 -0.0789 #&gt; 2 1 V2 0.142 #&gt; 3 1 V3 0.109 #&gt; 4 1 V4 0.140 #&gt; 5 1 V5 0.926 #&gt; 6 1 V6 0.120 Vemos que hay mucha variabilidad en la estimación de los coeficientes (en rojo están los verdaderos): dat_sim &lt;- dat_sim %&gt;% mutate(vars = reorder(vars, coefs, mean)) ggplot(dat_sim, aes(x=vars, y=coefs)) + geom_boxplot() + geom_line(data=data_frame(coefs=beta, vars=names(beta)), aes(y=beta, group=1), col=&#39;red&#39;,size=1.1) + coord_flip() En la práctica, nosotros tenemos una sola muestra de entrenamiento. Así que, con una muestra de tamaño \\(n=500\\) como en este ejemplo, obtendremos típicamente resultados no muy buenos. Estos coeficientes ruidosos afectan nuestras predicciones. Vemos ahora lo que pasa con nuestra \\(\\hat{p}_1(x)\\) estimadas, comparándolas con \\(p_1(x)\\), para la primera simulación: x &lt;- datos %&gt;% select(-y) %&gt;% as.matrix ps &lt;- data_frame(prob_hat_1 = h(mod_1$fitted.values), prob_1 = as.numeric(h(x%*% beta)), clase = datos$y) head(ps) #&gt; # A tibble: 6 x 3 #&gt; prob_hat_1 prob_1 clase #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.530 0.372 0 #&gt; 2 0.524 0.170 0 #&gt; 3 0.545 0.349 0 #&gt; 4 0.534 0.380 0 #&gt; 5 0.653 0.432 0 #&gt; 6 0.727 0.913 1 ggplot(ps, aes(x=prob_1, y=prob_hat_1, colour=factor(clase))) + geom_point() Si la estimación fuera perfecta, esta gráfica sería una diagonal. Vemos entonces que cometemos errores grandes. El problema no es que nuestro modelo no sea apropiado (logístico), pues ése es el modelo real. El problema es la variabilidad en la estimación de los coeficientes que notamos arriba. 9.5.1 Reduciendo varianza de los coeficientes Como el problema es la varianza, podemos atacar este problema poniendo restricciones a los coeficientes, de manera que caigan en rangos más aceptables. Una manera de hacer esto es sustituir el problema de minimización de regresión logística, que es minimizar la devianza: \\[ \\min_{\\beta} D(\\beta) \\] con un problema penalizado \\[ \\min_{\\beta} D(\\beta) + \\lambda\\sum_{i=1}^p \\beta_j^2 \\] escogiendo un valor apropiado de \\(\\lambda\\). También es posible poner restricciones sobre el tamaño de \\(\\sum_{i=1}^p \\beta_j^2\\), lo cual es equivalente al problema de penalización. En este caso obtenemos (veremos más del paquete glmnet): library(glmnet) mod_restringido &lt;- glmnet(x = x, y = datos$y, alpha = 0, family=&#39;binomial&#39;, intercept = F, lambda = 0.1) beta_penalizado &lt;- coef(mod_restringido)[-1] # quitar intercept Y podemos ver que el tamaño de los coeficientes se redujo considerablemente: sum(beta_penalizado^2) #&gt; [1] 0.465 sum(coef(mod_1)^2) #&gt; [1] 24.5 Los nuevos coeficientes estimados: qplot(beta, beta_penalizado) + xlab(&#39;Coeficientes&#39;) + ylab(&#39;Coeficientes estimados&#39;) + geom_abline(xintercept=0, slope =1) + xlim(c(-0.5,0.5))+ ylim(c(-0.5,0.5)) #&gt; Warning: Ignoring unknown parameters: xintercept ps$prob_hat_pen &lt;- h(x %*% as.numeric(beta_penalizado)) ggplot(ps, aes(x=prob_1, y=prob_hat_pen, colour=factor(clase))) + geom_point() tab &lt;- table(ps$prob_hat_pen &gt; 0.5, ps$clase) prop.table(tab, margin=2) #&gt; #&gt; 0 1 #&gt; FALSE 0.731 0.262 #&gt; TRUE 0.269 0.738 Comparamos con la tabla de confusión sin penalizar: tab &lt;- table(ps$prob_hat_1 &gt; 0.5, ps$clase) tab #&gt; #&gt; 0 1 #&gt; TRUE 186 214 9.6 Regularización Ridge Arriba vimos un ejemplo de regresión penalizada tipo Ridge. Recordemos que en regresión logística buscamos minimizar, \\[ D(\\beta)=-2\\sum_{i=1}^n y_i \\log\\left(h(x_i^\\prime \\beta)\\right) + (1-y_i) \\log\\left(1-h(x_i^\\prime \\beta)\\right). \\] En regresión ridge, para \\(\\lambda&gt;0\\) fija minimizamos \\[D_{\\lambda}^{ridge} (\\beta)=D(\\beta) + \\lambda\\sum_{i=1}^p \\beta_j^2\\], donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). Observaciones La idea de regresión penalizada consiste en estabilizar la estimación de los coeficientes, especialmente en casos donde tenemos muchas variables en relación al número de observaciones. La penalización no permite que varíen tan fuertemente los coeficientes. Cuando \\(\\lambda\\) es mas grande, los coeficientes se encogen más fuertemente hacia cero con respecto al problema no regularizado. En este caso, estamos reduciendo la varianza pero potencialmente incrementando el sesgo. Cuando \\(\\lambda\\) es mas chico, los coeficientes se encogen menos fuertemente hacia cero, y quedan más cercanos a los coeficientes de mínimos cuadrados/máxima verosimilitud. En este caso, estamos reduciendo el sesgo pero incrementando la varianza. Nótese que no penalizamos \\(\\beta_0\\). Es posible hacerlo, pero típicamente no lo hacemos. Recordemos que en regresión logística, la probabilidad ajustada cuando las entradas toman su valor en la media es igual a \\(h(\\beta_0)\\). Que las variables estén estandarizadas es importante para que tenga sentido la penalización. Si las variables \\(x_j\\) están en distintas escalas (por ejemplo pesos y dólares), entonces también los coeficientes \\(\\beta_j\\) están en distintas escalas, y una penalización fija no afecta de la misma forma a cada coeficiente. Resolver este problema por descenso en gradiente no tiene dificultad, pues: \\[ \\frac{\\partial D_{\\lambda}^{ridge} (\\beta)}{\\partial\\beta_j} = \\frac{\\partial D(\\beta)}{\\beta_j} + 2\\lambda\\beta_j \\] para \\(j=1,\\ldots, p\\), y \\[ \\frac{\\partial D_{\\lambda}^{ridge} (\\beta)}{\\partial\\beta_0} = \\frac{\\partial D(\\beta)}{\\beta_0}. \\] De forma que sólo hay que hacer una modificaciónmínima al algoritmo de descenso en gradiente para el caso no regularizado. 9.6.1 Selección de coeficiente de regularización Seleccionamos \\(\\lambda\\) para minimizar el error de predicción, es decir, para mejorar nuestro modelo ajustado en cuanto a sus predicciones. No tiene sentido intentar escoger \\(\\lambda&gt;0\\) usando la tasa de error. La razón es que siempre que aumentamos \\(\\lambda\\), obtenemos un valor mayor de la devianza del modelo, pues \\(\\lambda\\) más grande implica que pesa menos la minimización de la devianza en el problema de la minimización. En otras palabras, los coeficientes tienen una penalización más fuerte, de modo que el mínimo que se alcanza es mayor en términos de devianza. Intentamos escoger \\(\\lambda\\) de forma que se minimice el error de predicción. 9.7 Regularización Lasso Otra forma de regularización es Lasso, que en lugar de penalizar con la suma de cuadrados en los coeficientes, penaliza por la suma de su valor absoluto. En regresión Lasso para \\(\\lambda&gt;0\\) fija minimizamos \\[ D_{\\lambda}^2 (\\beta)=D(\\beta) + \\lambda\\sum_{i=1}^p |\\beta_j|, \\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). El problema de minimización de ridge y de Lasso se pueden reescribir como problemas de restricción: En regresión Lasso para \\(s&gt;0\\) fija minimizamos \\[D(\\beta),\\] sujeto a \\[\\sum_{i=1}^p |\\beta_j|&lt; s\\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). En regresión ridge para \\(t&gt;0\\) fija minimizamos \\[D(\\beta),\\] sujeto a \\[\\sum_{i=1}^p \\beta_j^2 &lt; t\\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). \\(s\\) y \\(t\\) chicas corresponden a valores de penalización \\(\\lambda\\) grandes. En un principio, puede parecer que ridge y Lasso deben dar resultados muy similares, pues en ambos casos penalizamos por el tamaño de los coeficientes. Sin embargo, son distintos de una manera muy importante. En la siguiente gráfica regresentamos las curvas de nivel de \\(D(\\beta)\\). Recordemos que en regresión logística intentamos minimizar esta cantidad sin restricciones, y este mínimo se encuentra en el centro de estas curvas de nivel. Para el problema restringido, buscamos más bien la curva de nivel más baja que intersecta la restricción: knitr::include_graphics(&#39;./figuras/ridge_lasso.png&#39;) Y obsérvese ahora que la solución de Lasso puede hacer algunos coeficientes igual a 0. Es decir, En regresión ridge, los coeficientes se encogen gradualmente desde la solución no restringida hasta el origen. Ridge es un método de encogimiento de coeficientes. En regresión Lasso, los coeficientes se encogen gradualmente, pero también se excluyen variables del modelo. Por eso Lasso es un método de encogimiento y selección de variables. Regresión ridge es especialmente útil cuando tenemos varias variables de entrada fuertemente correlacionadas. Regresión ridge intenta encoger juntos coeficientes de variables correlacionadas para reducir varianza en las predicciones. Lasso encoge igualmente coeficientes para reducir varianza, pero también comparte similitudes con regresión de mejor subconjunto, en donde para cada número de variables \\(l\\) buscamos escoger las \\(l\\) variables que den el mejor modelo. Sin embargo, el enfoque de Lasso es más escalable y puede calcularse de manera más simple. Descenso en gradiente no es apropiado para regresión Lasso (ver documentación de glmnet para ver cómo se hace en este paquete). El problema es que los coeficientes nunca se hacen exactamente cero, pues la restricción no es diferenciable en el origen (coeficientes igual a cero). 9.8 Tarea Consideramos datos para detección de spam en e-mail de spambase. library(tidyverse) spam &lt;- read_csv(&#39;datos/spam.csv&#39;) Las variables de entrada son extraídas de emails (los textos de emails fueron procesados para obtener estas entradas). Son frecuencias de ocurrencia de palabras (por ejemplo, wffree, wfremove, wfmail son frecuencias de las palabras free, remove, mail, etc.), y otras entradas cuentan aparición de ciertos caracteres (cfdollar, cfexc son frecuencias de caracteres signo de dólar y signo de exclamación). Queremos predecir con estas entradas si un mail es spam o no table(spam$spam) #&gt; #&gt; 0 1 #&gt; 2788 1813 Utiliza el método de regresión logística para hacer la estimación. Construye un modelo solamente usando las variables de caracteres (cfsc, cfpar, etc). Calcula la curva ROC. Construye un modelo utilizando todas las variables. Calcula la curva ROC. Haz gráficas de las curvas ROC de los dos modelos anteriores. ¿Qué modelo es superior? Discute un punto de corte apropiado para hacer un filtro de spam. ¿Escogerías especificidad más alta o sensibilidad más alta? Explica discutiendo los costos de cada tipo de error (falso positivo o falso negativo). Escoge el punto de corte y muestra la matriz de confusión correspondiente. Repite el ejercicio de spam (con todas las variables), y utiliza regresión logística ridge (glmnet). Escoge un parámetro de regularización y recalcula la matriz de confusión. ¿Obtuviste ganancias en clasificación? Checa los nuevos coeficientes y compara con los que obtuviste usando regresión logística sin regularización. Nota: para obtener coeficientes estandarizados con glmnet debes estandarizar los datos a mano antes de correr glmnet. "],
["modelos-lineales-generalizados.html", "Clase 10 Modelos lineales generalizados 10.1 Regresión lineal y logística 10.2 Otros modelos 10.3 Ejemplo: accidentes de tráfico 10.4 Interpretación de coeficientes Poisson 10.5 Diferencias entre el modelo binomial y Poisson 10.6 Ejemplo: fertilidad en Fiji 10.7 Variable de expuestos (offset) 10.8 Ejemplos: seguros 10.9 Ejemplo: árboles 10.10 Sobredispersión 10.11 Ejemplo: número de publicaciones 10.12 Tarea", " Clase 10 Modelos lineales generalizados .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 10.1 Regresión lineal y logística Los modelos lineales generalizados son una familia de modelos para el análisis estadístico. Incluye la regresión lineal y logística como casos especiales. La regresión lineal predice directamente datos continuos \\(y\\) de un predictor lineal \\(X\\beta = \\beta_0 + X_1\\beta_1 + \\cdots + X_k\\beta_k.\\) La regresión logística predice \\(P(y=1)\\) para datos binarios a partir de un predictor lineal transformado por la función logística inversa. Un modelo lineal generalizado consiste de: Un vector de datos \\(y=(y_1,\\ldots,y_n)\\) Predictores \\(X\\) y coeficientes \\(\\beta\\) para construir un predictor lineal \\(X\\beta\\). Una función liga \\(g\\) que da como resultado datos transformados \\[\\hat{y}=g^{-1}(X\\beta)\\] que son usados para modelar los datos. Una distribución para los datos \\(p(y|\\hat{y})\\). Posiblemente otros parámetros, como varianza, o puntos de corte, involucrados en los predictores, o bien, la función liga o la distribución de los datos. ¿Qué función liga se utiliza en regresión lineal? \\(g(x) = X\\beta\\). \\(g(x)=\\alpha + \\beta x\\). \\(g(x)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\,e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\\). \\(g(x)=x\\). ¿Qué función liga se utiliza en regresión logística? \\(g(x) = \\dfrac{e^x}{1-e^x}\\). \\(g(x)= \\mbox{logit}^{-1}(x)\\). \\(g(x)=\\left(\\dfrac{1}{1+e^{-x}}\\right)^{-1}\\). \\(g(x)=x\\). 10.2 Otros modelos Otros modelos que vamos a ver después son: El modelo Poisson se utiliza para datos de conteos; es decir, donde cada dato observado \\(y_i\\) puede ser igual a \\(0, 1, 2,\\dots\\). La función liga que se utiliza habitualmente \\(g\\) es logarítmica, de modo que \\(g(x) = \\mbox{exp}(x)\\) transforma un predictor lineal continuo \\(X_i\\beta\\) en un \\(y_i\\) positivo. La distribución de datos es Poisson. A veces es buena idea agregar un parámetro a este modelo para capturar la sobredispersión, es decir, la variación en los datos más allá de la que captura el modelo. El modelo logístico-binomial se utiliza en casos cuando los datos observados \\(y_i\\) representan el número de éxitos en \\(n_i\\) ensayos independientes. En este modelo la función liga es \\(\\mbox{logit}\\) y la distribución de los datos es binomial. Al igual que con la regresión Poisson, el modelo binomial típicamente se puede mejorar agregando un parámetro de sobredispersión. El modelo probit es igual que regresión logística pero se reemplaza la función liga por la distribución normal acumulada. Se puede pensar como usar la distribución normal en los errores estimados del modelo. Nota: Podemos usar glm() directamente para ajustar regresiones logísticas-binomiales, probit y Poisson, entre otras, y para corregir la sobredispersión cuando sea necesario. 10.3 Ejemplo: accidentes de tráfico En el modelo de Poisson, cada observación \\(i\\) corresponde a una situación (típicamente una ubicación espacial o un intervalo de tiempo) en la que se observan eventos \\(y_i\\). Por ejemplo, con \\(i\\) se puede representar esquinas de calles en una ciudad y \\(y_i\\) podría ser el número de accidentes de tráfico en la \\(i\\)-ésima esquina en un año determinado. Al igual que con la regresión lineal y logística, la variación en \\(y\\) puede explicarse con predictores lineales \\(X\\). En el ejemplo de accidentes de tráfico, estos predictores podrían incluir: un término constante, una medida de la velocidad promedio del tráfico cerca de la esquina y un indicador para si la esquina tiene una señal de tráfico. El modelo básico de regresión de Poisson tiene la forma \\[ y_i \\sim \\mbox{Poisson}(\\theta_i). \\] El parámetro \\(\\theta_i\\) debe ser positivo, por lo que tiene sentido ajustar un predictor lineal en una escala logarítmica: \\[ \\theta_i = \\exp(X_i\\beta). \\] 10.4 Interpretación de coeficientes Poisson Los coeficientes \\(\\beta\\) pueden exponenciarse y tratarse como efectos multiplicativos. Por ejemplo, supongamos que el modelo de accidentes de tráfico es \\[ y_i ∼ \\mbox{Poisson}(\\exp(2.8 + 0.012X_{i1} − 0.20X_{i2})), \\] donde \\(X_{i1}\\) es la velocidad promedio (en millas por hora) en las calles cercanas y \\(X_{i2} = 1\\) si la esquina (o intersección) tiene una señal de tráfico y \\(0\\) en caso contrario. Entonces podemos interpretar cada coeficiente de la siguiente manera: El término constante es el intercepto, es decir, la predicción de si \\(X_{i1} = 0\\) y \\(X_{i2} = 0\\). Como esto no es posible (ninguna calle tendrá una velocidad promedio de 0), entonces no intentaremos interpretar el término constante. El coeficiente de \\(X_{i1}\\) es la diferencia esperada en \\(y\\) (en la escala logarítmica) para cada milla por hora adicional de velocidad de tráfico. Por lo tanto, el aumento multiplicativo esperado es \\(e^{0.012} = 1.012\\), o una diferencia positiva de 1.2% en la tasa de accidentes de tráfico por milla por hora. Dado que la velocidad del tráfico varía en decenas de milla por hora, en realidad tendría sentido definir \\(X_{i1}\\) como velocidad en decenas de milla por hora, en cuyo caso su coeficiente sería 0.12, que corresponde a un aumento del 12% (más precisamente, \\(e^{0.12} = 1.127\\): a 12.7% de aumento) en la tasa de accidentes por diez milla por hora. El coeficiente de \\(X_{i2}\\) nos dice que la diferencia predictiva de tener una señal de tráfico se puede encontrar multiplicando la tasa de accidentes por \\(\\exp(-0.20) = 0.82\\) produciendo una reducción del 18%. Al igual que con los modelos de regresión en general, cada coeficiente se interpreta como una comparación en el que un predictor difiere en una unidad, mientras que todos los demás predictores permanecen constantes, lo cual no es necesariamente el supuesto más apropiado. Por ejemplo, no se debería esperar necesariamente que la instalación de señales de tráfico en todas las esquinas de la ciudad reduzca los accidentes en un 18%. 10.5 Diferencias entre el modelo binomial y Poisson El modelo de Poisson es similar al modelo binomial para conteos pero se aplica en situaciones ligeramente diferentes: Si cada observación \\(y_i\\) se puede interpretar como el número de “éxitos” de \\(n_i\\) experimentos aleatorios, entonces es común usar el modelo Binomial-logístico. Si cada observación \\(y_i\\) no tiene un límite superior natural (no se basa en un número determinado de ensayos independientes) entonces es común usar el modelo Poisson-logarítmico. 10.6 Ejemplo: fertilidad en Fiji La siguiente tabla adaptada de Little (1978) (RJ 1978):, proviene de la Encuesta de Fertilidad de Fiji, publicada en los informes de World Fertility Survey. La tabla muestra datos sobre el número de hijos nacidos de mujeres casadas de raza de indios nativos clasificados por duración desde su primer matrimonio (agrupados en seis categorías), tipo de lugar de residencia (Suva, otro urbano y rural) y nivel educativo (cuatro categorías: ninguno, primaria inferior, primaria superior y secundaria o superior). Cada casilla de la tabla muestra la media, la varianza y el número de observaciones. Marr. Suva Otro urbano Rural Dur. N LP UP S\\(+\\) N LP UP S\\(+\\) N LP UP S\\(+\\) 0–4 0.50 1.14 0.90 0.73 1.17 0.85 1.05 0.69 0.97 0.96 0.97 0.74 1.14 0.73 0.67 0.48 1.06 1.59 0.73 0.54 0.88 0.81 0.80 0.59 8 21 42 51 12 27 39 51 62 102 107 47 5–9 3.10 2.67 2.04 1.73 4.54 2.65 2.68 2.29 2.44 2.71 2.47 2.24 1.66 0.99 1.87 0.68 3.44 1.51 0.97 0.81 1.93 1.36 1.30 1.19 10 30 24 22 13 37 44 21 70 117 81 21 10–14 4.08 3.67 2.90 2.00 4.17 3.33 3.62 3.33 4.14 4.14 3.94 3.33 1.72 2.31 1.57 1.82 2.97 2.99 1.96 1.52 3.52 3.31 3.28 2.50 12 27 20 12 18 43 29 15 88 132 50 9 15–19 4.21 4.94 3.15 2.75 4.70 5.36 4.60 3.80 5.06 5.59 4.50 2.00 2.03 1.46 0.81 0.92 7.40 2.97 3.83 0.70 4.91 3.23 3.29 – 14 31 13 4 23 42 20 5 114 86 30 1 20–24 5.62 5.06 3.92 2.60 5.36 5.88 5.00 5.33 6.46 6.34 5.74 2.50 4.15 4.64 4.08 4.30 7.19 4.44 4.33 0.33 8.20 5.72 5.20 0.50 21 18 12 5 22 25 13 3 117 68 23 2 25–29 6.60 6.74 5.38 2.00 6.52 7.51 7.54 – 7.48 7.81 5.80 – 12.40 11.66 4.27 – 11.45 10.53 12.60 – 11.34 7.57 7.07 – 47 27 8 1 46 45 13 – 195 59 10 – En nuestro análisis, trataremos el número de hijos nacidos de cada mujer como la variable respuesta, y la duración de su matrimonio, el tipo de lugar de residencia y el nivel educativo como predictores. Consideremos: Las unidades \\(i\\) son mujeres en un lugar de residencia, para un nivel educativo, y una duración de matrimonio dados La respuesta \\(y_i\\) es el número de nacimientos de mujeres en dicho grupo Los predictores son la duración de su matrimonio, el tipo de lugar de residencia y el nivel educativo. Ajustamos el modelo únicamente con el intercepto: library(tidyverse) ceb &lt;- read_csv(&quot;datos/ceb.csv&quot;) ceb$educ &lt;- ordered(ceb$educ, levels=c(&quot;none&quot;,&quot;lower&quot;,&quot;sec+&quot;,&quot;upper&quot;)) ceb$dur &lt;- ordered(ceb$dur, levels=c(&quot;0-4&quot;,&quot;9-May&quot;,&quot;14-Oct&quot;,&quot;15-19&quot;,&quot;20-24&quot;,&quot;25-29&quot;), labels=c(&quot;0-4&quot;, &quot;5-9&quot;, &quot;10-14&quot;,&quot;15-19&quot;,&quot;20-24&quot;,&quot;25-29&quot;)) ceb$res &lt;- ordered(ceb$res, levels=c(&quot;rural&quot;, &quot;urban&quot;, &quot;Suva&quot;)) mod_1 &lt;- glm(formula = n ~ 1, family=poisson, data = ceb) summary(mod_1) #&gt; #&gt; Call: #&gt; glm(formula = n ~ 1, family = poisson, data = ceb) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.20 -4.74 -2.38 1.37 17.94 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 3.6440 0.0193 189 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 2059.5 on 69 degrees of freedom #&gt; Residual deviance: 2059.5 on 69 degrees of freedom #&gt; AIC: 2414 #&gt; #&gt; Number of Fisher Scoring iterations: 5 mod_11 &lt;- glm(formula = n ~ res + educ + dur, family=poisson, data = ceb) summary(mod_11) #&gt; #&gt; Call: #&gt; glm(formula = n ~ res + educ + dur, family = poisson, data = ceb) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.332 -2.688 -0.175 1.429 8.685 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 3.38686 0.02442 138.70 &lt; 2e-16 *** #&gt; res.L -0.86262 0.03684 -23.41 &lt; 2e-16 *** #&gt; res.Q 0.29008 0.03940 7.36 1.8e-13 *** #&gt; educ.L -0.53272 0.03905 -13.64 &lt; 2e-16 *** #&gt; educ.Q 0.27420 0.04382 6.26 3.9e-10 *** #&gt; educ.C 0.62796 0.04813 13.05 &lt; 2e-16 *** #&gt; dur.L -0.24192 0.04642 -5.21 1.9e-07 *** #&gt; dur.Q 0.27477 0.04659 5.90 3.7e-09 *** #&gt; dur.C 0.21025 0.04866 4.32 1.6e-05 *** #&gt; dur^4 0.13639 0.04963 2.75 0.006 ** #&gt; dur^5 0.00868 0.04928 0.18 0.860 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 2059.53 on 69 degrees of freedom #&gt; Residual deviance: 787.98 on 59 degrees of freedom #&gt; AIC: 1162 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Ahora agregamos primero la variable de lugar de residencia: ceb$res &lt;- C(ceb$res, treatment) # modelo con grupo control mod_2 &lt;- glm(formula = n ~ res, family=poisson, data = ceb, contrasts = NULL) summary(mod_2) #&gt; #&gt; Call: #&gt; glm(formula = n ~ res, family = poisson, data = ceb, contrasts = NULL) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -11.31 -2.87 -0.36 2.25 12.35 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 4.2366 0.0251 169.0 &lt;2e-16 *** #&gt; resurban -0.9652 0.0477 -20.2 &lt;2e-16 *** #&gt; resSuva -1.2409 0.0521 -23.8 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 2059.5 on 69 degrees of freedom #&gt; Residual deviance: 1247.5 on 67 degrees of freedom #&gt; AIC: 1605 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Vemos que la devianza disminuye significativamente. Ahora agregamos los otros predictores y evaluamos el modelo nuevamente: ceb$dur &lt;- forcats::fct_rev(ceb$dur) ceb$dur &lt;- C(ceb$dur,treatment) ceb$educ &lt;- C(ceb$educ,treatment) mod_3 &lt;- glm(formula = n ~ res + dur + educ, family=poisson, data = ceb) summary(mod_3) #&gt; #&gt; Call: #&gt; glm(formula = n ~ res + dur + educ, family = poisson, data = ceb) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.332 -2.688 -0.175 1.429 8.685 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 4.5793 0.0565 81.03 &lt; 2e-16 *** #&gt; resurban -0.9652 0.0477 -20.22 &lt; 2e-16 *** #&gt; resSuva -1.2199 0.0521 -23.41 &lt; 2e-16 *** #&gt; dur20-24 -0.4165 0.0728 -5.72 1.0e-08 *** #&gt; dur15-19 -0.2645 0.0698 -3.79 0.00015 *** #&gt; dur10-14 -0.0922 0.0667 -1.38 0.16689 #&gt; dur5-9 -0.0181 0.0655 -0.28 0.78204 #&gt; dur0-4 0.1313 0.0633 2.07 0.03814 * #&gt; educlower 0.0492 0.0468 1.05 0.29275 #&gt; educsec+ -1.0315 0.0699 -14.76 &lt; 2e-16 *** #&gt; educupper -0.4339 0.0534 -8.13 4.4e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 2059.53 on 69 degrees of freedom #&gt; Residual deviance: 787.98 on 59 degrees of freedom #&gt; AIC: 1162 #&gt; #&gt; Number of Fisher Scoring iterations: 5 La devianza disminuyó significativamente. Vemos cómo se comportan las predicciones: preds &lt;- predict(mod_3, ceb) #&gt; Warning: contrasts dropped from factor res #&gt; Warning: contrasts dropped from factor dur #&gt; Warning: contrasts dropped from factor educ ceb$pred &lt;- preds ceb_larga &lt;- ceb %&gt;% gather(clase, valor, n, pred) ggplot(ceb_larga, aes(x = dur, y = valor/sum(ceb$n), colour = clase)) + geom_point() + facet_grid(educ ~ res) + theme(axis.text.x = element_text(angle = 25)) 10.7 Variable de expuestos (offset) En la mayoría de las aplicaciones de regresión de Poisson, los conteos pueden interpretarse relativos a un número, por ejemplo, el número de vehículos que cruzan la esquina. En el modelo general de regresión de Poisson, pensamos en \\(y_i\\) como el número de casos en un proceso con tasa \\(\\theta_i\\) y expuestos \\(u_i\\): \\[ y_i \\sim \\mbox{Poisson}(u_i\\theta_i), \\] donde, como antes, \\(\\theta_i = \\exp(X_i\\beta)\\). El logaritmo de expuestos, \\(\\log(u_i)\\), se denomina offset (o desplazamiento) en la terminología del modelo lineal generalizado. Observaciones: Los coeficientes de regresión \\(\\beta\\) resumen las asociaciones entre los predictores y \\(θ_i\\) (en nuestro ejemplo, la tasa de accidentes de tráfico por vehículo). Poner el logaritmo de expuestos en el modelo es equivalente a incluirlo como un predictor de regresión, pero con coeficiente fijado en \\(1\\). Otra opción es incluirlo como un predictor y permitir que su coeficiente sea estimado a partir de los datos, pero a veces es más simple mantenerlo como un desplazamiento para que la tasa estimada \\(\\theta\\) tenga una interpretación más directa. 10.8 Ejemplos: seguros Los datos Insurance en el paquete MASS consisten del número de asegurados de una compañía de seguros de autos que estuvieron expuestos a riesgo por accidente y el número de reclamos realizados por los asegurados en el tercer trimestre de 1973. library(MASS) data(Insurance) Insurance %&gt;% head %&gt;% knitr::kable() District Group Age Holders Claims 1 &lt;1l &lt;25 197 38 1 &lt;1l 25-29 264 35 1 &lt;1l 30-35 246 20 1 &lt;1l &gt;35 1680 156 1 1-1.5l &lt;25 284 63 1 1-1.5l 25-29 536 84 Las variables son: Distrito. residencia del que tiene la póliza 1 a 4 (ciudades importantes) Grupo. tipo de coche: &lt;1 litro, 1-1.5 litros, 1.5-2 litros, &gt;2 litros Edad. grupo de edad: &lt;25, 25-29, 30-35, &gt;35. Holders. número de asegurados. Reclamos. número de reclamos. Esta vez tenemos: cada observación \\(i\\) corresponde a un grupo de asegurados de acuerdo a su distrito, tipo de coche, y edad. el resultado \\(y_i\\) es el número de reclamos en dicho grupo los expuestos \\(u_i\\) son el número de asegurados las entradas son los índices de precinto y etnicidad los predictores son: distrito, grupo y edad Ilustramos el ajuste del modelo en tres pasos. Primero, ajustamos un modelo con los expuestos y un término constante solo: mod_1 &lt;- glm(formula = Claims ~ 1, family=poisson, offset=log(Holders), data = Insurance) summary(mod_1) #&gt; #&gt; Call: #&gt; glm(formula = Claims ~ 1, family = poisson, data = Insurance, #&gt; offset = log(Holders)) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.973 -0.325 0.790 1.976 4.073 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.0033 0.0178 -112 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 236.26 on 63 degrees of freedom #&gt; Residual deviance: 236.26 on 63 degrees of freedom #&gt; AIC: 555.6 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Ahora agregamos el predictor de distrito: mod_2 &lt;- glm(formula = Claims ~ District, family=poisson, offset=log(Holders), data = Insurance) summary(mod_2) #&gt; #&gt; Call: #&gt; glm(formula = Claims ~ District, family = poisson, data = Insurance, #&gt; offset = log(Holders)) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.556 -0.486 0.847 1.703 4.112 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.0328 0.0269 -75.54 &lt; 2e-16 *** #&gt; District2 0.0224 0.0430 0.52 0.60273 #&gt; District3 0.0133 0.0503 0.26 0.79232 #&gt; District4 0.2218 0.0616 3.60 0.00031 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 236.26 on 63 degrees of freedom #&gt; Residual deviance: 223.53 on 60 degrees of freedom #&gt; AIC: 548.9 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Nota: El AIC disminuye de 555.58 a 548.85. Se puede ver que solamente District4 tiene un coeficiente significativo. Se puede interpretar como que aquellos que son del Distrito 4 tiene 22% más reclamos comparado con el Distrito control, el Distrito 1. Ahora agregamos el predictor de edad: Insurance$Age &lt;- C(Insurance$Age,treatment) mod_3 &lt;- glm(formula = Claims ~ District + Age, family=poisson, offset=log(Holders), data = Insurance) summary(mod_3) #&gt; #&gt; Call: #&gt; glm(formula = Claims ~ District + Age, family = poisson, data = Insurance, #&gt; offset = log(Holders)) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.024 -0.988 -0.126 1.227 3.403 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.6351 0.0681 -24.00 &lt; 2e-16 *** #&gt; District2 0.0345 0.0430 0.80 0.42278 #&gt; District3 0.0468 0.0505 0.93 0.35381 #&gt; District4 0.2470 0.0617 4.01 6.2e-05 *** #&gt; Age25-29 -0.1563 0.0828 -1.89 0.05890 . #&gt; Age30-35 -0.2986 0.0812 -3.68 0.00023 *** #&gt; Age&gt;35 -0.5089 0.0698 -7.29 3.2e-13 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 236.26 on 63 degrees of freedom #&gt; Residual deviance: 140.09 on 57 degrees of freedom #&gt; AIC: 471.4 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Notamos que: El AIC disminuye nuevamente de 548.85 a 471.41, una reducción mucho más significativa. Esto nos dice que la variable de edad mejora considerablemente el ajuste del modelo. El coeficiente de Distrito 4 aumentó de 22% a 24% cuando controlamos por el grupo de edad. Comparando con el grupo base (grupo control) Distrito 1, el número de reclamos es \\(\\exp(0.22)=1.24\\) veces más en el Distrito 4. El grupo de edad L (25-29) tiene un coeficiente significativo de -0.37 que podemos interpretar como que pertenecer a ese grupo de edad tiene 37% menos reclamos comparado con el grupo de edad control (&lt;25). Por último incluyendo todos los predictores el modelo ajustado es: Insurance$Group &lt;- C(Insurance$Group, treatment) mod_4 &lt;- glm(formula = Claims ~ District + Age + Group, family=poisson, offset=log(Holders), data = Insurance) summary(mod_4) #&gt; #&gt; Call: #&gt; glm(formula = Claims ~ District + Age + Group, family = poisson, #&gt; data = Insurance, offset = log(Holders)) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.466 -0.508 -0.032 0.556 1.940 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.8217 0.0768 -23.72 &lt; 2e-16 *** #&gt; District2 0.0259 0.0430 0.60 0.54760 #&gt; District3 0.0385 0.0505 0.76 0.44566 #&gt; District4 0.2342 0.0617 3.80 0.00015 *** #&gt; Age25-29 -0.1910 0.0829 -2.31 0.02115 * #&gt; Age30-35 -0.3450 0.0814 -4.24 2.2e-05 *** #&gt; Age&gt;35 -0.5367 0.0700 -7.67 1.7e-14 *** #&gt; Group1-1.5l 0.1613 0.0505 3.19 0.00141 ** #&gt; Group1.5-2l 0.3928 0.0550 7.14 9.2e-13 *** #&gt; Group&gt;2l 0.5634 0.0723 7.79 6.6e-15 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 236.26 on 63 degrees of freedom #&gt; Residual deviance: 51.42 on 54 degrees of freedom #&gt; AIC: 388.7 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Observamos que controlando por el grupo de auto los coeficientes de las otras variables no cambian mucho. Número de expuestos (interpretación) Interpretamos que los asegurados con coches de 1-1.5 litros tienen 16.1% más reclamos que los asegurados con autos de &lt;1 litro. Además, veamos que en este ejemplo el número de reclamos se compara con el número de asegurados, de modo que como el coeficiente para el indicador de edad entre 25 y 29 es menor que 1, entonces las personas de este grupo de edad tienen un número desproporcionadamente menor en sus tasas de reclamos, en comparación con las personas del grupo de edad menor a 25 años. 10.9 Ejemplo: árboles Veamos un ejemplo con datos de árboles. dat &lt;- read_csv(&quot;datos/treedata.csv&quot;) dat %&gt;% head %&gt;% knitr::kable() plotID date plotsize spcode species cover utme utmn elev tci streamdist disturb beers ATBN-01-0403 08-28-2001 1000 ABIEFRA Abies fraseri 1 275736 3942439 1660 5.70 491 CORPLOG 0.224 ATBN-01-0532 07-24-2002 1000 ABIEFRA Abies fraseri 8 302847 3942772 1712 3.82 454 VIRGIN 0.834 ATBN-01-0533 07-24-2002 1000 ABIEFRA Abies fraseri 3 303037 3943039 1722 3.89 453 LT-SEL 1.333 ATBN-01-0536 07-25-2002 1000 ABIEFRA Abies fraseri 3 273927 3935488 1754 3.15 492 SETTLE 1.471 ATBP-01-0001 05-11-1999 10000 ABIEFRA Abies fraseri 8 273857 3937870 1945 5.68 492 VIRGIN 1.644 ATBP-01-0005 08-25-1999 10000 ABIEFRA Abies fraseri 4 273876 3935462 1751 5.42 546 SETTLE 0.000 La variable cover de cobertura es un número entero entre mayor o igual a 1 y representa la presencia de árboles de determinada especie en una parcela. Las variables en los datos son: plotID: unique code for each spatial unit (note some sampled more than once) date: when species occurrence recorded plotsize: size of quadrat in m2 spcode: unique 7-letter code for each species species: species name cover: local abundance measured as estimated horizontal cover (ie, relative area of shadow if sun is directly above) classes 1-10 are: 1=trace, 2=0-1%, 3=1-2%, 4=2-5%, 5=5-10%, 6=10-25%, 7=25-50%, 8=50-75%, 9=75-95%, 10=95-100% utme: plot UTM Easting, zone 17 (NAD27 Datum) utmn: plot UTM Northing, zone 17 (NAD27 Datum) elev: elevation in meters from a digital elevation model (10 m res) tci: topographic convergence index, or site “water potential”; measured as the upslope contributing area divided by the tangent of the slope angle (Beven and Kirkby 1979) streamdist: distance of plot from the nearest permanent stream (meters) disturb: plot disturbance history (from a Park report); CORPLOG=corporate logging; SETTLE=concentrated settlement, VIRGIN=“high in virgin attributes”, LT-SEL=light or selective logging beers: transformed slope aspect (‘heat load index’); 0 is SW (hottest), 2 is NE (coolest) Cumple dos criterios comunes: la variable respuesta es discreta y entera tiene una variación que generalmente aumenta con la media (se puede considerar esto desde los primeros principios: si una especie tiene una abundancia media alrededor de 1, la varianza tiene que ser baja porque no se puede obtener más baja que esto dados nuestros datos, una media de 5, sin embargo, podría tener una gran varianza). Tomemos como ejemplo primero el abeto oriental: dat_2 &lt;- dat %&gt;% filter(species == &quot;Tsuga canadensis&quot;) Veamos media y varianza: mean(dat_2$cover) #&gt; [1] 4.66 var(dat_2$cover) #&gt; [1] 4.47 Veamos la distribución: ggplot(dat_2, aes(x=cover)) + geom_bar(stat=&#39;count&#39;) Si los datos tuvieran una distribución Poisson: dat_sim &lt;- data.frame(cover.sim= rpois(700,4.66)) ggplot(dat_sim, aes(x=cover.sim)) + geom_bar(stat=&#39;count&#39;) Ajustamos primero el modelo únicamente con el intercepto: mod_1 &lt;- glm(cover~1,data=dat_2,family=poisson) summary(mod_1) #&gt; #&gt; Call: #&gt; glm(formula = cover ~ 1, family = poisson, data = dat_2) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.059 -0.823 -0.313 1.008 2.143 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.539 0.017 90.7 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 749.25 on 745 degrees of freedom #&gt; Residual deviance: 749.25 on 745 degrees of freedom #&gt; AIC: 3212 #&gt; #&gt; Number of Fisher Scoring iterations: 4 La salida es similar a la salida de lm. Una diferencia importante es cómo se ajustan los coeficientes: esperábamos una media de 4.66, pero en lugar de eso obtuvimos 1.54. Nota: Aquí y en todos los demás casos donde la función liga no es la identidad, los coeficientes ajustados están en la escala de la función liga, no en la escala de los datos originales. Para recuperar el coeficiente apropiadamente escalado aplicamos la inversa de la función liga. Aplicamos la función exponencial: exp(coefficients(mod_1)) #&gt; (Intercept) #&gt; 4.66 Agregamos un predictor continuo elev de elevación: ggplot(dat_2, aes(x=elev,y=cover)) + geom_jitter(width = 0, height = 0.3, size=0.8, alpha = 0.8) + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; mod_2 &lt;- glm(cover~tci+elev+beers+streamdist,data=dat_2,family=poisson) summary(mod_2) #&gt; #&gt; Call: #&gt; glm(formula = cover ~ tci + elev + beers + streamdist, family = poisson, #&gt; data = dat_2) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.3035 -0.7929 -0.0869 0.6952 2.6517 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.45e+00 7.99e-02 18.16 &lt; 2e-16 *** #&gt; tci 1.49e-02 6.90e-03 2.16 0.031 * #&gt; elev 8.97e-05 5.71e-05 1.57 0.116 #&gt; beers 6.21e-02 2.48e-02 2.51 0.012 * #&gt; streamdist -8.14e-04 1.21e-04 -6.73 1.7e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 748.23 on 744 degrees of freedom #&gt; Residual deviance: 677.39 on 740 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; AIC: 3145 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Nota: la prueba de \\(\\chi^2\\) generalmente se “recomienda” para los modelos con “varianza conocida” (Poisson y Binomial). el modelo con elevación no le añade al modelo ningún poder explicativo Usamos ahora la variable disturb de disturbance (o perturbación), que es un cambio temporal en las condiciones ambientales que puede causar un cambio pronunciado en un ecosistema. Ajustamos el modelo: mod_3 &lt;- glm(cover~tci+elev+beers+disturb+streamdist,data=dat_2,family=poisson) summary(mod_3) #&gt; #&gt; Call: #&gt; glm(formula = cover ~ tci + elev + beers + disturb + streamdist, #&gt; family = poisson, data = dat_2) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.187 -0.776 -0.105 0.721 2.500 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.39e+00 1.07e-01 13.02 &lt; 2e-16 *** #&gt; tci 1.57e-02 6.91e-03 2.28 0.0228 * #&gt; elev 6.77e-05 7.45e-05 0.91 0.3638 #&gt; beers 6.44e-02 2.51e-02 2.56 0.0103 * #&gt; disturbLT-SEL 5.66e-02 5.26e-02 1.07 0.2824 #&gt; disturbSETTLE 7.99e-02 6.34e-02 1.26 0.2079 #&gt; disturbVIRGIN 1.52e-01 5.32e-02 2.85 0.0044 ** #&gt; streamdist -8.18e-04 1.22e-04 -6.72 1.8e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 748.23 on 744 degrees of freedom #&gt; Residual deviance: 668.92 on 737 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; AIC: 3142 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Parece haber relación con que el bosque sea “virgen”, pero el modelo realmente no es muy bueno (el AIC es similar). AIC(mod_1, mod_2, mod_3) #&gt; Warning in AIC.default(mod_1, mod_2, mod_3): models are not all fitted to #&gt; the same number of observations #&gt; df AIC #&gt; mod_1 1 3212 #&gt; mod_2 5 3145 #&gt; mod_3 8 3142 La función step nos dice qué variables disminuyen mayor el AIC. step(mod_3) #&gt; Start: AIC=3142 #&gt; cover ~ tci + elev + beers + disturb + streamdist #&gt; #&gt; Df Deviance AIC #&gt; - elev 1 670 3141 #&gt; &lt;none&gt; 669 3142 #&gt; - disturb 3 677 3145 #&gt; - tci 1 674 3145 #&gt; - beers 1 676 3147 #&gt; - streamdist 1 716 3187 #&gt; #&gt; Step: AIC=3141 #&gt; cover ~ tci + beers + disturb + streamdist #&gt; #&gt; Df Deviance AIC #&gt; &lt;none&gt; 670 3141 #&gt; - tci 1 674 3144 #&gt; - disturb 3 680 3145 #&gt; - beers 1 677 3146 #&gt; - streamdist 1 716 3185 #&gt; #&gt; Call: glm(formula = cover ~ tci + beers + disturb + streamdist, family = poisson, #&gt; data = dat_2) #&gt; #&gt; Coefficients: #&gt; (Intercept) tci beers disturbLT-SEL disturbSETTLE #&gt; 1.464757 0.015326 0.065147 0.035010 0.052524 #&gt; disturbVIRGIN streamdist #&gt; 0.156454 -0.000799 #&gt; #&gt; Degrees of Freedom: 744 Total (i.e. Null); 738 Residual #&gt; (1 observation deleted due to missingness) #&gt; Null Deviance: 748 #&gt; Residual Deviance: 670 AIC: 3140 Tomemos en cuenta el modelo con una interacción entre streamdist y disturb: mod_4 = glm(cover~disturb*streamdist,data=dat_2,family=poisson) summary(mod_4) #&gt; #&gt; Call: #&gt; glm(formula = cover ~ disturb * streamdist, family = poisson, #&gt; data = dat_2) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.4084 -0.7680 -0.0713 0.6472 2.3969 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.515210 0.062617 24.20 &lt; 2e-16 *** #&gt; disturbLT-SEL 0.257633 0.074157 3.47 0.00051 *** #&gt; disturbSETTLE 0.097346 0.083845 1.16 0.24563 #&gt; disturbVIRGIN 0.262322 0.088359 2.97 0.00299 ** #&gt; streamdist -0.000165 0.000262 -0.63 0.52771 #&gt; disturbLT-SEL:streamdist -0.001261 0.000317 -3.98 6.8e-05 *** #&gt; disturbSETTLE:streamdist -0.000132 0.000402 -0.33 0.74289 #&gt; disturbVIRGIN:streamdist -0.000630 0.000358 -1.76 0.07883 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 748.23 on 744 degrees of freedom #&gt; Residual deviance: 659.54 on 737 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; AIC: 3133 #&gt; #&gt; Number of Fisher Scoring iterations: 4 ¿Cómo interpretarías los coeficientes del término de interacción? Veamos la distribución de streamdist. ggplot(data = dat_2, aes(x = streamdist)) + geom_histogram(binwidth = 20) El modelo de regresión anterior presenta sobredispersión. \\(g(x)=\\alpha + \\beta x\\). \\(g(x)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\,e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\\). \\(g(x)=x\\). 10.10 Sobredispersión Sobredispersión: Una peculiaridad de la distribución de Poisson es que su media es igual a su varianza. Sin embargo, en ciertos conjuntos de datos se observa una varianza superior a la esperada. El fenómeno se conoce como sobredispersión e indica que el modelo no es adecuado. Un motivo frecuente es la omisión de alguna variable relevante. En algunos casos se aconseja recurrir a la distribución binomial negativa. En el modelo \\[ E(y_i)=u_i \\theta_i,\\qquad \\sqrt{V(y_i)} = \\sqrt{u_i \\theta_i} \\] Definimos los residuales estandarizados como \\[ \\begin{eqnarray*} z_i &amp;=&amp; \\dfrac{y_i -\\hat{y}_i}{\\sqrt{V(\\hat{y}_i)} } \\\\ &amp;=&amp; \\dfrac{y_i - u_i\\hat{\\theta}_i}{\\sqrt{u_i \\hat{\\theta}_i}} \\end{eqnarray*} \\] donde \\[ \\hat{\\theta}_i = e^{X_i\\hat{\\beta}} \\] Si el modelo de Poisson es verdadero, entonces las \\(z_i\\)’s deben ser aproximadamente independientes (no exactamente independiente, ya que se utiliza la misma estimación \\(\\beta\\) para calcularlos todos), cada uno con media 0 y desviación estándar 1. Sin embargo, si hay sobredispersión, esperaríamos que las \\(z_i\\)’s fueran más grandes, en valor absoluto, reflejando la variación adicional más allá de lo que predice el modelo. Podemos probar la sobredispersión en una regresión Poisson calculando la suma de cuadrados de los \\(n\\) residuos estandarizados \\[ \\sum_{i=1}^n{z_i^2} \\] y comparando esta cantidad con la distribución \\(\\chi^2_{n-k}\\), que es lo que esperaríamos bajo el modelo (usando \\(n-k\\) en lugar de \\(n\\) grados de libertad para explicar la estimación de \\(k\\) coeficientes en la regresión). La distribución de una \\(\\chi^2_{n-k}\\) tiene media \\(n-k\\), por lo que se define la sobredispersión como \\[ \\mbox{sobredispersión estimada} = \\dfrac{1}{n-k}\\sum_{i=1}^n{z_i^2}. \\] 10.11 Ejemplo: número de publicaciones Usamos datos de Long (1990) sobre la cantidad de publicaciones producidas por Ph.D. bioquímicos para ilustrar la aplicación de Poisson con sobredispersión (Long, Freese, and others 2001). Las variables en el conjunto de datos son: art: articles in last three years of Ph.D. fem: coded one for females mar: coded one if married kid5: number of children under age six phd: prestige of Ph.D. program ment: articles by mentor in last three years library(foreign) ab &lt;- read.dta(&quot;http://www.stata-press.com/data/lf2/couart2.dta&quot;) ab %&gt;% head %&gt;% knitr::kable() art fem mar kid5 phd ment 0 Men Married 0 2.52 7 0 Women Single 0 2.05 6 0 Women Single 0 3.75 6 0 Men Married 1 1.18 3 0 Women Single 0 3.75 26 0 Women Married 2 3.59 2 r &lt;- c(mean(ab$art), var(ab$art)) c(mean=r[1], var=r[2], r[2]/r[1]) #&gt; mean var #&gt; 1.69 3.71 2.19 La cantidad media de artículos es 1.69 y la varianza es 3.71, un poco más que el doble de la media. Los datos están muy dispersos, pero por supuesto no hemos considerado ninguna covariable todavía. Vamos a ajustar el modelo utilizado por Long y Freese (2001), un modelo aditivo simple que utiliza los cinco predictores. mp &lt;- glm(art~fem+mar+kid5+phd+ment, family=poisson, data=ab) summary(mp) #&gt; #&gt; Call: #&gt; glm(formula = art ~ fem + mar + kid5 + phd + ment, family = poisson, #&gt; data = ab) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.567 -1.540 -0.366 0.572 5.447 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.30462 0.10298 2.96 0.0031 ** #&gt; femWomen -0.22459 0.05461 -4.11 3.9e-05 *** #&gt; marMarried 0.15524 0.06137 2.53 0.0114 * #&gt; kid5 -0.18488 0.04013 -4.61 4.1e-06 *** #&gt; phd 0.01282 0.02640 0.49 0.6271 #&gt; ment 0.02554 0.00201 12.73 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 1817.4 on 914 degrees of freedom #&gt; Residual deviance: 1634.4 on 909 degrees of freedom #&gt; AIC: 3314 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Vemos que el modelo obviamente no se ajusta a los datos. Calculamos la sobredispersión estimada: yhat &lt;- predict(mp, type=&quot;response&quot;) z &lt;- (ab$art-yhat)/sqrt(yhat) k &lt;- length(mp$coefficients) n &lt;- nrow(ab) cat(&quot;la sobredispersión estimada es &quot;, sum(z^2)/(n-k), &quot;\\n&quot;) #&gt; la sobredispersión estimada es 1.83 cat(&quot;valor p de la prueba de sobredispersión&quot;, pchisq(sum(z^2), n-k), &quot;\\n&quot;) #&gt; valor p de la prueba de sobredispersión 1 El valor p es 1, lo que indica que la probabilidad de que una variable aleatoria con distribución \\(\\chi^2_{909}\\) tome un valor tan grande como 1662.547 es esencialmente cero. También podemos hacer una gráfica de residuales estandarizados vs ajustados: ab$ajustados &lt;- predict(mp, type=&quot;response&quot;) ab$residuales &lt;- (ab$art-ab$ajustados) ab$residuales_est &lt;- (ab$art-ab$ajustados)/sqrt(ab$ajustados) ggplot(ab, aes(x=ajustados, y=residuales_est)) + geom_jitter(width = .5, height = .5, alpha = 0.3) + geom_abline(slope = 0, intercept = 0, color =&#39;red&#39;) + geom_abline(slope = 0, intercept = 2, color =&#39;navyblue&#39;) + geom_abline(slope = 0, intercept = -2, color =&#39;navyblue&#39;) La gráfica de residuales vs ajustados: ggplot(ab, aes(x=ajustados, y=residuales)) + geom_jitter(width = 1, height = 1) + geom_abline(slope = 0, intercept = 0, color =&#39;red&#39;) Para ajustar la inferencia por sobredispersión en glm utilizamos la familia quasipoisson: mq &lt;- glm(art~fem+mar+kid5+phd+ment, family=quasipoisson, data=ab) summary(mq) #&gt; #&gt; Call: #&gt; glm(formula = art ~ fem + mar + kid5 + phd + ment, family = quasipoisson, #&gt; data = ab) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.567 -1.540 -0.366 0.572 5.447 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.30462 0.13927 2.19 0.02898 * #&gt; femWomen -0.22459 0.07386 -3.04 0.00243 ** #&gt; marMarried 0.15524 0.08300 1.87 0.06176 . #&gt; kid5 -0.18488 0.05427 -3.41 0.00069 *** #&gt; phd 0.01282 0.03570 0.36 0.71954 #&gt; ment 0.02554 0.00271 9.41 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for quasipoisson family taken to be 1.83) #&gt; #&gt; Null deviance: 1817.4 on 914 degrees of freedom #&gt; Residual deviance: 1634.4 on 909 degrees of freedom #&gt; AIC: NA #&gt; #&gt; Number of Fisher Scoring iterations: 5 Escribimos este modelo como \\[ y_i \\sim \\mbox{Poisson sobredispersado }(u_i \\exp(X_i\\beta), \\omega), \\] donde \\(\\omega\\) es el parámetro de sobredispersión. Estrictamente hablando, “Poisson sobredispersado” no es un modelo único, sino que describe cualquier modelo de recuento de datos para el cual la varianza de los datos es \\(\\omega\\) veces la media, reduciéndose al Poisson si \\(\\omega = 1\\). Un modelo específico común mente utilizado en este escenario es la distribución llamada binomial negativa: \\[ y_i \\sim \\mbox{Negativo-binomial} (\\mbox{media} = u_i \\exp (X_i\\beta), \\mbox{sobredispersión} = \\omega). \\] Ahora ajustamos un modelo binomial negativo con los mismos predictores. Para hacer esto, necesitamos la función glm.nb() del paquete MASS. library(MASS) mnb &lt;- glm.nb(art ~ fem + kid5 + ment, data = ab) summary(mnb) #&gt; #&gt; Call: #&gt; glm.nb(formula = art ~ fem + kid5 + ment, data = ab, init.theta = 2.240577103, #&gt; link = log) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.174 -1.377 -0.284 0.422 3.449 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.39102 0.06453 6.06 1.4e-09 *** #&gt; femWomen -0.23270 0.07218 -3.22 0.0013 ** #&gt; kid5 -0.13775 0.04815 -2.86 0.0042 ** #&gt; ment 0.02937 0.00312 9.42 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for Negative Binomial(2.24) family taken to be 1) #&gt; #&gt; Null deviance: 1105.0 on 914 degrees of freedom #&gt; Residual deviance: 1004.2 on 911 degrees of freedom #&gt; AIC: 3135 #&gt; #&gt; Number of Fisher Scoring iterations: 1 #&gt; #&gt; #&gt; Theta: 2.241 #&gt; Std. Err.: 0.267 #&gt; #&gt; 2 x log-likelihood: -3125.329 Podemos ver el parámetro \\(1/\\omega\\), que interpretamos como la varianza estimada: mnb$theta #&gt; [1] 2.24 Se puede ajustar el modelo usando glm indicando que se trata de una familia binomial negativa usando el mismo parámetro de sobredispersión utilizando únicamente los 3 predictores más explicativos: mnbg &lt;- glm(art ~ fem + kid5 + ment, family=negative.binomial(mnb$theta), data = ab) summary(mnbg) #&gt; #&gt; Call: #&gt; glm(formula = art ~ fem + kid5 + ment, family = negative.binomial(mnb$theta), #&gt; data = ab) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.174 -1.377 -0.284 0.422 3.449 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.39103 0.06562 5.96 3.6e-09 *** #&gt; femWomen -0.23270 0.07340 -3.17 0.0016 ** #&gt; kid5 -0.13776 0.04897 -2.81 0.0050 ** #&gt; ment 0.02937 0.00317 9.27 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for Negative Binomial(2.24) family taken to be 1.03) #&gt; #&gt; Null deviance: 1105.0 on 914 degrees of freedom #&gt; Residual deviance: 1004.2 on 911 degrees of freedom #&gt; AIC: 3133 #&gt; #&gt; Number of Fisher Scoring iterations: 6 El ajuste del modelo es similar. Sin embargo, la varianza es varias veces la media en este modelo, y dado que los errores estándar se basan en el supuesto de que la varianza es igual a la media, esto crea un problema esn las estimaciones. La varianza real es varias veces más de lo que debería ser, y los errores estándar estimados con el modelo Poisson estaban claramente subestimados. 10.12 Tarea Se tienen datos de una prueba controlada aleatorizada (en inglés se la llama RCT por randomized controlled trial) dirigida a parejas con alto riesgo de infección por VIH. La intervención brindó sesiones de asesoramiento sobre prácticas que podrían reducir la probabilidad de contraer el VIH. Las parejas se asignaron al azar a un grupo control (en el que solo participó la mujer) o un grupo en el que participaron los dos miembros de la pareja. Después de tres meses se registró como resultado el “número de actos sexuales sin protección”. risky_behaviors &lt;- read_csv(&quot;datos/risky_behaviors.csv&quot;) risky_behaviors %&gt;% sample_n(10) %&gt;% knitr::kable() sex couples women_alone bs_hiv bupacts fupacts woman 0 1 negative 28 0 woman 0 1 negative 9 4 man 0 1 negative 3 0 woman 0 1 negative 3 3 man 0 0 negative 87 6 man 0 1 negative 30 3 man 0 1 positive 50 28 man 1 0 negative 16 8 man 1 0 positive 20 5 woman 0 1 negative 5 25 Modela este resultado (fupacts) como una función de la asignación del tratamiento (women_alone) usando una regresión Poisson. ¿El modelo se ajusta bien? ¿Hay evidencia de sobredispersión? A continuación, añade al modelo las variables restantes. ¿El modelo se ajusta mejor? ¿Hay evidencia de sobredispersión? Compara el AIC con el del inciso anterior. Ajusta un modelo Poisson sobredispersado. ¿Qué puedes concluir con respecto a la efectividad de la intervención? Estos datos incluyen respuestas tanto de hombrescomo de mujeres de las parejas participantes. ¿Esto nos dice algo con respecto a los supuestos del modelado? Referencias "],
["discriminante-lineal-lda-1.html", "Clase 11 Discriminante Lineal (LDA) 1 11.1 Problemas de clasificación 11.2 Funciones de discriminante 11.3 Regresión lineal en una matriz indicadora 11.4 Discriminante lineal de Fisher 11.5 Tarea", " Clase 11 Discriminante Lineal (LDA) 1 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 11.1 Problemas de clasificación En los problemas de clasificación deseamos predecir categorías de clase discretas, o más generalmente las probabilidades, que se encuentran entre 0 y 1. Para lograr predecir probabilidades, consideramos una generalización del modelo linea en el cual transformamos la función lineal de \\(\\beta\\) usando una función no lineal \\(g^{-1}\\) tal que \\[ y_i = g^{-1}(x_i^T\\beta). \\] A \\(g\\) la conocemos como función liga. Para clasificar utilizamos las superficies de decisión que corresponden a \\[ y_i(\\beta) = \\mbox{constante}, \\] de modo que \\[ x_i^T\\beta = \\mbox{constante} \\] y, por lo tanto, las superficies de decisión son funciones lineales de \\(x\\), incluso si la función \\(g\\) no es lineal. Por esta razón, los modelos descritos se llaman modelos lineales generalizados. 11.2 Funciones de discriminante Definimos un discriminante (clasificador) lineal así: \\[ y_i = x_i^T\\beta \\] lo denotaremos por \\[ y(x) = w^T x+ w_0 \\] porque vemos \\(w_0\\) como el umbral de tal forma que un vector de entrada \\(x\\) se asigna a la clase 1 cuando \\(y(x)&gt;0\\) y a la clase 2 en otro caso. La superficie de decisión está definida, por lo tanto, por la ecuación \\(y(x) = 0\\), que corresponde a un hiperplano de \\(D-1\\) dimensiones dentro del conjunto de entrada de \\(D\\) dimensiones. Consideremos dos puntos \\(x_A\\) y \\(x_B\\) en la superficie de decisión. Como \\[ y(x_A) = y(x_B) = 0, \\] entonces \\[ w^T(x_A −x_B) = 0. \\] Por lo tanto, el vector \\(w\\) es ortogonal a todo vector en la superficie de decisión, y así \\(w\\) determina la orientación de la superficie. Similarmente, si \\(x\\) está en la superficie de decisión, entonces \\(y(x)=0\\), y entonces, la distancia del origen a la superficie de decisión es: \\[ \\dfrac{w^Tx}{\\|{w}\\|} = -\\dfrac{w_0}{\\|{w}\\|}. \\] 11.3 Regresión lineal en una matriz indicadora Supongamos que \\(Y\\) es una matriz indicadora donde \\(Y_{ij}\\) es \\(1\\) si la \\(i\\)-ésima obsevación pertenece a la categoría \\(j\\) y \\(0\\) en otro caso. \\[ \\hat{W} = (X^TX)^{−1}X^TY. \\] Si \\(\\tilde{x}\\) es un vector de entradas, entonces \\[ y(x) = \\hat{W}^T\\tilde{x} \\] library(tidyverse) library(matlib) set.seed(186923) n &lt;- 300 x1 &lt;- c(rnorm(n/3,-5), rnorm(n/3, 0, 1.5), rnorm(n/3, 5)) x2 &lt;- x1 + rnorm(n, sd = 2) + c(rep(-5, n/3), rep(0, n/3), rep(5, n/3)) y &lt;- c(rep(&quot;a&quot;,n/3),rep(&quot;b&quot;,n/3),rep(&quot;c&quot;,n/3)) df &lt;- data.frame(x1, x2, y) Y &lt;- matrix(nrow = n, ncol = 3) Y[,1] &lt;- c(rep(1,n/3),rep(0,2*n/3)) Y[,2] &lt;- c(rep(0,n/3),rep(1,n/3),rep(0,n/3)) Y[,3] &lt;- 1 - Y[,1] - Y[,2] df$y1 &lt;- Y[,1] df$y2 &lt;- Y[,2] df$y3 &lt;- Y[,3] X &lt;- as.matrix(df[,c(1,2)]) p &lt;- X %*% inv(t(X) %*% X) %*% t(X) %*% Y df$p1 &lt;- p[,1] df$p2 &lt;- p[,2] df$p3 &lt;- p[,3] ggplot(df, aes(x = x1, y = x2))+ geom_point(aes(color = y)) + geom_line(aes(y = p1), color = &#39;red&#39;) + geom_line(aes(y = p2), color = &#39;green&#39;) + geom_line(aes(y = p3), color = &#39;blue&#39;) Hay un problema serio con el enfoque de regresión cuando el número de clases \\(K \\geq3\\), y en especial si \\(K\\) es grande. Debido a la naturaleza rígida del modelo de regresión, unas clases pueden enmascarar a otras. Esto lo vemos en la gráfica de arriba con \\(K = 3\\). Las tres clases están perfectamente separadas por límites de decisión lineales, sin embargo, la regresión lineal pierde por completo a la clase de en medio. lm_1 &lt;- lm(y1 ~ x1 + x2, data = df) lm_2 &lt;- lm(y2 ~ x1 + x2, data = df) df$pred_1 &lt;- predict(lm_1) df$pred_2 &lt;- predict(lm_2) df$pred_3 &lt;- 1 - df$pred_1 - df$pred_2 Obtenemos la predicción tomando el máximo de cada renglón: df$pred &lt;- ifelse(df$pred_1 &gt; df$pred_2 &amp; df$pred_1 &gt; df$pred_3, &#39;a&#39;, ifelse(df$pred_2 &gt; df$pred_1 &amp; df$pred_2 &gt; df$pred_3, &#39;b&#39;, &#39;c&#39;)) Podemos hacer una gráfica de las predicciones en este caso de \\(x_1\\) contra los ajustados por el modelo lineal: ggplot(df, aes(x=x1, y = pred_1)) + geom_point(color = &#39;red&#39;) + geom_point(aes(x=x1, y = pred_2), color = &#39;green&#39;) + geom_point(aes(x=x1, y = pred_3), color = &#39;blue&#39;) Comparamos contra los observados y calculamos el error: 1 - mean(df$pred == df$y) #&gt; [1] 0.24 Si utilizamos un modelo lineal con términos cuadráticos entonces el ajuste lo haríamos de esta manera: lm_1_q &lt;- lm(y1 ~ poly(x1 + x2, 2), data = df) lm_2_q &lt;- lm(y2 ~ poly(x1 + x2, 2), data = df) lm_3_q &lt;- lm(y3 ~ poly(x1 + x2, 2), data = df) df$pred_1_q &lt;- predict(lm_1_q) df$pred_2_q &lt;- predict(lm_2_q) df$pred_3_q &lt;- predict(lm_3_q) Obtenemos nuevamente la predicción tomando el máximo de cada renglón: df$pred_q &lt;- ifelse(df$pred_1_q &gt; df$pred_2_q &amp; df$pred_1_q &gt; df$pred_3_q, &#39;a&#39;, ifelse(df$pred_2_q &gt; df$pred_1_q &amp; df$pred_2_q &gt; df$pred_3_q, &#39;b&#39;, &#39;c&#39;)) ggplot(df, aes(x=x2, y = pred_1_q)) + geom_point(color = &#39;red&#39;) + geom_point(aes(x=x2, y = pred_2_q), color = &#39;green&#39;) + geom_point(aes(x=x2, y = pred_3_q), color = &#39;blue&#39;) Comparamos contra los observados y calculamos el error: 1 - mean(df$pred_q == df$y) #&gt; [1] 0.02 Repetimos una tercera vez pero ahora utilizando regresión logística: glm_1 &lt;- glm(I(y==&#39;a&#39;) ~ x1 + x2, family = binomial, data = df) #&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred glm_2 &lt;- glm(I(y==&#39;b&#39;) ~ x1 + x2, family = binomial, data = df) glm_3 &lt;- glm(I(y==&#39;c&#39;) ~ x1 + x2, family = binomial, data = df) #&gt; Warning: glm.fit: algorithm did not converge #&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred df$pred1_glm &lt;- predict(glm_1, type = &#39;response&#39;) df$pred2_glm &lt;- predict(glm_2, type = &#39;response&#39;) df$pred3_glm &lt;- predict(glm_3, type = &#39;response&#39;) Calculamos las predicciones de acuerdo a este modelo logístico: df$pred_glm &lt;- ifelse(df$pred1_glm &gt; df$pred2_glm &amp; df$pred1_glm &gt; df$pred3_glm, &#39;a&#39;, ifelse(df$pred2_glm &gt; df$pred1_glm &amp; df$pred2_glm &gt; df$pred3_glm, &#39;b&#39;, &#39;c&#39;)) Calculamos el error: 1 - mean(df$pred_glm == df$y) #&gt; [1] 0.01 11.4 Discriminante lineal de Fisher Una forma de ver un modelo de clasificación lineal es en términos de reducción de dimensionalidad. Esto se refiere a la extracción de características de los datos que los transforma de un espacio de dimensión alta a un espacio de dimensión baja, usualmente \\(2\\) ó \\(3\\). Consideremos primero el caso de dos clases, y supongamos que tomamos un vector de predictores de dimensión \\(D\\) y lo proyectamos en una dimensión usando la misma función: \\[ y = w^Tx. \\] Si colocamos un umbral y clasificamos como clase 1 si \\(y\\geq-w_0\\), y clase 2 en otro caso, entonces obtenemos nuestro clasificador lineal estándar ya visto antes. En general, la proyección en una dimensión conduce a una pérdida considerable de información, y las clases que están bien separadas en el espacio original de dimensión \\(D\\) pueden superponerse fuertemente en una dimensión. Sin embargo, ajustando los componentes de \\(w\\), podemos seleccionar una proyección que maximiza la separación entre clases. Para empezar, consideremos un problema de dos clases en el que hay \\(N_1\\) puntos en la clase 1 y \\(N_2\\) puntos en la clase 2, de modo que los vectores de medias de las dos clases están dados por \\[ m_1 = \\dfrac{1}{N_1}\\sum_{i\\in C_1}x_i,\\qquad m_2=\\dfrac{1}{N_2}\\sum_{i\\in C_2}x_i. \\] Podríamos elegir \\(w\\) para maximizar la separación entre clases: \\[ w^T(m_2 − m_1) \\] Sin embargo, esta expresión se puede hacer arbitrariamente grande simplemente aumentando la magnitud de \\(w\\), entonces se utiliza la restricción \\[ \\sum_i{w_i^2}=1 \\] Resolvemos usando multiplicadores de Lagrange. Vemos que \\(w\\) es proporcional a \\(m_2 - m_1\\). Sin embargo, todavía hay un problema con este enfoque. Esto muestra Las dos clases de arriba que están bien separadas en el espacio bidimensional original \\((x_1, x_2)\\) pero se sobrelapan considerablemente cuando se proyectan sobre la línea que une sus medios. Esta dificultad surge por las covarianzas de las distribuciones de clase no son diagonales. La idea propuesta por Fisher es maximizar una función que: separe las observaciones por las medios de clase proyectadas, y al mismo tiempo dé una variación pequeña dentro de cada clase. La varianza dentro de la clase de los datos transformados de la clase \\(C_k\\) está dada por \\[ s_k = \\sum_{i\\in C_k}{(y_i - w^Tm_k)^2}. \\] donde \\(y_i = w^Tx_i\\). Para el caso de \\(K=2\\) clases, la varianza total dentro de las clases se define como \\(s_1^2 + s_2^2\\). Se define una cantidad llamada “criterio de Fisher” \\(J(w)\\) que se desea maximizar \\[ \\begin{eqnarray*} J(w) &amp;=&amp; \\dfrac{m_2-m_1}{s_1^2+s_2^2} \\\\ &amp;=&amp; \\dfrac{w^TS_Ww}{w^TS_Bw} \\end{eqnarray*} \\] donde \\(S_B\\) es la matriz de covarianzas entre clases \\[ S_B = (m_2 - m_1)(m_2-m_1)^T \\] y \\(S_W\\) es la matriz de covarianzas dentro de las clases \\[ S_W = \\sum_{i \\in C_1}{(x_i - m_1)(x_i-m_1)^T} + \\sum_{i \\in C_2}{(x_i - m_2)(x_i-m_2)^T}. \\] Buscamos maximizar \\(J(w)\\) con respecto a \\(w\\) para encontrar los pesos \\(w\\) para el discriminador lineal \\(y(x)=w^Tx\\). Notas: Este se conoce como discriminante lineal de Fisher, aunque estrictamente no es un discriminante, sino más bien una elección de dirección específica para la proyección de los datos en una dimensión. Sin embargo, los datos proyectados pueden usarse posteriormente para construir un discriminante, eligiendo un umbral \\(y_0\\) para que clasifiquemos un nuevo punto como clase 1 si \\(y(x)\\geq y_0\\) y como clase 2 en caso contrario. Podemos modelar las densidades condicionales de clase \\(p(y|C_k)\\) usando distribuciones normales y estimar parámetros por máxima verosimilitud. El supuesto de normalidad se justifica por el teorema del límite central porque \\(y = w^Tx\\) es una suma ponderada de variables aleatorias. 11.4.1 Ejemplo: separación entre clases Regresamos al ejemplo de arriba pero ahora hacemos el ajuste con discriminante lineal: library(MASS) lda_1 &lt;- lda(y ~ x1 + x2, data = df) df$pred_lda &lt;- predict(lda_1)$class Vemos nuevamente el error: 1 - mean(df$pred_lda == df$y) #&gt; [1] 0.0167 11.4.2 Ejemplo: iris de Fisher El estadístico y biólogo británico Ronald Fisher publicó su artículo de 1936 “El uso de mediciones múltiples en problemas taxonómicos como un ejemplo de análisis discriminante lineal”. El conjunto de datos consiste de 50 observaciones de cada una de las tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midieron cuatro características de cada muestra: la longitud y el ancho de los sépalos y pétalos, en centímetros. iris %&gt;% sample_n(10) %&gt;% knitr::kable() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 21 5.4 3.4 1.7 0.2 setosa 113 6.8 3.0 5.5 2.1 virginica 1 5.1 3.5 1.4 0.2 setosa 38 4.9 3.6 1.4 0.1 setosa 5 5.0 3.6 1.4 0.2 setosa 23 4.6 3.6 1.0 0.2 setosa 140 6.9 3.1 5.4 2.1 virginica 2 4.9 3.0 1.4 0.2 setosa 145 6.7 3.3 5.7 2.5 virginica 99 5.1 2.5 3.0 1.1 versicolor ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(aes(shape = Species), size = 3) ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point(aes(shape = Species), size = 3) En resumen tenemos 4 variables y todas están relacionadas con la especie a la cual pertenece (hay 3 especies): library(GGally) ggpairs(iris, columns = 1:ncol(iris), title = &quot;&quot;, axisLabels = &quot;show&quot;, columnLabels = colnames(iris)) require(MASS) iris.lda&lt;-lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris) datPred&lt;-data.frame(Species=predict(iris.lda)$class,predict(iris.lda)$x) #create data.frame Creamos las superficies de decisión y las probabilidades de clase: iris.lda2 &lt;- lda(datPred[,2:3], datPred[,1]) x &lt;- seq(min(datPred[,2]), max(datPred[,2]), length.out=30) y &lt;- seq(min(datPred[,3]), max(datPred[,3]), length.out=30) Xcon &lt;- matrix(c(rep(x,length(y)),rep(y, rep(length(x), length(y)))), ncol = 2) iris.pr1 &lt;- predict(iris.lda2, Xcon)$post[, c(&quot;setosa&quot;,&quot;versicolor&quot;)] %*% c(1,1) iris.pr2 &lt;- predict(iris.lda2, Xcon)$post[, c(&quot;virginica&quot;,&quot;setosa&quot;)] %*% c(1,1) Graficamos las superficies (rectas) de decisión sobre los ajustados por el modelo por especie de flor: pr&lt;-data.frame(x=rep(x, length(y)), y=rep(y, each=length(x)), z1=as.vector(iris.pr1), z2=as.vector(iris.pr2)) ggplot(datPred, aes(x=LD1, y=LD2) ) + geom_point(size = 3, aes(pch = Species, col=Species)) + geom_contour(data=pr, aes(x=x, y=y, z=z1), breaks=c(0,.5)) + geom_contour(data=pr, aes(x=x, y=y, z=z2), breaks=c(0,.5)) 11.5 Tarea Para comparar regresión lineal, análisis de discriminante lineal y regresión logística con dos clases utiliza el siguiente código para responder los incisos siguientes. Utiliza tu clave única como semilla. clave_unica &lt;- 123456 set.seed(clave_unica) n &lt;- 300 x1 &lt;- c(rnorm(n/3,-5), rnorm(n/3, 0, 1.5), rnorm(n/3, 5)) x2 &lt;- -12*c(rep(0,n/2),rep(1,n/2)) + x1 + rnorm(n, sd = 2) + c(rep(-5, n/2), rep(0, n/2)) out &lt;- sample((n/2):n, size = n/10) x1[out] &lt;- rnorm(n/10, 5, sd = 0.5) + 10 x2[out] &lt;- -12 + x1[out] + rnorm(n/10, sd = 0.5) - 15 y &lt;- c(rep(&#39;a&#39;, n/2), rep(&#39;b&#39;, n/2)) df &lt;- data.frame(x1, x2, y) ggplot(df, aes(x = x1, y = x2, color = y)) + geom_point() Haz una gráfica de las superficies de decisión (rectas que separan las dos clases) utilizando el modelo simple de regresión lineal con predictores lineales. Calcula el error de predicción. ¿Habrá alguna diferencia entre el error utilizando regresión lineal con términos de polinomios cuadráticos y el error calculado en el inciso anterior? Repite el inciso a. utilizando regresión logística. ¿Por qué la regresión logística es más consistente en la presencia de datos atípicos? Repite el inciso a. utilizando análisis de discriminante lineal y compara con el error obtenido en los incisos anteriores. Demuestra que, en regresión lineal, si cada vector objetivo \\(y_i\\) satisface una restricción lineal \\[ a^T y_i + b = 0 \\] para algunas constantes \\(a\\) y \\(b\\), entonces la predicción del modelo para cualquier valor de \\(x\\) satisface la misma restricción, de modo que \\[ a^T y(x) + b = 0. \\] Por lo tanto, utilizando la codificación 1-a-K indicadoras para las \\(K\\) clases, entonces las predicciones que produce el modelo tendrán la propiedad de que los elementos de \\(y(x)\\) sumarán a 1 para cualquier valor de \\(x\\). Aunque no necesariamente están en el intervalo \\((0,1)\\). "],
["discriminante-lineal-lda-2.html", "Clase 12 Discriminante Lineal (LDA) 2 12.1 Aplicaciones 12.2 Ejemplo: vinos 12.3 Ejemplo: admisiones al MBA 12.4 Repaso 12.5 Supuestos probabilísticos 12.6 Relación con mínimos cuadrados 12.7 Tarea", " Clase 12 Discriminante Lineal (LDA) 2 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 12.1 Aplicaciones El análisis discriminante lineal (LDA) de Fisher es un método utilizado en estadística, reconocimiento de patrones y aprendizaje estadístico para encontrar una combinación lineal de variables que caracteriza o separa dos o más clases de objetos o eventos. La combinación lineal resultante se puede utilizar como un clasificador lineal o, más comúnmente, para la reducción de dimensionalidad antes de usar otro método de clasificación. El análisis de discriminante lineal tiene entre sus posibles aplicaciones: Predicción de bancarrota: la predicción de bancarrota se basa en datos de en indicadores contables y otras variables financieras. El análisis discriminante lineal fue el primer método estadístico aplicado para explicar sistemáticamente qué empresas entraron en bancarrota vs. cuáles sobrevivieron. Marketing: el análisis discriminante solía utilizarse para determinar los factores que distinguen diferentes tipos de clientes y/o productos utilizando datos provenientes de encuestas u otras fuentes de datos. Estudios biomédicos: la principal aplicación del análisis discriminante en medicina es la evaluación del estado de gravedad de un paciente y el pronóstico del desenlace de la enfermedad. Por ejemplo, en un análisis retrospectivo, los pacientes se dividen en grupos según la gravedad de la enfermedad: leve, moderada y grave. Luego, se estudian los resultados de los análisis clínicos y de laboratorio para ver qué variables son toman valores diferentes entre los grupos estudiados. Usando estas variables, se construyen funciones discriminantes que ayudan a clasificar objetivamente la enfermedad en un futuro paciente en leve, moderada o severa. El análisis de discriminante lineal también se conoce como “análisis discriminante canónico”, o simplemente “análisis discriminante”. 12.2 Ejemplo: vinos Tenemos 13 concentraciones químicas que describen muestras de vino de tres cultivares. wine &lt;- read_csv(&quot;datos/wine.csv&quot;) wine %&gt;% head %&gt;% knitr::kable() Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids Proanthocyanins Color Hue Dilution Proline 1 14.2 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 1 13.2 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 1 13.2 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 1 14.4 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 1 13.2 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 1 14.2 1.76 2.45 15.2 112 3.27 3.39 0.34 1.97 6.75 1.05 2.85 1450 library(GGally) ggpairs(wine[2:6], columns = 1:5, title = &quot;&quot;, axisLabels = &quot;show&quot;, columnLabels = colnames(wine[2:6])) El propósito del análisis de discriminante lineal (LDA) en este ejemplo es encontrar las combinaciones lineales de las variables originales (las 13 concentraciones químicas) que proporcionen la mejor separación posible entre los grupos (variedades de vino) en nuestro conjunto de datos. Supongamos entonces que queremos separar los vinos por cultivar. Los vinos provienen de tres cultivares diferentes, por lo que el número de clases es \\(K = 3\\) y el número de variables predictoras es 13 (concentraciones de 13 componentes químicos, \\(p = 13\\)). El número máximo de funciones discriminantes útiles que pueden separar los vinos por cultivar es el mínimo entre \\(K-1\\) y \\(p\\), por lo que en este caso es el número de superficies de decisión es 2. Por lo tanto, podemos encontrar como máximo 2 funciones discriminantes útiles para separar los vinos por cultivar, utilizando las 13 variables de concentraciones químicas. Ajustamos el modelo de discriminante lineal: library(MASS) wine_lda &lt;- lda(Type ~ ., data = wine) Para obtener los valores de los pesos de las funciones discriminantes, podemos escribir: wine_lda #&gt; Call: #&gt; lda(Type ~ ., data = wine) #&gt; #&gt; Prior probabilities of groups: #&gt; 1 2 3 #&gt; 0.331 0.399 0.270 #&gt; #&gt; Group means: #&gt; Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids #&gt; 1 13.7 2.01 2.46 17.0 106.3 2.84 2.982 0.290 #&gt; 2 12.3 1.93 2.24 20.2 94.5 2.26 2.081 0.364 #&gt; 3 13.2 3.33 2.44 21.4 99.3 1.68 0.781 0.448 #&gt; Proanthocyanins Color Hue Dilution Proline #&gt; 1 1.90 5.53 1.062 3.16 1116 #&gt; 2 1.63 3.09 1.056 2.79 520 #&gt; 3 1.15 7.40 0.683 1.68 630 #&gt; #&gt; Coefficients of linear discriminants: #&gt; LD1 LD2 #&gt; Alcohol -0.40340 0.871793 #&gt; Malic 0.16525 0.305380 #&gt; Ash -0.36908 2.345850 #&gt; Alcalinity 0.15480 -0.146381 #&gt; Magnesium -0.00216 -0.000463 #&gt; Phenols 0.61805 -0.032213 #&gt; Flavanoids -1.66119 -0.491998 #&gt; Nonflavanoids -1.49582 -1.630954 #&gt; Proanthocyanins 0.13409 -0.307088 #&gt; Color 0.35506 0.253231 #&gt; Hue -0.81804 -1.515634 #&gt; Dilution -1.15756 0.051184 #&gt; Proline -0.00269 0.002853 #&gt; #&gt; Proportion of trace: #&gt; LD1 LD2 #&gt; 0.688 0.312 Esto significa que la primera función discriminante es una combinación lineal de las variables: \\[ -0.403 \\cdot \\mbox{Alcohol} + 0.165 \\cdot \\mbox{Malic} \\;+\\cdots+\\; -0.003 \\cdot \\mbox{Proline}. \\] Por conveniencia, el valor de cada función discriminante (por ejemplo, la primera función discriminante) se escala de modo que su media sea cero y su varianza sea uno. La “proporción de traza” que se imprime al final es una medida de la separación porcentual lograda por cada función discriminante. Por ejemplo, para estos datos de vinos obtenemos los mismos valores que acabamos de calcular (68.75% y 31.25%). Una buena forma de mostrar los resultados de un análisis de discriminante lineal es hacer un histograma apilado de los valores de la función discriminante para las diferentes clases. Podemos hacer esto usando la función ldahist(). Por ejemplo, para hacer un histograma apilado de los valores de la primera función discriminante: wine_pred &lt;- predict(wine_lda) ldahist(data = wine_pred$x[,1], g=wine$Type) Por lo tanto, investigamos si la segunda función discriminante separa esos cultivares, al hacer un histograma apilado de los valores de la segunda función discriminante: ldahist(data = wine_pred$x[,2], g=wine$Type) Concluimos que la segunda función discriminante distingue los vinos del Tipo 2 de los vinos de Tipo 1 y 3. Podemos obtener un diagrama de dispersión de las funciones discriminantes, etiquetando los puntos por clase: wine$LD1 &lt;- wine_pred$x[,1] wine$LD2 &lt;- wine_pred$x[,2] wine$Type &lt;- as.factor(wine$Type) ggplot(wine, aes(x = LD1, y = LD2, color = Type, pch = Type)) + geom_point() Notas: En el diagrama de dispersión de las dos primeras funciones discriminantes, podemos ver que los vinos de los tres cultivares están bien separados. La primera función discriminante (eje x) separa muy bien los cultivares 1 y 3, pero no separa perfectamente los cultivares 1 y 3, ni los cultivares 2 y 3. La segunda función discriminante (eje y) logra una separación bastante buena de los cultivares 1 y 3, y los cultivares 2 y 3, aunque no es totalmente perfecto. Para lograr una muy buena separación de los tres cultivares, sería mejor usar juntas la primera y la segunda función discriminante, ya que la primera función discriminante puede separar los cultivares 1 y 3 muy bien, y la segunda función discriminante puede separar los cultivares 1 y 2, y los cultivares 2 y 3, razonablemente bien. 12.3 Ejemplo: admisiones al MBA Se tienen datos de admisión para los solicitantes a las escuelas de posgrado en administración de negocios. El objetivo es usar los puntajes de GPA y GMAT para predecir la probabilidad de admisión (admitido, no admitido y en el límite). url &lt;- &#39;http://www.biz.uiowa.edu/faculty/jledolter/DataMining/admission.csv&#39; mba &lt;- read_csv(url) #&gt; Parsed with column specification: #&gt; cols( #&gt; GPA = col_double(), #&gt; GMAT = col_integer(), #&gt; De = col_character() #&gt; ) mba %&gt;% sample_n(10) %&gt;% knitr::kable() GPA GMAT De 3.26 664 admit 2.35 321 notadmit 2.36 399 notadmit 2.57 542 notadmit 2.85 381 notadmit 3.12 463 border 3.44 692 admit 3.47 552 admit 2.19 411 notadmit 2.51 412 notadmit Primero veamos un diagrama de dispersión de los datos: ggplot(mba, aes(x=GPA, y = GMAT, color = De)) + geom_point() Comencemos haciendo el análisis de discriminante lineal, observemos que en este caso tenemos 3 clases: m1 &lt;- lda(De ~ ., mba) m1 #&gt; Call: #&gt; lda(De ~ ., data = mba) #&gt; #&gt; Prior probabilities of groups: #&gt; admit border notadmit #&gt; 0.365 0.306 0.329 #&gt; #&gt; Group means: #&gt; GPA GMAT #&gt; admit 3.40 561 #&gt; border 2.99 446 #&gt; notadmit 2.48 447 #&gt; #&gt; Coefficients of linear discriminants: #&gt; LD1 LD2 #&gt; GPA 5.00877 1.8767 #&gt; GMAT 0.00857 -0.0145 #&gt; #&gt; Proportion of trace: #&gt; LD1 LD2 #&gt; 0.9673 0.0327 Para analizar podemos hacer una predicción para una nueva observación en específico: predict(m1, newdata = data.frame(GPA = 3.21, GMAT = 497)) #&gt; $class #&gt; [1] admit #&gt; Levels: admit border notadmit #&gt; #&gt; $posterior #&gt; admit border notadmit #&gt; 1 0.518 0.482 0.000356 #&gt; #&gt; $x #&gt; LD1 LD2 #&gt; 1 1.25 0.318 La predicción es muy ambigua. La razón es que con una sola función de discriminante no es posible separar los datos. La razón es que no se cumple el supuesto \\[ \\Sigma_k = \\Sigma\\quad \\mbox{para toda } k. \\] Usamos análisis discriminante cuadrático: m2 &lt;- qda(De ~ ., mba) m2 #&gt; Call: #&gt; qda(De ~ ., data = mba) #&gt; #&gt; Prior probabilities of groups: #&gt; admit border notadmit #&gt; 0.365 0.306 0.329 #&gt; #&gt; Group means: #&gt; GPA GMAT #&gt; admit 3.40 561 #&gt; border 2.99 446 #&gt; notadmit 2.48 447 Repetimos la predicción para el mismo punto: predict(m2, newdata = data.frame(GPA = 3.21, GMAT = 497)) #&gt; $class #&gt; [1] admit #&gt; Levels: admit border notadmit #&gt; #&gt; $posterior #&gt; admit border notadmit #&gt; 1 0.923 0.0769 0.000454 ¿Qué modelo es el mejor? Para responder a esta pregunta, evaluamos el análisis de discriminante lineal seleccionando aleatoriamente 60 de 85 estudiantes, estimando los parámetros en los datos y clasificando a los 25 estudiantes restantes de la muestra retenida. Repetimos esto 100 veces primero con LDA: n &lt;- 85 nt &lt;- 60 neval &lt;- n - nt rep &lt;- 100 set.seed(123456) calcula_error &lt;- function(i){ muestra &lt;- sample(1:n, nt) m1 &lt;- lda(De ~ ., mba[muestra,]) tablin &lt;- table(mba$De[-muestra],predict(m1,mba[-muestra,])$class) return((neval-sum(diag(tablin)))/neval) } merrlin &lt;- map_dbl(.x = 1:rep, .f = calcula_error) mean(merrlin) #&gt; [1] 0.0964 Ahora con QDA: calcula_error_Q &lt;- function(i){ muestra &lt;- sample(1:n, nt) m1 &lt;- qda(De ~ ., mba[muestra,]) tablin &lt;- table(mba$De[-muestra],predict(m1,mba[-muestra,])$class) return((neval-sum(diag(tablin)))/neval) } qerrlin &lt;- map_dbl(.x = 1:rep, .f = calcula_error_Q) mean(qerrlin) #&gt; [1] 0.0588 Logramos una tasa de clasificación errónea del 9% en LDA y aproximadamente del 6% en QDA. En este caso ambos métodos funcionan bien porque las dimensiones del problema son más menores: \\(p\\), \\(n\\), y \\(K\\) chicas. R también nos da algunas herramientas de visualización. Por ejemplo el paquete klaR: library(klaR) mba$De &lt;- as.factor(mba$De) partimat(formula = De ~ GMAT + GPA, data = mba, method = &quot;lda&quot;) partimat(formula = De ~ GMAT + GPA, data = mba, method = &quot;qda&quot;) 12.4 Repaso Se tienen datos \\(x_1, x_2, \\ldots, x_n \\in \\mathbb{R}^p\\) y \\(y_1,\\ldots,y_n\\in \\{1,2,\\ldots,K\\}\\) donde \\[ \\begin{eqnarray*} n &amp;=&amp; \\mbox{# observaciones} \\\\ p &amp;=&amp; \\mbox{# de covariables o predictores} \\\\ k &amp;=&amp; \\mbox{# clases que dividen a todos los datos.} \\end{eqnarray*} \\] Definimos \\[ m_k = \\dfrac{1}{N_k}\\sum_{i\\in C_k}x_i \\] donde $N_k=kC_kk Una medida de la variabilidad dentro de cada clase es \\[ s_k^2 = \\sum_{i\\in C_k}{(y_i - w^T x_i)^2} \\] porque el modelo está determinado por la ecuación \\(y(x)=w^Tx\\). Caso \\(k=2\\) El vector \\(w\\) lo interpretamos como el vector de pesos que determinan el discriminante lineal. La clasificación subsecuente se hace de tal forma que si \\(y(x)&gt;y_0\\) se clasifica a la observación como perteneciente a la clase 1 y en otro caso a la clase 2. Definimos el criterio de Fisher como \\[ J(w) = \\dfrac{w^T(m_2 - m_1)}{s_1^2 + s_2^2}. \\] Buscamos \\[ \\begin{eqnarray*} \\mbox{max} &amp;&amp; J(w) \\\\ \\mbox{s.a.} &amp;\\;&amp; \\| w \\| = 1. \\end{eqnarray*} \\] Idea: En general, necesitamos encontrar la dirección de \\(w\\) y pensar en que luego es posible normalizar. Definimos las matrices \\[ S_W = \\sum_{i\\in C_k}(x_i - m_1)(x_i-m_1)^T + \\sum_{i\\in C_k}(x_i - m_2)(x_i - m_2)^T \\] y \\[ S_B = (m_2 - m_1)(m_2 - m_1)^T \\] Se puede ver que \\[ J(w) = \\dfrac{w^T S_B w}{w^T S_W w}. \\] Si \\(x\\) es un vector y \\(A\\) es una matriz, entonces el gradiente de una forma cuadrática \\[ \\dfrac{1}{2}x^T A x + b^T x + c \\] es \\[ Ax + b. \\] Por lo tanto, \\[ \\nabla J(w) = w^T S_B w \\cdot \\nabla_w(w^T S_W w) - w^T S_W w \\cdot \\nabla_w(w^T S_B w), \\] por lo que \\(J(w)\\) se maximiza cuando \\[ \\begin{eqnarray*} (w^T S_B w) S_W w &amp;=&amp; (w^T S_W w)S_B w \\\\ S_W w&amp;=&amp; \\left(\\dfrac{w^T S_W w}{w^T S_B w}\\right) S_B w \\\\ &amp;=&amp; \\dfrac{w^T S_W w}{w^T S_B w} (m_2-m_1)(m_2 - m_1)^T w\\\\ &amp;=&amp; \\left\\{\\left(\\dfrac{w^T S_W w}{w^T S_B w}\\right)(m_2 - m_1)^T w\\right\\}(m_2-m_1). \\end{eqnarray*} \\] Por lo tanto, \\(w\\) es tal que \\[ w = c \\cdot S_W ^{-1}(m_2 - m_1) \\] donde \\(c\\) es una constante tal que \\(\\|w\\|_2^2=1\\). 12.5 Supuestos probabilísticos Definimos como \\(\\pi_k\\) la probabilidad inicial de la pertenencia a la clase \\(k\\), es decir, \\[ \\pi_k = P(y_i = k). \\] En la literatura las \\(\\pi_i\\)’s se conocen como probabilidades a priori. Además podemos definir una dsitribución de probabilidad para \\(x\\) para cada clase \\(k\\) \\[ f_k(x) = P(x|k). \\] Por el teorema de Bayes \\[ p(k|x) = \\dfrac{f_k(x)\\pi_k}{\\sum_{l=1}^K f_l(x)\\pi_l}. \\] A las probabilidades finales de cada clase \\(k\\) dada una observación \\(x\\) se les conoce comúnmente como probabilidades posteriores. En el análisis de discriminante lineal suponemos que \\(p(x|k)\\) es la densidad normal \\[ f_k(x) = \\dfrac{1}{(2\\pi)^{p/2}|\\Sigma_k|^{1/2}}\\exp\\left(-\\dfrac{1}{2}(x-\\mu_k)^T\\Sigma_k ^{-1} (x-\\mu_k)\\right). \\] En LDA suponemos que \\(\\Sigma_k = \\Sigma\\) para toda \\(k\\). A este supuesto le llamamos supuesto de homogeneidad. Para comparar la pertenencia de clases podemos ver el logaritmo del cociente entre las clases \\(k\\) y \\(l\\): \\[ \\begin{eqnarray*} \\log\\dfrac{p(k|x)}{p(j|x)} &amp;=&amp; \\log\\dfrac{f_k(x)}{f_l(x)} + \\log\\left(\\dfrac{\\pi_k}{\\pi_l}\\right)\\\\ &amp;=&amp; \\log\\left(\\dfrac{\\pi_k}{\\pi_l}\\right) -\\dfrac{1}{2}(\\mu_k - \\mu_l)^T\\Sigma^{-1}(\\mu_k - \\mu_l) + x^T\\Sigma^{-1}(\\mu_k - \\mu_l). \\end{eqnarray*} \\] Ésta es una ecuación lineal en \\(x\\). Esto implica que la superficie de decisión (que separa a las clases) es un hiperplano de dimensión \\(p\\). Si dividimos \\(\\mathbb{R}^p\\) entre regiones que separan las clases \\(1,2,\\ldots,K\\), estas regiones están determinadas por hiperplanos. Definimos las funciones de discriminante lineal como \\[ f_k(x) = x^T \\Sigma ^{-1} \\mu_k -\\dfrac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k + \\log(\\pi_k). \\] Estos los estimamos por máxima verosimilitud y es equivalente a la vista antes \\[ \\hat{\\pi}_k= \\dfrac{N_k{}}N \\] \\[ \\hat{\\mu}_k = \\dfrac{1}{N_k}\\sum_{i\\in C_k}x_i \\] \\[ \\hat{\\Sigma} = \\dfrac{1}{N-K} \\sum_{k=1}^{K}\\sum_{i\\in C_k}(x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T \\] Si \\(\\Sigma_k\\) no es igual a \\(\\Sigma\\) para todas las clases entonces definimos las funciones de discriminante cuadráticas (QDA) como \\[ f_k(x) = -\\dfrac{1}{2}\\log|\\Sigma_k| - \\dfrac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k) + \\log(\\pi_k). \\] Las superficies de decisión son ecuaciones cuadráticas en \\(\\mathbb{R}^p\\). Nota Las estimaciones se hacen de manera similar que para LDA excepto que en este caso se deben estimar todas las \\(\\Sigma_k\\)`s por separado. El número de parámetros puede aumentar sustancialmente. 12.6 Relación con mínimos cuadrados Vimos cómo utilizar regresión lineal para encontrar regiones de discriminante lineal. Codificamos \\(Y\\) de la siguiente forma: \\[ y_i=(0,\\ldots,0,N/N_1,0,\\ldots,0) \\] donde la posición distinta de \\(0\\) es la \\(i\\)-ésima posición correspondiente a la observación cuando está en la clase 1, y cuando está en la clase 2 es \\[ y_i=(0,\\ldots,0,-N/N_2,0,\\ldots,0). \\] El error como suma de cuadrados es: \\[ \\dfrac{1}{2}\\sum_{i=1}^n{(w^Tx_i+w_0-y_i)^2}. \\] Derivando con respecto a \\(w_0\\) y \\(w\\) obtenemos: \\[ \\begin{eqnarray*} \\sum_{i=1}^n{(w^Tx_i+w_0-y_i)} &amp;=&amp; 0\\\\ \\sum_{i=1}^n{(w^Tx_i+w_0-y_i)\\cdot x_i} &amp;=&amp; 0 \\end{eqnarray*} \\] Sustituyendo las \\(y_i\\)`s obtenemos que \\[ w_0 = -w^Tm \\] donde \\[ \\sum_{i=1}^n{y_i} = N_1\\dfrac{N}{N_1} - N_2\\dfrac{N}{N_2}, \\] y \\(m\\) es la media del conjunto de datos y está dado por \\[ m = \\dfrac{1}{N}\\sum_{i=1}^N x_i = \\dfrac{1}{N}(N_1 m_1 + N_2 m_2). \\] Se puede demostrar (tarea) que: \\[ \\left(S_W + \\dfrac{N_1N_2}{N}S_B\\right)w=N(m_1 - m_2). \\] Notemos que \\(S_B w\\) está en la dirección de \\(m_2-m_1\\) porque \\[ S_B = (m_2 - m_1)(m_2 - m_1)^T, \\] y entonces \\(w\\) es proporcional a \\(S_W^{-1}(m_2 - m_1)\\). Por lo tanto, el resultado obtenido por mínimos cuadrados es equivalente a la dirección de discriminante lineal, ignorando factores escalares que no son relevantes. Por lo tanto, el vector de pesos coincide con el encontrado mediante el criterio de Fisher. El umbral \\(w_0\\) es tal que si \\(y(x) = w^T(x-m)&gt; 0\\), entonces se clasifica en la clase 1, y en otro caso, se clasifica en la clase 2. Nota: La extensión para múltiples clases es análoga a lo que ya hemos visto. 12.7 Tarea Utilizando las definiciones de las matrices de covarianza intraclase \\(S_W\\) e interclase \\(S_B\\) y que \\[ w_0 = -w^tm, \\] donde \\[ m = \\dfrac{1}{N}\\sum_{i=1}^N x_i = \\dfrac{1}{N}(N_1 m_1 + N_2 m_2), \\] demuestra que \\[ \\sum_{i=1}^n{(w^Tx_i+w_0-y_i)\\cdot x_i}= 0 \\] se puede escribir de la forma \\[ \\left(S_W + \\dfrac{N_1N_2}{N}S_B\\right)w=N(m_1 - m_2). \\] "],
["componentes-principales-1.html", "Clase 13 Componentes Principales 1 13.1 Motivación 13.2 Formulación de máxima varianza 13.3 Formulación de error mínimo 13.4 Aplicaciones de PCA 13.5 Tarea", " Clase 13 Componentes Principales 1 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) library(RColorBrewer) 13.1 Motivación Muchos conjuntos de datos tienen la propiedad que los puntos caen en un variedad de dimensión mucho menor a la dimensión original de los datos. Para entender esta idea consideremos una base de datos consrtuida con uno de los dígitos de la base de datos mnist, esta imagen esta representada por una matriz de \\(64 \\times 64\\) pixeles, ahora insertamos este dígito en una matriz más grande de \\(100 \\times 100\\) agregando espacio en blanco y variamos de manera aleatoria la orientación y ubicación del dígito. Cada una de las imágenes resultantes está representada por un punto en el espacio de dimensión \\(100 \\times 100 = 10,000\\); sin embargo, en una base de datos construida de esta manera solamente hay 3 grados de libertad de variabilidad, estas corresponden a las rotaciones, trasalación vertical y traslación horizontal. Para datos reales habrá más grados de libertad debido por una parte a escalamiento y por otra habrá múltiples grados adicionales debidas a deformaciones debidas a la variabilidad en la escritura de un individuo y entre individuos. Aún así, la la dimensionalidad de los grados de libertad es mucho menor que la de los datos completos. if(!file.exists(&quot;datos/mnist/train-images-idx3-ubyte&quot;)){ mnistR::downloadMNIST(dest = &quot;datos/mnist/&quot;) } mnist &lt;- mnistR::loadMNIST(dest = &quot;datos/mnist/&quot;)[[&quot;train&quot;]] data_tres &lt;- mnist %&gt;% filter(y == 3) %&gt;% select(-y) par(mfrow=c(1,5)) imageD &lt;- function(vec, main = NULL){ mat_digit &lt;- matrix(vec, nrow = 28)[, 28:1] mat &lt;- matrix(unlist(mat_digit), nrow = 28) image(mat, col = brewer.pal(5, &quot;GnBu&quot;), xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, bty = &quot;n&quot;, asp = 1, main = main) } for(i in sample(1:nrow(data_tres), 5)){ imageD(data_tres[i, ]) } El análisis de componentes principales (PCA) es una técnica que se utiliza con distintos objetivos: Reducción de dimensionalidad. Compresión de información con pérdida (lossy). Extracción de características o (features). Visualización de datos. Podemos ver PCA desde dos puntos de vista que nos llevan al mismo resultado: PCA se puede definir como una proyección de los datos en un espacio de dimensión menor (conocido como subespacio principal), tal que la varianza de los datos proyectados es máxima. PCA se puede definir como la proyección lineal que minimiza el costo medio de proyección, donde el costo promedio es la distancia media al cuadrado entre los puntos y sus proyecciones. 13.2 Formulación de máxima varianza Consideremos un vector de observaciones \\((y^1,...,y^n)\\) donde \\(y_i\\) es de dimensión \\(d\\). Nuestro objetivo es proyectar los datos en un espacio de dimensión \\(M&lt;D\\) maximizando la varianza de la proyección. Comencemos considerando la proyección en un espacio de dimensión uno, denotamos la dirección de este espacio por \\(u_1\\) y por conveniencia usamos un vector unitario (\\(u_1^Tu_1=1\\)). La proyección de cada punto \\(y_i\\) es un escalar (pues \\(M=1\\)) cuyo valor es \\(u_1^Ty_i\\). La media de los datos proyectados es \\(u_1^T\\bar{y}\\) donde \\[ \\bar{y}=\\frac{1}{N}\\sum_{i=1}^N y_i \\] por tanto la varianza de los datos proyectados es \\[ \\frac{1}{N}\\sum_{i=1}^N (u_1^Ty_i-u_1^T\\bar{y})^2=u_1^TSu_1 \\] donde S es la matriz de covarianzas de los datos: \\[ S=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\bar{y})(y_i-\\bar{y})^T. \\] Ahora maximizamamos la varianza de la proyección respecto a \\(u_1\\): \\[ \\mbox{argmax}_{u_1}u_1^TSu_1 + \\lambda_1(1-u_1^Tu_1) \\] Derivando encontramos un punto estacionario en \\[ Su_1=\\lambda_1u_1 \\] por lo que \\(u_1\\) debe ser un eigenvector de S, notamos también que la varianza esta dada por: \\[ u_1^TSu_1=\\lambda_1, \\] y por tanto, la varianza será máxima si igualamos \\(u_1\\) con el mayor eigenvector de \\(S\\), que llamamos primer componente principal. Si elegimos \\(M&gt;1\\), definimos los componentes de manera incremental, en cada paso seleccionamos una nueva dirección eligiendo cada nueva dirección como aquella que maximiza la varianza de la proyección sujeta a ser ortogonal a las direcciones (componentes) ya elegidos. Esto resulta en que la proyección lineal óptima para la cual la varianza de los datos proyectados es máxima esta definida por el conjunto de \\(M\\) eigenvectores \\(u_1,...,u_M\\) de la matriz de covarianzas \\(S\\). 13.3 Formulación de error mínimo Ahora discutimos el segundo punto de vista de PCA. Sea \\((u_1,...,u_D)\\) una base ortonormal de vectores, esto es \\(u_i^Tu_j = 0\\) para toda \\(i\\) distinta de \\(j\\) y \\(u_i^Tu_i = 1\\). Como esta es una base de \\(R^D\\) podemos expresar los datos observados como \\[ y_i=\\sum_{j=1}^D \\alpha_{ij}u_j \\] Esto corresponde a una rotación en el sistema de coordenadas. Utilizando la propiedad ortonormal obtenemos \\(\\alpha_{ij}={y_i} ^Tu_j\\), por tanto: \\[ y_i=\\sum_{j=1}^D ({y_i} ^Tu_j) u_j \\] Ahora, como buscamos aproximar este punto (\\(y_i\\)) usando una representación que involucre un número de variables \\(M&lt;D\\), la subespacio de dimensión \\(M\\) se puede representar usando los primeros \\(M\\) vectores de la base, de tal manera que podemos aproximar cada punto como: \\[ \\hat{y}_i=\\sum_{j=1}^M x_{ij}{u_j} + \\sum_{j=M+1}^D b_j u_j \\] donde los valores \\(x_{ij}\\) dependen del dato que estamos proyectando y las \\(b_j\\) son constantes para todos los datos. Buscamos \\((u_1,...,u_D)\\), \\(x_{ij}\\) y \\(b_j\\) tal que se minimice la distorsión introducida por la reducción de dimensión, donde definimos la distorsión como la distancia al cuadrado entre el punto original \\(y_i\\) y la aproximación \\(\\hat{y}_i\\) promediada sore todos los puntos de la base de datos: \\[ J=\\frac{1}{N}\\sum_{j=1}^N(y_j-\\hat{y}_j)^T(y_j-\\hat{y}_j) \\] La minimización (derivar e igualar a cero) nos lleva a: \\(x_{ij}=y_i^Tu_j\\), con \\(j=1,...,M\\) \\(b_{j}=\\bar{y}^Tu_j\\), con \\(j=M+1,...,D\\) Sustituyendo \\(x_{ij}\\) y \\(b_j\\) en \\(y_i=\\sum_{j=1}^D ({y_i} ^Tu_j) u_j\\) llegamos a \\[ y_i-\\hat{y}_i=\\sum_{j=M+1}^D [(y_n-\\bar{y})^Tu_j]u_j \\] y vemos que el error mínimo ocurre en la proyección ortogonal sobre el subespacio generado por \\(\\{u1,...,u_M\\}\\). Usando lo anterior obtenemos \\[ J=\\frac{1}{N}\\sum_{j=1}^N \\sum_{i=M+1}^D [(y_n-\\bar{y})^Tu_j]^T[(y_n-\\bar{y})^Tu_j] \\] \\[ J=\\frac{1}{D}\\sum_{j=1}^D u_i^TSu_i \\] Aún falta minimizar \\(J\\) respecto a \\(u_i\\), esta es una minimización con la restricción \\(u_i^Tu_i=1\\), si derivamos respecto a \\(u_i\\) obtenemos \\[ Su_i=\\lambda_i u_i \\] por lo que cualquier eigenvector de S corresponde a un punto crítico. Si todos corresponden a un punto crítico ¿cómo elegimos? Notemos que si sustituimos la solución de \\(u_i\\) en J obtenemos \\[ J=\\sum_{j=M+1}^D \\lambda_j \\] por lo que para obtener el mínimo valor de \\(J\\) hay que seleccionar los \\(D-M\\) eigenvectores corresponidientes a los menores eigenvalores y por tanto los eigenvectores que definen el subespacio principal corresponden a los \\(M\\) eigenvectores mayores. 13.4 Aplicaciones de PCA Veamos un par de aplicaciones de PCA, comenzaremos con compresión de imágenes y luego examinaremos PCA como preprocesamiento. 13.4.1 Compresión de datos Veamos un ejemplo de PCA para compresión de información usando la base de datos de mnist, en particular veamos los dígitos tres. dim(data_tres) #&gt; [1] 6131 784 Como cada eigenvector es un vector en el espcio original de \\(D\\) dimensiones podemos representarlos como imágenes. tres_mean &lt;- apply(data_tres, 2, mean) S &lt;- cov(data_tres) eigen_S &lt;- eigen(S) lambda &lt;- eigen_S$values u &lt;- eigen_S$vectors par(mfrow=c(1,5)) imageD(tres_mean) for(i in 1: 4){ imageD(u[, i]) } Podemos ver el resto de los eigenvalores en la gráfica de abajo. Graficamos también la medida de distorsión \\(J\\) asociada a la elección del número de componentes \\(M\\) (dada por la suma de los eigenvalores \\(M+1\\) a \\(D\\)). D &lt;- length(lambda) J &lt;- sapply(1:D, function(i){sum(lambda[i:D])}) par(mfrow=c(1,2)) plot(lambda, type = &quot;l&quot;) plot(J, type = &quot;l&quot;) Si vemos las fórmulas de arriba podemos escribir el vector de aproximación correspondiente a una observación. \\[ \\begin{eqnarray*} \\hat{y}_i&amp;=&amp;\\sum_{j=1}^M x_{ij}{u_j} + \\sum_{j=M+1}^D b_j u_j\\\\ &amp;=&amp;\\sum_{j=1}^M y_i^Tu_j{u_j} + \\sum_{j=M+1}^D \\bar{y}^Tu_j u_j\\\\ &amp;=&amp;\\bar{x} + \\sum_{j=1}^M (y_i^Tu_j-\\bar{x}^Tu_j)u_j \\end{eqnarray*} \\] donde usamos: \\[ \\bar{x}=\\sum_{j=1}^D (\\bar{x}^Tu_j)u_j. \\] La compresión está en que reemplazamos cada vector de observaciones de dimensión \\(D\\) (\\(y_i\\)) por un vector de dimensión \\(M\\). La siguiente figura muestra la compresión para distintos valores de \\(M\\) del primer dígito de la base de datos. tres_1 &lt;- data_tres[3, ] par(mfrow=c(1,5)) imageD(tres_1) for(M in c(1, 10, 50, 300)){ u_M &lt;- u[, 1:M] y_M &lt;- as.numeric(tres_1) %*% u_M y_approx &lt;- tres_mean + y_M %*% t(u_M) imageD(y_approx) } 13.4.2 Ejemplo: compresión de una imagen 13.4.2.1 Preprocesamiento Otra aplicación de componentes principales es preprocesamento, en este caso el objetivo no es reducción de dimensión sino la transformación de un conjunto de daros con el fin de estandarizar algunas de sus propiedades. Esto puede ser importante para el correcto funcionamiento de algoritmos o métodos que se desean usar después. Veamos los datos faithful de erupciones del volcán Old Faithful. head(faithful) %&gt;% knitr::kable() eruptions waiting 3.60 79 1.80 54 3.33 74 2.28 62 4.53 85 2.88 55 Notamos que el tiempo entre erupciones es de un orden de magnitud mayor que la duración de la erupción. Por ejemplo, si quisiéramos hacer k-medias (siguiente tema) sería natural estandarizar los datos. Sin embargo, con PCA podemos normalizar los datos para tener cero media y covarianza unitaria, de tal manera que la correlación entre distintas variables es cero. Para hacer esto escribimos la ecuación de eigenvectores como \\[ SU=UL \\] donde \\(L\\) es una matriz diagonal con los elementos \\(\\lambda_i\\) y \\(U\\) es una matriz ortogonal cuyas columnas son los vectores \\(u_i\\). Entonces, para cada observación \\(y_i\\) definimos su valor transformado \\[ z_i=L^{-1/2}U^T(y_i-\\bar{y}) \\] es claro que el conjunto \\((z_1,...,z_N)\\) tiene media cero, veamos ahora la covarianza: \\[ \\frac{1}{N}\\sum_{j=1}^Nz_jz_j^T=\\frac{1}{N}\\sum_{j=1}^NL^{-1/2}U^T(y_j-\\bar{y})(y_j-\\bar{y})^TUL^{-1/2} \\] \\[ =L^{-1/2}U^TSUL^{-1/2}=L{-1/2}LL^{-1/2}=I \\] Esta operación se conoce como whitening o sphereing. 13.5 Tarea Comprime una imagen utilizando análisis de componentes principales. Para esto haz lo siguiente: Descarga una imagen en formato jpg o jpeg y lee la imagen en R utilizando el siguiente código: library(jpeg) library(pixmap) nick &lt;- readJPEG(&quot;figuras/nick_bateman.jpg&quot;, native = FALSE)[,,1] pr &lt;- pixmapGrey(nick) #&gt; Warning in rep(cellres, length = 2): &#39;x&#39; is NULL so the result will be NULL plot(pr) Haz una gráfica de los primeros 100 eigenvalores de la matriz de varianzas y covarianzas. ¿Cuántas componentes Completa el siguiente código para comprimir la imagen con \\(M=1, 5, 10, 40, 50\\) componentes principales. \\[ S = X^TX = P DP^T \\] \\[ \\Sigma = D^{1/2} \\] \\[ X = U \\Sigma V^T \\] S &lt;- %*% eigen_S &lt;- eigen(S) P &lt;- eigen_S$vectors lambda &lt;- eigen_S$values sqrt_lambda &lt;- sqrt(lambda) sigma &lt;- diag(sqrt_lambda, nrow=, ncol = ) V = P sigma_1 &lt;- solve(sigma) U = %*% V %*% X = U %*% sigma %*% t(V) for(M in c(1, 5, 10, 40, 50)){ sigma_M &lt;- sigma[1:M,1:M] sigma_Mk &lt;- diag(0,,) sigma_Mk[1:M,1:M] &lt;- sigma_M approx &lt;- U %*% %*% t(V) pr &lt;- pixmapGrey(approx) plot(pr) } Considera los datos de erupciones de volcanes. library(tidyverse) ggplot(faithful, aes(x = eruptions, y = waiting)) + geom_point() faith_pca &lt;- prcomp(faithful, scale. = TRUE) faithful$pc1 &lt;- faith_pca$x[, 1] faithful$pc2 &lt;- faith_pca$x[, 2] ggplot(faithful, aes(x = pc1, y = pc2)) + geom_point() Implementa whitening en los datos faithful. Compara las gráficas de los datos crudos y preprocesados. "],
["componentes-principales-2.html", "Clase 14 Componentes Principales 2 14.1 PCA probabilístico y Análisis de Factores 14.2 Análisis de factores (descripción tradicional) 14.3 Tarea", " Clase 14 Componentes Principales 2 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 14.1 PCA probabilístico y Análisis de Factores La formulación de PCA esta fundadda en una proyección lineal de los datos sobre un subespacio de dimensión menor. En esta sección veremos que PCA también se puede expresar como la solución de máxima verosimilitud de en un modelo probabilístico de variable latente. El PCA probabilístico y el análisis de factores tienen las siguientes propiedades deseables: Representan una distribución Gaussiana con restricciones en el que el número de parámetros se puede restringir, mientras que podemos capturar las correlaciones dominantes de los datos. En general una distribución Gaussiana multivariada tiene \\(p(p+1)/2\\) parámetros independientes en la matriz de covarianzas por lo que el número de parámetros crece de manera cuadrática con \\(p\\). Por otra parte si restringimos a una matriz de covarianzas diagonal tenemos solamente \\(p\\) parámetros pero no podemos entender las correlaciones. PCA probabilístico (y AF) es un punto medio en el que las \\(q\\) correlaciones más fuertes se pueden capturar mientras que el número de parámetros crece de manera lineal con \\(p\\). En el caso de CPP con \\(q\\) componentes: \\(p\\cdot q + 1 - q\\cdot(q-1)/2\\). Podemos derivar un algoritmo EM para CPP que es eficente computacionalmente en situaciones en los que nos interesa calcular pocas componentes. La combinación de un modelo probabilísitico y el algoritmo EM nos permite tratar con datos faltantes en la base de datos. La existencia de la verosimilitud nos permite comparar modelos. Por ejemplo podemos hacer validación cruzada para elegir el número de componentes/factores que ajustan mejor a los datos. Podemos utilizar el modelo para generar muestras de la distribución. Para formular PCA probabilístico introducimos una variable latente \\(X\\) que corresponde al subespacio de componentes principales, suponemos \\(X\\sim N(0, I)\\). Por otra parte, la distribución de la variable aleatoria observada \\(Y\\) condicional a la variable latente \\(X\\) es \\(Y|X\\sim N(Wx+\\mu, \\sigma^2I\\) Veremos que las columnas de \\(W\\) (dimensión \\(D\\times M\\)) generan un subsepacio que correponde al subespacio de componentes principales. El siguiente esquema explica el modelo PCA probabilístico desde el punto de vista generativo. Desde este enfoque vemos que primero selecciona aleatoriamente un valor de la variable latente (\\(x\\)) y después muestreamos el valor observado condicional a la variable latente, en particular la variable obsevada (de dimensión \\(D\\)) se define usando una transformación lineal del espacio latente mas ruido Gaussiano aleatorio: \\[y=Wx + \\mu + \\epsilon\\] donde \\(x\\sim N(0, I)\\) de dimensión \\(M\\) y \\(\\epsilon \\sim N(0, \\sigma^2I)\\) de dimensión \\(D\\). Ahora, si queremos usar máxima verosimilitud para estimar \\(W\\), \\(\\mu\\) y \\(\\sigma^2\\), necesitamos una expresión para la distribución marginal de la variable observada: \\[p(y)=\\int p(y|x)p(x)dx\\] dado que este corresponde a un modelo Gaussiano lineal su distribución marginal es nuevamente Gaussiana con media \\(\\mu\\) y matriz de covarianzas \\(C=WW^T+\\sigma^2I.\\) Entonces, la distribución \\(p(y)\\) depende de los parámetros \\(W\\), \\(\\mu\\) y \\(\\sigma^2\\); sin embargo hay una redundancia en la parametrización que corresponde a rotaciones en el espacio de coordenadas de las variables latentes. Para ver esto consideremos \\(Q\\) una matriz ortonormal de dimensión \\(D \\times D\\) (\\(Q\\) es una matriz de rotación), \\[Q^T Q = Q Q^T = I\\] Al observar la igualdad \\(C=WW^T+\\sigma^2I\\), notamos que no existe una única \\(W\\) que la satisfaga pues si definimos \\(\\tilde{W}=WQ\\) tenemos que \\[\\tilde{W}\\tilde{W}^T=WQQ^TW^T=WW^T\\] y por tanto \\(C=\\tilde{W}{W}^T+\\sigma^2I\\). Este es un aspecto que consideraremos más a fondo en la parte de estimación. 14.1.0.1 Máxima verosimilitud Consideramos la determinación de los parámetros usando máxima verosimilitud: \\[ \\begin{aligned} \\log p(y)&amp;=\\sum_{i=1}^N\\log p(y_j)\\\\ &amp;=-\\frac{ND}{2}-\\frac{N}{2}\\log(2\\pi)\\log|C| -\\frac{1}{2}\\sum_{j=1}^N(y_j-\\mu)^TC^{-1}(y_j-\\mu) \\end{aligned} \\] Derivando e igualando a cero obtenemos \\(\\hat{\\mu}=\\bar{y}\\), la maximización con respecto a \\(W\\) y \\(\\sigma^2\\) es más difícil pero tiene forma cerrada (Tipping y Bishop 1999). \\[\\hat{W}=U_{M}(L_M-\\sigma^2I)^{1/2}R\\] donde \\(U_{M}\\) es una matriz de dimensión \\(D \\times M\\) cuyas columnas corresponden a los \\(M\\) eigenvectores asociados a los mayores eigenvalores de la matriz de covarianzas \\(S\\). La matriz \\(L\\) de dimensión \\(M \\times M\\) esta conformada por los eigenvalores correspondientes. Por último, R res cualquier matriz ortonormal de dimensión \\(M \\times M\\). Suponemos que los eigenvectores están ordenados en orden decreciente de acuerdo a sus eigenvalores correspondientes \\(u_1,...,u_M\\), en este caso las columnas de \\(W\\) definen el subespacio de PCA estándar. Por su parte, la solución de máxima verosimilitud para \\(\\sigma^2\\) es: \\[\\hat{\\sigma^2}=\\frac{1}{D-M}\\sum_{j=M+1}^D \\lambda_j\\] notemos que \\(\\hat{\\sigma}^2\\) es la varianza promedio asociada a las dimensiones que no incluimos. Ahora, como R es ortogonal se puede interpretar como una matriz de rotación en el espacio de variables latentes. Por ahora, pensemos \\(R=I\\) notamos que las columnas de \\(W\\) son los vectores de componentes principales escalados por los parámetros de varianza \\(\\lambda_i-\\sigma^2\\), para ver la interpretación notemos que en la suma de Gaussianas independientes las varianzas son aditivas. Por tanto, la varianza \\(\\lambda_i\\) en la dirección de un eigenvector \\(u_i\\) se compone de la contribución \\((\\lambda_i-\\sigma^2)\\) de la proyección del espacio latente (varianza 1) al espacio de los datos a través de la columna correspondiente de \\(W\\) mas la contribución del ruido con varianza isotrópica \\(\\sigma^2\\). 14.1.0.2 Observaciones El método convencional de PCA se suele describir como una proyección de los puntos en un espacio de dimensión \\(D\\) en un subespacio de dimensión \\(M\\). PCA probabilístco se expresa de manera más natural como un mapeo del espacio latente al espacio de los datos observados. Una función importante de PCA probabilítico es definir una distribución Gaussiana multivariada en donde el número de grados de libertad se puede controlar al mismo tiempo que podemos capturar las correlaciones más importantes de los datos. PCA convencional corrresponde al límite \\(\\sigma^2 \\to 0\\) PCA probabilístico se puede escribir en términos de un espacio latente por lo que la implementación del algoritmo EM es una opción natural. En casos donde \\(M&lt;&lt;D\\) la estimación mediante EM puede ser más eficiente. Debido a que tenemos un modelo probabilitico para PCA podemos trabajar con faltantes (MCAR y MAR) marginalizando sobre la distribución de los no observados. El manejo de faltantes es otra ventaja de la implementación EM. El algoritmo EM se puede extender al caso de Análisis de factores para el cuál no hay una solución cerrada. 14.1.1 Análisis de factores El análisis de factores es muy similar a PCA probabilístico, la diferencia radica en que en la distribución condicional de \\(Y|X\\) la matriz de covarianza se supone diagonal en lugar de isotrópica: \\[Y|X \\sim N(Wx + \\mu, \\Psi)\\] donde \\(\\Psi\\) es una matriz diagonal de dimensión \\(D \\times D\\). Al igual que en PCA probabilístico, el modelo de FA supone que las variables observadas son independientes dado las latentes. En escencia el análisis de factores está explicando la estructura de covarianza observada representando la varianza independiente asociada a cada variable en la matriz \\(W\\) y capturando la varianza compartda en \\(W\\). La distribución marginal de las variables observadas es \\(X\\sim N(\\mu, C)\\) donde \\[C=WW^T+\\Psi.\\] De manera similar a PCA probabilístico el modelo es invariante a rotaciones en el espacio latente. 14.2 Análisis de factores (descripción tradicional) Trataremos ahora con análisis de factores, los modelos que veremos se enfocan en variables observadas y latentes continuas. La idea esencial del análisis de factores es describir las relaciones entre varias variables observadas (\\(Y=Y_1,...,Y_p\\)) a través de variables latentes (\\(X_1,...,X_q\\)) donde \\(q &lt; p\\). Como ejemplo consideremos una encuesta de consumo de hogares, donde observamos el nivel de consumo de \\(p\\) productos diferentes. Las variaciones de los componentes de \\(Y\\) quizá se puedan explicar por 2 o 3 factores de conducta del hogar, estos podrían ser un deseo básico de comfort, o el deseo de alcanzar cierto nivel social u otros conceptos sociales. Es común que estos factores no observados sean de mayor interés que las observaciones en si mismas. En la gráfica inferior vemos un ejemplo en educación donde las variables vocab, reading, maze,… corresponden a las variables observadas mientras que \\(X_1\\) y \\(X_2\\) son las variables latentes. Observamos que añadir estructura al problema resulta en una simplificación del modelo. En ocasiones, el análisis de factores se utiliza como una técnica de reducción de dimensión que esta basada en un modelo. Idealmente, toda la información en la base de datos se puede reproducir por un número menor de factores. 14.2.1 El modelo Sea \\(Y = (Y_1,...,Y_p)^T\\) un vector de variables aleatorias observables donde todas las variables son cuantitativas. Supongamos que cada \\(Y_j\\) en \\(Y\\) (\\(j=1,...,p\\)) satisface: \\[Y_j = \\sum_{k=1}^K \\lambda_{jk} X_k + u_j\\] donde * \\(X_k\\) son los factores comunes (variables aleatorias continuas no observables). \\(u_j\\) son errores (aleatorios). \\(\\lambda_{jk}\\) son las cargas de la variable \\(j\\) en el factor \\(k\\), (parámetros). En notación matricial el modelo se escribe: \\[Y_{p\\times 1} = \\Lambda_{p\\times K} X_{K\\times 1} + U_{p\\times 1}\\] donde \\(\\Lambda, X\\) y \\(U\\) no son observadas, únicamente observamos \\(Y\\). Adicionalmente, tenemos los siguientes supuestos: \\(X \\perp U\\), esto es, los errores y los factores son independientes. \\(E(X)=E(U)=0\\). \\(Cov(X) = I_k\\) (modelo ortogonal de factores) ésto se ve en la gráfica pues no hay arcos que unan a \\(X_1\\) y \\(X_2\\). \\(Cov(U) = \\Psi\\), donde \\(\\Psi\\) es una matriz diagonal (\\(p \\times p\\)). Típicamente, se asume que \\(U\\) y \\(X\\) son Normales multivariadas. ¿Cómo vemos que \\(Y_i \\perp Y_j|X\\) Lo que buscamos es explicar la relación entre las variables observadas a través de las variables latentes, las relaciones que buscamos explicar están resumidas en la matriz de varianzas y covarianzas. En nuestro ejemplo la matriz es la siguiente: ability.cov$cov #&gt; general picture blocks maze reading vocab #&gt; general 24.64 5.99 33.5 6.02 20.75 29.70 #&gt; picture 5.99 6.70 18.1 1.78 4.94 7.20 #&gt; blocks 33.52 18.14 149.8 19.42 31.43 50.75 #&gt; maze 6.02 1.78 19.4 12.71 4.76 9.07 #&gt; reading 20.75 4.94 31.4 4.76 52.60 66.76 #&gt; vocab 29.70 7.20 50.8 9.07 66.76 135.29 y la matriz de correlaciones es: cov2cor(ability.cov$cov) #&gt; general picture blocks maze reading vocab #&gt; general 1.000 0.466 0.552 0.340 0.576 0.514 #&gt; picture 0.466 1.000 0.572 0.193 0.263 0.239 #&gt; blocks 0.552 0.572 1.000 0.445 0.354 0.356 #&gt; maze 0.340 0.193 0.445 1.000 0.184 0.219 #&gt; reading 0.576 0.263 0.354 0.184 1.000 0.791 #&gt; vocab 0.514 0.239 0.356 0.219 0.791 1.000 Entonces, volviendo al modelo examinemos que implicaciones tiene en la matriz de varianzas y covarianzas de las variables aleatorias observables. Denotemos la matriz de varianzas y covarianzas por \\(\\Sigma = Var(Y)\\) y la expresaremos en términos de los parámetros del modelo. \\[\\Sigma = \\Lambda \\Lambda^T + \\Psi\\] Los términos en la diagonal de \\(\\Sigma\\) (varianzas de cada variable observada) son: \\[Var(Y_j) = \\sum_{k= 1}^K \\lambda_{jk}^2 + \\Psi_{jj}\\] \\[= comunalidad + unicidad\\] La comunalidad de la variable \\(Y_j\\) dada por \\(\\sum_{k= 1}^K \\Lambda^2(j,k)\\) es la varianza que comparte esta variable con otras variables por medio de los factores, mientras que la unicidad \\(\\Psi(j,j)\\) es la varianza de la variable \\(j\\) que no comparte con el resto. Un buen análisis de factores tiene comunalidades altas y unicidades bajas (relativamente). Los términos fuera de la diagonal están dados por: \\[Cov(Y_j, Y_i)= \\sum_{k=1}^K\\lambda_{jk}\\lambda_{ik}\\] Sea \\(X \\sim N(0, 1), u_1 \\sim N(0,1),u_2 \\sim N(0,2)\\). Definimos \\[Y_1 = X + u_1\\] \\[Y_2 = -X+u_2\\] Comunalidades: Unicidades: Descomposición de la matriz de varianzas y covarianzas: Ejemplo: Pruebas de habilidad. ability_fa &lt;- factanal(factors = 2, covmat = ability.cov, rotation = &quot;none&quot;) ability_fa #&gt; #&gt; Call: #&gt; factanal(factors = 2, covmat = ability.cov, rotation = &quot;none&quot;) #&gt; #&gt; Uniquenesses: #&gt; general picture blocks maze reading vocab #&gt; 0.455 0.589 0.218 0.769 0.052 0.334 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; general 0.648 0.354 #&gt; picture 0.347 0.538 #&gt; blocks 0.471 0.748 #&gt; maze 0.253 0.408 #&gt; reading 0.964 -0.135 #&gt; vocab 0.815 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.420 1.162 #&gt; Proportion Var 0.403 0.194 #&gt; Cumulative Var 0.403 0.597 #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The chi square statistic is 6.11 on 4 degrees of freedom. #&gt; The p-value is 0.191 14.2.2 Estimación del modelo Antes de adentrarnos en la estimación vale la pena considerar dos aspectos: Rotaciones: Al observar la igualdad \\(\\Sigma = \\Lambda\\Lambda^T + \\Psi\\), notamos que no existe una única \\(\\Lambda\\) que la satisfaga. Sea \\(Q\\) una matriz ortonormal de dimensión \\(K \\times K\\) (\\(Q\\) es una matriz de rotación), \\[Q^T Q = Q Q^T = I\\] Si \\(\\Lambda\\) es tal que \\(Y = \\Lambda X + U\\) y \\(\\Sigma = \\Lambda\\Lambda^T + \\Psi\\) entonces, \\[Y=(\\Lambda Q)(Q^TX) + U\\] \\[\\Sigma = (\\Lambda Q) (\\Lambda Q)^T + \\Psi = \\Lambda\\Lambda^T + \\Psi\\] por lo tanto, \\(\\Lambda_1 = (\\Lambda Q)\\) y \\(X_1 = Q^TX\\) también son una solución para el modelo. Esto nos dice, que cualquier rotación de las cargas nos da una solución. Hay amplia literatura en este tema, típicamente la elección de una rotación busca mejorar la interpretación. ¿Cuántos factores?: No hay una respuesta directa a la pregunta pero para aspirar a contestarla respondamos primero: ¿Cuántos factores puedo estimar? Contemos el número de parámetros que vamos a estimar y veamos los grados de libertad: Parámetros en \\(\\Sigma:p(p+1)/2\\) Parámetros en \\(\\Lambda\\) y \\(\\Psi:pK + p\\) Restricciones necesarias para fijar la rotación: \\(K(K-1)/2\\) Grados de libertad: \\(d = p(p+1)/2 - (pK + p - K(K-1)/2)\\) Si \\(d &lt; 0\\), no podemos estimar el modelo, por lo tanto el mayor número de factores que puedo estimar depende del número de variables observadas. Por ejemplo si \\(p = 5\\), únicamente podemos estimar modelos con 1 ó 2 factores. Volviendo a la pregunta original: ¿Cuántos factores debo modelar? La respuesta depende del objetivo del análisis de factores, en ocasiones se desea utilizar las variables latentes como un _resumen__ de las variables observadas e incorporarlas a ánalisis posteriores, en este caso es conveniente analizar el porcentaje de la varianza en las variables observadas que se puede explicar con los factores, por ejemplo si el tercer factor no contribuye de manera importante a explicar la variabilidad observada, el modelo con dos factores sería preferible. Por otra parte, si asumimos normalidad (\\(X\\sim N(0, I), U\\sim N(0, \\Psi)\\)) podemos comparar la verosimilitud (o AIC, BIC) de los modelos con distinto número de factores y elegir de acuerdo a este criterio. Una vez que fijamos el número de factores, hay varios métodos de estimación, el más popular implementa el algoritmo EM, sin embargo este método requiere supuestos de normalidad. Dentro de los métodos que no requieren supuestos adicionales está el método de factores principales. 14.2.2.1 Método del factor principal En adelante utilzamos la matriz de covarianzas muestral, \\[S = \\frac{1}{N} \\sum_{n = 1}^N(X_n-\\bar{X})(X_n-\\bar{X})^T\\] como la estimación de la matriz de covarianzas poblacional \\(\\Sigma\\). Usualmente no es posible encontrar matrices \\(\\hat{\\Lambda},\\hat{\\Psi}\\) tales que la igualdad \\(S = \\hat{\\Lambda}\\hat{\\Lambda}^T+\\hat{\\Psi}\\) se cumpla de manera exacta. Por tanto el objetivo es encontrar matrices tales que se minimice \\(traza(S-\\hat{S})^T(S-\\hat{S})\\) donde \\(\\hat{S} = \\hat{\\delta}\\hat{\\delta}^T+\\hat{Psi}\\). El algoritmo del método del factor principal funciona de la siguiente manera: Inicializa \\(\\hat{\\Psi}\\) (cualquier valor) \\(\\hat{\\Psi}=\\) los \\(K\\) mayores eigenvectores de la matriz \\[(\\hat{S} - \\hat{\\Psi})\\] Nos fijamos en esta diferencia porque nos interesa explicar las covarianzas a través de los factores comunes. \\(\\hat{\\Psi} = \\mbox{diag}(S-\\hat{\\Lambda}\\hat{\\Lambda}^T)\\) Los pasos 2 y 3 se repiten hasta alcanzar convergencia. Este algoritmo no es muy popular debido a que la convergencia no está asegurada, se considera lento y los valores iniciales de \\(\\Psi\\) suelen influenciar la solución final. 14.2.3 Análisis de factores de máxima verosimilitud Supongamos ahora que, \\[X \\sim N(0, I)\\] \\[U \\sim N(0,\\Psi)\\] Entonces la distribución del vector de variables aleatorias observables \\(Y\\) es \\[Y \\sim N(\\mu + \\Lambda x, \\Sigma)\\] donde \\(\\Sigma = \\Lambda \\Lambda^T + \\Psi\\) (igual que antes). Es fácil ver que la distribución condicional de \\(Y\\) es: \\[Y|X \\sim N(\\mu + \\Lambda x, \\Psi)\\] por tanto, se cumple las independencias condicionales que leemos en la gráfica. Ahora, la log verosimilitud es: \\[\\log L(\\Sigma) = - \\frac{np}{2} \\log(2\\pi) - \\frac{n}{2}\\log \\det(\\Sigma) - \\frac{n}{2}\\mbox{tr}(\\Sigma^{-1}S)\\] buscamos parámetros\\(\\hat{\\Lambda}\\) y \\(\\hat{Psi}\\) que maximizen esta log-verosimilitud, sin embargo, estos parámetros no se pueden separar facilmente (es decir maximizar individualmente) ya que están relacionados a través de \\(det(\\Sigma)\\) y \\(\\Sigma^{-1}\\). No hay una forma cerrada para encontrar los parámetros de máxima verosimilitud de la expresión anterior. Recurrimos entonces al algoritmo EM, donde en el paso E rellanamos los valores de \\(X\\) y en el paso M estimamos \\(\\Lambda\\) y \\(\\Psi\\) utilizando que éstos parámetros se pueden separar si conozco \\(X\\). 14.2.4 Evaluación del modelo Volviendo al número de factores, una vez que hacemos supuestos de normalidad podemos calcular la devianza del modelo: \\[D = n*(tr(\\hat{\\Sigma}^{-1}S) - log det(\\hat{\\Sigma}^{-1}S) - p)\\] y el BIC. Por tanto, podemos comparar modelos con distintos factores utilizando este criterio. \\[d = p - {1}{2}((p-q)^2 - (p+q))\\] y por tanto \\(BIC = D + d log N\\). library(psych) #&gt; #&gt; Attaching package: &#39;psych&#39; #&gt; The following objects are masked from &#39;package:ggplot2&#39;: #&gt; #&gt; %+%, alpha dev &lt;- function(fit){ S &lt;- fit$correlation n &lt;- fit$n.obs p &lt;- nrow(S) Sigma &lt;- (fit$loadings) %*% t(fit$loadings) + diag(fit$uniqueness) mat.aux &lt;- solve(Sigma) %*% S D &lt;- n * (tr(mat.aux) - log(det(mat.aux)) - p) return(D) } BIC &lt;- function(fit){ p &lt;- nrow(fit$loadings) q &lt;- ncol(fit$loadings) v &lt;- p - 1/2 * ((p - q) ^ 2 - (p + q)) D &lt;- dev(fit) BIC &lt;- D + v * log(fit$n.obs) / 2 return(BIC) } ability.fa.1 &lt;- factanal(factors = 1, covmat = ability.cov, rotation = &quot;none&quot;) ability.fa.2 &lt;- factanal(factors = 2, covmat = ability.cov, rotation = &quot;none&quot;) ability.fa.3 &lt;- factanal(factors = 3, covmat = ability.cov, rotation = &quot;none&quot;) BIC(ability.fa.1) #&gt; [1] 71.2 BIC(ability.fa.2) #&gt; [1] 11.1 BIC(ability.fa.3) #&gt; [1] 14.2 Veamos también el porcentaje de la varianza observada que se puede explicar con los distintos modelos. ability.fa.1 #&gt; #&gt; Call: #&gt; factanal(factors = 1, covmat = ability.cov, rotation = &quot;none&quot;) #&gt; #&gt; Uniquenesses: #&gt; general picture blocks maze reading vocab #&gt; 0.535 0.853 0.748 0.910 0.232 0.280 #&gt; #&gt; Loadings: #&gt; Factor1 #&gt; general 0.682 #&gt; picture 0.384 #&gt; blocks 0.502 #&gt; maze 0.300 #&gt; reading 0.877 #&gt; vocab 0.849 #&gt; #&gt; Factor1 #&gt; SS loadings 2.443 #&gt; Proportion Var 0.407 #&gt; #&gt; Test of the hypothesis that 1 factor is sufficient. #&gt; The chi square statistic is 75.2 on 9 degrees of freedom. #&gt; The p-value is 1.46e-12 ability.fa.2 #&gt; #&gt; Call: #&gt; factanal(factors = 2, covmat = ability.cov, rotation = &quot;none&quot;) #&gt; #&gt; Uniquenesses: #&gt; general picture blocks maze reading vocab #&gt; 0.455 0.589 0.218 0.769 0.052 0.334 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; general 0.648 0.354 #&gt; picture 0.347 0.538 #&gt; blocks 0.471 0.748 #&gt; maze 0.253 0.408 #&gt; reading 0.964 -0.135 #&gt; vocab 0.815 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.420 1.162 #&gt; Proportion Var 0.403 0.194 #&gt; Cumulative Var 0.403 0.597 #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The chi square statistic is 6.11 on 4 degrees of freedom. #&gt; The p-value is 0.191 ability.fa.3 #&gt; #&gt; Call: #&gt; factanal(factors = 3, covmat = ability.cov, rotation = &quot;none&quot;) #&gt; #&gt; Uniquenesses: #&gt; general picture blocks maze reading vocab #&gt; 0.441 0.217 0.329 0.580 0.040 0.336 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 Factor3 #&gt; general 0.636 0.367 0.139 #&gt; picture 0.350 0.766 -0.272 #&gt; blocks 0.441 0.639 0.263 #&gt; maze 0.236 0.325 0.509 #&gt; reading 0.974 -0.109 #&gt; vocab 0.811 #&gt; #&gt; Factor1 Factor2 Factor3 #&gt; SS loadings 2.382 1.249 0.427 #&gt; Proportion Var 0.397 0.208 0.071 #&gt; Cumulative Var 0.397 0.605 0.676 #&gt; #&gt; The degrees of freedom for the model is 0 and the fit was 0 Finalmente, volvamos a las rotaciones. La interpretación de los factores se facilita cuando cada variable observada carga principalmente en un factor, por ello, muchos de los métodos de rotación buscan acentuar esta característica: Rotación varimax: Resulta en algunas cargas altas y otras bajas para cada factor, de manera que las cargas bajas se puedan ignorar en la interpretación. Rotación promax: Esta es una rotación oblicua, lo que implica que se pierde la ortogonalidad de los factores. El resultado de esta rotación es que usualmente las cargas se vuelven incluso más extremas que con la rotación varimax. ability.varimax &lt;- factanal(factors = 2, covmat = ability.cov, rotation = &quot;varimax&quot;) ability.promax &lt;- factanal(factors = 2, covmat = ability.cov, rotation = &quot;promax&quot;) cbind(ability.varimax$loadings, ability.promax$loadings) # cutoff = 0.1 #&gt; Factor1 Factor2 Factor1 Factor2 #&gt; general 0.499 0.543 0.3642 0.47041 #&gt; picture 0.156 0.622 -0.0577 0.67120 #&gt; blocks 0.206 0.860 -0.0915 0.93189 #&gt; maze 0.109 0.468 -0.0537 0.50800 #&gt; reading 0.956 0.182 1.0234 -0.09549 #&gt; vocab 0.785 0.225 0.8112 0.00911 14.2.5 Visualización Cuando realizamos componentes principales es común querer proyectar los datos en las componentes. En el caso de AF no es tan sencillo porque los factores son aleatorios, pero hay métodos para calcular puntajes (scores). Método de Bartlett. Supongamos que conocemos \\(\\Lambda\\) y \\(\\Psi\\), denotemos los puntajes del individuo \\(i\\) en los factores por \\(x_i\\), entonces si \\(y_i\\) es el vector de variables observables del i-ésimo individuo, tenemos que \\(y_i\\) dada \\(x_i\\) se distribuye \\(N(\\Lambda x_i, \\Psi)\\), por lo que la log-verosimilitud de la observación \\(y_i\\) esta dada por \\[-\\frac{1}{2} log|2\\pi\\Psi| - \\frac{1}{2}(y_i- \\Lambda f_i)^T \\Psi^{-1}(y_i - \\Lambda x_i)\\] Derivando e igualando a cero se obtiene: \\[\\hat{x}_i = (\\Lambda^T\\Psi^{-1}\\Lambda)\\Lambda^T\\Psi^{-1}y_i\\] Método de Thompson. Consideramos \\(x_i\\) aleatorio, i.e. \\(X\\sim N(0,I)\\), entonces \\(f|y\\) se distribuye \\(N(\\Lambda^T\\Psi^{-1}y, I-\\Lambda^T \\Psi^{-1}\\Lambda)\\) por lo que un estimador natural para \\(x_i\\) es \\[\\hat{x}_i = \\Lambda^T\\Psi^{-1}y_i\\] Ejemplo. La base de datos wine contiene medidas en 13 atributos diferentes de 180 vinos. library(gridExtra) wine &lt;- read_csv(&quot;datos/wine.csv&quot;) head(wine) %&gt;% knitr::kable() Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids Proanthocyanins Color Hue Dilution Proline 1 14.2 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 1 13.2 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 1 13.2 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 1 14.4 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 1 13.2 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 1 14.2 1.76 2.45 15.2 112 3.27 3.39 0.34 1.97 6.75 1.05 2.85 1450 pc.wine.1 &lt;- princomp(wine, scores = TRUE) fa.wine &lt;- factanal(wine, factors = 2, scores = &quot;Bartlett&quot;) fa.pc.wine &lt;- data.frame(fa1 = fa.wine$scores[, 1], pc1 = pc.wine.1$scores[, 1], fa2 = fa.wine$scores[, 2], pc2 = pc.wine.1$scores[, 2]) comp_1 &lt;- ggplot(fa.pc.wine, aes(x = fa1, y = pc1)) + geom_point() comp_2 &lt;- ggplot(fa.pc.wine, aes(x = fa1, y = pc2)) + geom_point() grid.arrange(comp_1, comp_2, ncol = 2) pc.wine.2 &lt;- princomp(wine, scores = T, cor = T) fa.pc.wine &lt;- data.frame(fa1 = fa.wine$scores[, 1], pc1 = pc.wine.2$scores[, 1], fa2 = fa.wine$scores[, 2], pc2 = pc.wine.2$scores[, 2]) comp_1 &lt;- ggplot(fa.pc.wine, aes(x = fa1, y = pc1)) + geom_point() comp_2 &lt;- ggplot(fa.pc.wine, aes(x = fa2, y = pc2)) + geom_point() grid.arrange(comp_1, comp_2, ncol = 2) par(mfrow=c(1,2)) biplot(pc.wine.1) biplot(pc.wine.2) # Ejemplo simulación x1 &lt;- rnorm(1000) x2 &lt;- x1 + 0.001 * rnorm(1000) x3 &lt;- 10 * rnorm(1000) x &lt;- data.frame(x1, x2, x3) fact.x &lt;- fa(x, factors = 1, covar = TRUE, fm =&quot;ml&quot;) pc.x &lt;- princomp(x) fact.x$loadings #&gt; #&gt; Loadings: #&gt; ML1 #&gt; x1 0.999 #&gt; x2 0.999 #&gt; x3 0.789 #&gt; #&gt; ML1 #&gt; SS loadings 2.617 #&gt; Proportion Var 0.872 pc.x$loadings #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 #&gt; x1 0.707 0.707 #&gt; x2 0.707 -0.707 #&gt; x3 1.000 #&gt; #&gt; Comp.1 Comp.2 Comp.3 #&gt; SS loadings 1.000 1.000 1.000 #&gt; Proportion Var 0.333 0.333 0.333 #&gt; Cumulative Var 0.333 0.667 1.000 y &lt;- scale(x) fact.y &lt;- fa(y, factors = 1, fm =&quot;ml&quot;) pc.y &lt;- princomp(y) fact.y$loadings #&gt; #&gt; Loadings: #&gt; ML1 #&gt; x1 0.999 #&gt; x2 0.999 #&gt; x3 #&gt; #&gt; ML1 #&gt; SS loadings 1.999 #&gt; Proportion Var 0.666 pc.y$loadings #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 #&gt; x1 0.705 0.707 #&gt; x2 0.705 -0.707 #&gt; x3 -0.997 #&gt; #&gt; Comp.1 Comp.2 Comp.3 #&gt; SS loadings 1.000 1.000 1.000 #&gt; Proportion Var 0.333 0.333 0.333 #&gt; Cumulative Var 0.333 0.667 1.000 fact.y #&gt; Factor Analysis using method = ml #&gt; Call: fa(r = y, fm = &quot;ml&quot;, factors = 1) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; ML1 h2 u2 com #&gt; x1 1.00 0.9975 0.0025 1 #&gt; x2 1.00 0.9975 0.0025 1 #&gt; x3 0.06 0.0035 0.9965 1 #&gt; #&gt; ML1 #&gt; SS loadings 2.00 #&gt; Proportion Var 0.67 #&gt; #&gt; Mean item complexity = 1 #&gt; Test of the hypothesis that 1 factor is sufficient. #&gt; #&gt; The degrees of freedom for the null model are 3 and the objective function was 13.9 with Chi Square of 13848 #&gt; The degrees of freedom for the model are 0 and the objective function was 7.59 #&gt; #&gt; The root mean square of the residuals (RMSR) is 0 #&gt; The df corrected root mean square of the residuals is NA #&gt; #&gt; The harmonic number of observations is 1000 with the empirical chi square 0.01 with prob &lt; NA #&gt; The total number of observations was 1000 with Likelihood Chi Square = 7560 with prob &lt; NA #&gt; #&gt; Tucker Lewis Index of factoring reliability = -Inf #&gt; Fit based upon off diagonal values = 1 #&gt; Measures of factor score adequacy #&gt; ML1 #&gt; Correlation of (regression) scores with factors 1 #&gt; Multiple R square of scores with factors 1 #&gt; Minimum correlation of possible factor scores 1 En el ejemplo de simulación vemos que el análisis de componentes principales se alinea con la dirección de máxima varianza \\(X_3\\) mientras que el análisis de factores ignora el componente no correlacionado y captura el componente correlacionado \\(X_2 + X_1\\). Debido a que en FA modelamos diferentes unicidades \\(u_j\\) para cada \\(Y_j\\) el análisis de factores puede verse como un modelo para la estructura de correlación de \\(Y_j\\) en lugar de la estructura de covarianzas. 14.3 Tarea El grado de marginación fue definido por CONAPO en 1990: tiene como misión incluir a la población en los programas de desarrollo económico y social que se formulen dentro del sector gubernamental y vincular sus objetivos a las necesidades que plantean los fenómenos demográficos; permite diferenciar municipios según el impacto global de carencias que padece la población como resultado de la falta de acceso a la educación, la residencia en viviendas inadecuadas, la percepción de ingresos monetarios insuficientes y las relacionadas con la residencia en localidades pequeñas; contribuye a identificar las disparidades territoriales que existen entre los municipios; ha sido utilizado como criterio de las reglas de operación de diversos programas, lo que es un indicativo de su aceptación y uso cada vez más generalizado; y define 4 dimensiones de la marginación: educación, vivienda, distribución de la población, ingresos monetarios. library(tidyverse) base_estados &lt;- read_csv(&quot;datos/base_estados.csv&quot;) base_estados %&gt;% head(10) %&gt;% select(NOM_ENT, ANALF:PO2SM, ANIO) %&gt;% knitr::kable() NOM_ENT ANALF SPRIM OVSDE OVSEE OVSAE VHAC OVPT PL&lt;5000 PO2SM ANIO Aguascalientes 7.06 33.9 10.88 4.97 4.18 51.0 7.45 26.97 62.5 1990 Baja California 4.68 24.0 4.89 10.49 19.59 45.4 8.13 11.95 40.0 1990 Baja California Sur 5.39 27.8 7.03 11.10 10.17 48.6 13.99 25.60 54.1 1990 Campeche 15.40 44.8 24.80 15.02 29.52 65.0 24.16 36.21 68.1 1990 Coahuila 5.48 28.2 10.81 5.24 7.76 50.0 8.48 17.13 60.9 1990 Colima 9.30 36.0 8.37 5.82 6.61 56.0 21.13 22.39 50.4 1990 Chiapas 30.12 62.1 42.66 34.92 42.09 74.1 50.90 66.56 80.1 1990 Chihuahua 6.12 30.9 14.10 13.23 11.96 47.8 9.58 25.57 52.8 1990 CDMX 4.00 16.8 1.81 0.76 3.33 45.6 2.45 0.32 60.5 1990 Durango 6.99 39.5 32.38 13.73 14.98 53.5 20.04 49.29 67.7 1990 Aplica el método de componentes principales (utiliza la función princomp como en los ejemplos de arriba.) Las variables para obtener las componentes son: ANALF, SPRIM, OVSEE, OVSDE, OVSAE, VHAC, OVPT, PL&lt;5000, PO2SM. Haz una gráfica para 1990 de las componentes principales donde el eje principal esté en el eje horizontal y el eje secundario en el eje vertical. Sustituye los espacios que digan &lt;rellenar&gt;. datos &lt;- base_estados %&gt;% select(CVE_ENT, NOM_ENT, ANIO, ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, `PL&lt;5000`, PO2SM) %&gt;% mutate_at(.funs = ~as.numeric(.), vars(-NOM_ENT)) %&gt;% group_by(&lt;rellenar&gt;) %&gt;% mutate_at(.funs = funs((. - mean(.))/sd(.)), vars(-&lt;rellenar&gt;, -&lt;rellenar&gt;)) %&gt;% ungroup() %&gt;% as_tibble() indice &lt;- datos %&gt;% filter(&lt;rellenar&gt; == 1990) indice_pca &lt;- princomp(x = &lt;rellenar&gt;, scores = &lt;rellenar&gt;, cor = &lt;rellenar&gt;) indice$comp1 &lt;- indice_pca$scores[,1] indice$comp2 &lt;- indice_pca$scores[,2] ggplot(data = indice, aes(x=&lt;rellenar&gt;, y=&lt;rellenar&gt;)) + geom_point() Haz una gráfica de la medida de distorsión \\(J\\) asociada a la elección del número de componentes \\(M\\) (dada por la suma de los eigenvalores \\(M+1\\) a \\(D\\)). Utiliza la siguiente función para clasificar los componentes principales con el método de Dalenius. La tabla resultado debe tener las siguientes columnas: anio ,cve_ent, nom_ent, eje1, eje2, dal, y GM. No olvides que se debe hacer componentes principales para cada año. library(stratification) clasif_dalenius = function(x, n = NULL, Ls = 5, add = 5, labels = NULL){ x_1 = x + add if(is.null(n)){ n &lt;- length(x) } dalenius = strata.cumrootf(x_1, n = n, Ls = Ls, model = &#39;none&#39;) boundaries = dalenius$bh - add cluster = cut(x, c(-Inf, boundaries, Inf), labels = labels) return(cluster) } Completa el código sustituyendo donde diga &lt;rellenar&gt;: options(warn=-1) anios &lt;- unique(datos$ANIO) calcula_gm &lt;- function(anio){ indice &lt;- datos %&gt;% filter(ANIO == anio) cve_ent &lt;- indice$CVE_ENT nom_ent &lt;- indice$NOM_ENT x = &lt;rellenar&gt; indice_pca &lt;- princomp(x, scores = &lt;rellenar&gt;, cor = &lt;rellenar&gt;) eje1 &lt;- indice_pca$scores[,1] eje2 &lt;- indice_pca$scores[,2] dal &lt;- clasif_dalenius(x = &lt;rellenar&gt;, add = &lt;rellenar&gt;, n = &lt;rellenar&gt;) GM &lt;- ordered(dal, labels = c(&#39;Muy Baja&#39;, &#39;Baja&#39;, &#39;Media&#39;, &#39;Alta&#39;, &#39;Muy Alta&#39;)) tibble(anio, cve_ent, nom_ent, eje1, eje2, dal, GM) } df &lt;- map_df(.x = anios, .f = calcula_gm) Haz una gráfica de los dos componentes como la que se muestra abajo. Utiliza el siguiente código para producir la gráfica. Sustituye los espacios que digan &lt;rellenar&gt;: library(maptools) etiquetar &lt;- function(df, x, y, etiq = &quot;etiq&quot;, size = 3.5){ df &lt;- as.data.frame(df) plot(df[, x], df[, y]) orden &lt;- pointLabel(df[, x], df[, y], df[, etiq], doPlot = TRUE, cex = 0.5 * size, xpd = TRUE) dev.off() df$a &lt;- orden$x df$b &lt;- orden$y df } library(RColorBrewer) set.seed(110265) df_etiq &lt;- etiquetar(df, &quot;eje1&quot;, &quot;eje2&quot;, etiq = &quot;nom_ent&quot;, size = 1.5) df_etiq %&gt;% filter(&lt;rellenar&gt;) %&gt;% ggplot(&lt;rellenar&gt;) + geom_vline(xintercept = 0, linetype = &#39;dashed&#39;, color = &#39;gray60&#39;) + geom_hline(yintercept = 0, linetype = &#39;dashed&#39;, color = &#39;gray60&#39;) + geom_point(aes(&lt;rellenar&gt;), size = 3, show.legend = T) + labs(title = &#39;Dalenius&#39;, subtitle = 1990, x = &#39;Eje Principal&#39;, y = &#39;Eje Secundario&#39;) + geom_text(aes(x = a, y = b, label = nom_ent), size = 2.5) + scale_colour_manual(labels = c(&#39;Muy Baja&#39;, &#39;Baja&#39;, &#39;Media&#39;, &#39;Alta&#39;, &#39;Muy Alta&#39;), values = rev(brewer.pal(n = 5, name = &quot;PiYG&quot;)), name = &quot;Marginación&quot;) Haz una gráfica como ésta que muestre en un mapa tus resultados: Para hacer la gráfica usa este código: library(scales) library(rgdal) library(ggmap) # Leer geometrías del mapa edo_shp &lt;- readOGR(&quot;datos/estados_ligero&quot;, layer = &quot;estados_ligero&quot;) edo_shp@data$id &lt;- c(1:4,7,8,5,6,9:32) edo_shp@data$CVE_ENT &lt;- edo_shp@data$id edo_df &lt;- fortify(edo_shp, region = &quot;CVE_ENT&quot;) # Añadir variables al mapa df_2010 &lt;- df %&gt;% filter(anio == 2010) # Añadimos las variables de interés a la base de datos mun_df edo_ind &lt;- edo_df %&gt;% mutate(CVE_ENT = as.integer(id)) %&gt;% left_join(df_2010, by = c(&#39;CVE_ENT&#39;=&#39;cve_ent&#39;)) # Obtenemos el mapa de Google mapa &lt;- get_googlemap(c(lon = -102, lat = 24), zoom = 4, maptype = &quot;hybrid&quot;, style = &#39;feature:administrative.country|element:labels|visibility:off&#39;) ggmap(mapa) + geom_polygon(data = edo_ind, aes(long, lat, group = group, fill = GM), color = &quot;gray30&quot;, size = 0.5, alpha = 0.7) + labs(title = &quot;Grado de marginación en 2010&quot;) + scale_x_continuous(limits = c(-119, -86), expand = c(0, 0)) + scale_y_continuous(limits = c(14, 33), expand = c(0, 0)) + scale_fill_manual(values = rev(brewer.pal(n = 5, name = &quot;PiYG&quot;)), name= &quot;Marginación&quot;, guide = guide_legend(reverse = T)) "],
["correlacion-canonica-cca.html", "Clase 15 Correlación Canónica (CCA) 15.1 CCA vs PCA 15.2 Variables y correlaciones canónicas 15.3 Ejemplo: test psicológico 15.4 Ejemplo: fenómenos meteorológicos 15.5 Tarea", " Clase 15 Correlación Canónica (CCA) .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } 15.1 CCA vs PCA Anteriormente, estudiamos los métodos analíticos de factores como un enfoque para comprender las fuentes clave de variación dentro de conjuntos de variables. Hay situaciones en las que tenemos varios conjuntos de variables, y buscamos una comprensión de las dimensiones clave que se correlacionan entre conjuntos. El análisis de correlación canónica es uno de los métodos más antiguos y mejor conocidos para descubrir y explorar dimensiones que están correlacionadas entre conjuntos, pero no están correlacionadas dentro del conjunto. El análisis de correlación canónica se concentra en la correlación entre una combinación lineal de variables en un conjunto y la combinación lineal de variables en otro conjunto. La idea es determinar primero el par de combinaciones lineales que tienen la combinación lineal más grande. Después, el par de combinaciones lineales que tienen correlación más grande entre todos los pares que no están correlacionados con el primero, y así sucesivamente. A los pares de combinaciones lineales se les llama variables canónicas y a sus correlaciones se les llama correlaciones canónicas. Las correlaciones canónicas miden qué tan fuerte es la asociación entre dos conjuntos de variables. 15.2 Variables y correlaciones canónicas Nos interesa medir la asociación entre dos grupos de variables: El primer grupo (de \\(p\\) variables) está representado por vector aleatorio \\(X^{(1)}\\) El segundo grupo (de \\(q\\) variables) está representado por otro vector aleatorio \\(X^{(2)}\\) Suponemos en lo que sigue que \\(p \\leq q\\). Para los vectores aleatorios \\(X^{(1)}\\) y \\(X^{(2)}\\) sea \\[ E(X^{(1)}) = \\mu^{(1)},\\qquad \\mbox{Var}(X^{(1)}) = \\Sigma_{11} \\] \\[ E(X^{(2)}) = \\mu^{(2)},\\qquad \\mbox{Var}(X^{(2)}) = \\Sigma_{22} \\] \\[ \\mbox{Cov}\\left(X^{(1)},X^{(2)}\\right) = \\Sigma_{12} = \\Sigma_{21} \\] Ponemos \\[ X = \\left(X^{(1)}, X^{(2)}\\right) \\] con vector de media \\[ \\mu = (\\mu^{(1)},\\mu^{(2)}) \\] y matriz de covarianzas \\[ \\Sigma = \\left[ \\begin{array}{c|c} \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\hline \\Sigma_{21} &amp; \\Sigma_{22} \\end{array} \\right] \\] Observaciones: Las correlaciones entre pares de variables, una de \\(X^{(1)}\\) y la otra de \\(X^{(2)}\\) está contenidas en la matriz \\(\\Sigma_{12}\\). \\(\\Sigma_{12}\\) es de dimensión \\(pq\\). Si \\(p\\) y \\(q\\) son grandes, la interpretación de \\(\\Sigma_{12}\\) puede ser muy complicada. En este caso, es usual interpretar más fácilmente combinaciones lineales de las variables de cada grupo. La tarea principal del análisis canónico es resumir la asociación entre \\(X^{(1)}\\) y \\(X^{(2)}\\). 15.2.1 Combinaciones lineaes de factores Las combinaciones lineales proveen de interpretación a conjuntos de variables. Sea \\[ \\begin{eqnarray*} U &amp;=&amp; a^T X^{(1)} \\\\ V &amp;=&amp; b^T X^{(2)} \\end{eqnarray*} \\] para un par de vectores \\(a\\) y \\(b\\). Por lo tanto, \\[ \\mbox{Var}(U) = a^T \\mbox{Var}(X^{(1)}) a = a^T\\Sigma_{11}a \\] \\[ \\mbox{Var}(V) = b^T \\mbox{Var}(X^{(2)}) b = b^T\\Sigma_{22}b \\] \\[ \\mbox{Cov}(U,V) = a^T \\mbox{Cov}\\left(X^{(1)},X^{(2)}\\right) b = a^T\\Sigma_{12}b. \\] Buscamos coeficientes de los vectores \\(a\\) y \\(b\\) tales que: \\[ \\mbox{Corr}(U,V) = \\dfrac{a^T\\Sigma_{12}b}{\\sqrt{a^T\\Sigma_{11}a}\\sqrt{b^T\\Sigma_{22}b}}. \\] Sea \\(k\\) entre \\(1\\) y \\(p\\leq q\\), entonces al \\(k\\)-ésimo par de combinaciones lineales \\(U_k\\) y \\(V_kl\\) con varianzas unitarias y que maximizan la correlación entre las opciones que no estén correlacionadas con las anteriores, se les llama \\(k\\)-ésimo par de variables canónicas. Suponemos que \\(\\Sigma_{12}\\) es de rango completo, entonces \\[ \\max_{u,v}\\mbox{Corr}(U,V) = \\rho_1^*, \\] se alcanza en las combinaciones lineales (primer par canónico): \\[ U_{1}=\\underbrace{e_{1}^{T}\\Sigma_{11}^{-1}}_{a_{1}^{T}}X^{(1)}, \\qquad \\mbox{ y } \\qquad V_{1}=\\underbrace{f_{1}^{T}\\Sigma_{22}^{-1}}_{b_{1}^{T}}X^{(2)} \\] donde \\(\\mbox{Var}\\left(X_i^{(1)}\\right) = \\sigma_{ii}\\) para \\(i=1,2,\\ldots,p\\). El \\(k\\)-ésimo par de variables canónicas \\(k=2,3,\\ldots,p\\), \\[ U_k = e_k^T\\Sigma_{11}^{-1/2}X^{(1)},\\qquad V_k = f_k^T\\Sigma_{22}^{-1/2}X^{(2)} \\] maximiza \\[ \\mbox{Corr}(U_k,V_k)=\\rho_k^* \\] entre las combinaciones lineales que no están correlacionadas con las variables canónicas precedentes de \\(1,2,\\ldots,k-1\\). Aquí \\({\\rho_1^*}^2 \\geq {\\rho_2^*}^2 \\geq \\cdots \\geq {\\rho_p^*}^2\\) son los eigenvalores de \\[ \\Sigma_{11}^{-1/2} \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\Sigma_{11}^{-1/2}, \\] y \\(e_1,e_2,\\ldots,e_p\\) son los eigenvectores asociados. De la misma forma \\({\\rho_1^*}^2,{\\rho_2^*}^2,\\ldots,{\\rho_p^*}^2\\) son los eigenvalores asociados a los \\(p\\) eigenvectores de dimensión \\(q\\), \\(f_1,f_2,\\ldots,f_p\\), de la matriz \\[ \\Sigma_{22}^{-1/2} \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} \\Sigma_{22}^{-1/2}. \\] Propiedades: Para \\(k,\\,l = 1,2,\\ldots,p\\): \\(\\mbox{Var}(U_k) = \\mbox{Var}(V_k)\\) = 1 \\(\\mbox{Cov}(U_k,U_l) = \\mbox{Corr}(U_k,U_l) = 0 \\quad k\\neq l\\) \\(\\mbox{Cov}(V_k,V_l) = \\mbox{Corr}(V_k,V_l) = 0 \\quad k\\neq l\\) \\(\\mbox{Cov}(U_k,V_l) = \\mbox{Corr}(U_k,V_l) = 0 \\quad k\\neq l\\) Si las variables originales están estandarizadas \\(Z^{(1)}=\\left(Z^{(1)}_1,\\ldots,Z^{(1)}_p\\right)\\) entonces \\[ U_k = a_k^T Z^{(1)} = e_k^T \\rho_{11}^{-1} Z^{(1)} \\] \\[ V_k = b_k^T Z^{(2)} = f_k^T \\rho_{22}^{-1} Z^{(2)} \\] Aquí sucede que \\(\\mbox{Cov}(Z^{(1)}) = \\rho_{11}\\), es decir, la covarianza de \\(Z^{(1)}\\) la representamos como matriz de correlaciones utilizando la letra \\(\\rho\\), y de forma similar, \\(\\mbox{Cov}(Z^{(2)}) = \\rho_{22}\\). Además se puede ver que: \\[ \\mbox{Corr}(U_k,V_k) = \\rho_k^*, \\quad k=1,2,\\ldots,p \\] donde \\({\\rho_1^*}^2\\geq{\\rho_2^*}^2\\geq\\ldots\\geq{\\rho_p^*}^2\\) son los eigenvalores de la matriz \\[ \\rho_{11}^{-1/2}\\rho_{12}\\rho_{22}^{-1}\\rho_{21}\\rho_{11}^{-1/2} \\] Notemos que: \\[ \\begin{eqnarray*} a_k^T(X^{(1)}-\\mu^{(1)}) &amp;=&amp; a_{k1}(X_1^{(1)}-\\mu_1^{(1)}) + \\cdots+a_{kp}(X_p^{(1)}-\\mu_p^{(1)})\\\\ &amp;=&amp; a_{k1}\\sqrt{\\sigma_{11}}\\dfrac{(X_1^{(1)}-\\mu_1^{(1)})}{\\sqrt{\\sigma_{11}}} + \\cdots+a_{kp}\\sqrt{\\sigma_{pp}}\\dfrac{(X_p^{(1)}-\\mu_p^{(1)})}{\\sqrt{\\sigma_{pp}}}. \\end{eqnarray*} \\] Por lo tanto, los coeficientes canónicos de las variables estandarizadas \\[ Z_i ^{(1)} = \\dfrac{X_i^{(1)}- \\mu_i^{(1)}}{\\sqrt{\\sigma_{ii}}} \\] están relacionados simplemente con los coeficientes canónicos de la variable original \\(X_i^{(1)}\\). Más específicamente, si \\(a_k\\) es el vector de coeficientes de la \\(k\\)-ésima variable canónica \\(U_k\\), entonces \\[ a_k^TV_{11}^{1/2} \\] es el vector de coeficientes de la \\(k\\)-ésima variable canónica de las variables estandarizadas \\(Z^{(1)}\\). Aquí \\(V_{11}^{1/2}\\) es la matriz diagonal con \\(i\\)-ésimo elemento \\(\\sqrt{\\sigma_{ii}}\\). Al estandarizar las variables \\(X^{(1)}\\) y \\(X^{(2)}\\) y realizar análisis de correlación canónica sobre \\(Z^{(1)}\\) y \\(Z^{(2)}\\), las correlaciones canónicas Se multiplican por un factor de \\(\\sqrt{\\sigma_{ii}}\\). No cambian. Deben sumar \\(1\\). Ninguna de las anteriores. 15.2.2 Ejemplo simple \\[ X=\\begin{pmatrix} 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 3 &amp; 2 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 2 \\\\ 2 &amp; 2 &amp; 3 \\\\ 3 &amp; 3 &amp; 2 \\\\ 1 &amp; 3 &amp; 2 \\\\ 4 &amp; 3 &amp; 5 \\\\ 5 &amp; 5 &amp; 5 \\end{pmatrix}, \\qquad Y= \\left(\\begin{array}{rrr} 4 &amp; 4 &amp; −1.07846 \\\\ 3 &amp; 3 &amp; 1.214359 \\\\ 2 &amp; 2 &amp; 0.307180 \\\\ 2 &amp; 3 &amp; −0.385641 \\\\ 2 &amp; 1 &amp; −0.078461 \\\\ 1 &amp; 1 &amp; 1.61436 \\\\ 1 &amp; 2 &amp; 0.814359 \\\\ 2 &amp; 1 &amp; -0.0641016 \\\\ 1 &amp; 2 &amp; 1.5359 \\\\ \\end{array}\\right) \\] En este ejemplo, construimos la tercera columna de \\(Y\\) de las columnas de \\(X\\) con los pesos lineales \\(a\\) = \\((0.4, 0.6, -\\sqrt{0.48})\\). Algunas preguntas: ¿Cuál debería ser el primer vector de pesos canónicos para las variables \\(Y\\)? ¿Cuál debería ser la primera correlación canónica? Respuestas: El propósito del análisis de correlación canónica es (a) buscar y (b) caracterizar la redundancia lineal entre ambos conjuntos de variables. En nuestro ejemplo simple, una de las variables en \\(Y\\) se puede reproducir exactamente como una combinación lineal de las tres variables en X. El análisis de correlación canónica (si funciona correctamente) simplemente seleccionará \\(y_3\\) como la primera variable canónica en el conjunto \\(Y\\), con pesos canónicos \\(b = (0,0,1)\\), y recupera la combinación lineal de las variables del primer grupo utilizada para generar \\(y_3\\), generando \\(a = (0.4, 0.6, -\\sqrt{0.48})\\) como los pesos canónicos para el conjunto \\(X\\). La primera correlación canónica será, por supuesto, 1. X &lt;- matrix(c( 1,1,3,2,3,2,1,1,1, 1,1,2,2,2,3,3,3,2,1,3,2, 4,3,5,5,5,5),9,3,byrow=T) Y &lt;- matrix(c( 4,4,-1.07846, 3,3,1.214359, 2,2,0.307180, 2,3,-0.385641, 2,1,-0.078461, 1,1,1.61436, 1,2,0.814359, 2,1,-0.0641016, 1,2,1.535900),9,3,byrow=T) Para calcular los pesos completamente, necesitamos las matrices de varianzas y covarianzas para \\(X\\) e \\(Y\\), así como las matrices de covarianza cruzada. S_xy &lt;- cov(X, Y) S_xx &lt;- var(X) S_yx &lt;- cov(Y, X) S_yy &lt;- var(Y) S_xy #&gt; [,1] [,2] [,3] #&gt; [1,] -0.625 -0.778 0.69679 #&gt; [2,] -0.750 -0.556 0.95242 #&gt; [3,] -0.125 -0.347 -0.00826 Ahora que tenemos estas matrices, es fácil calcular los pesos canónicos y las correlaciones canónicas: A &lt;- eigen(solve(S_xx) %*% S_xy %*% solve(S_yy) %*% S_yx)$vectors B &lt;- eigen(solve(S_yy) %*% S_yx %*% solve(S_xx) %*% S_xy)$vectors R &lt;- sqrt(eigen(solve(S_yy) %*% S_yx %*% solve(S_xx) %*% S_xy)$values) A #&gt; [,1] [,2] [,3] #&gt; [1,] 0.400 0.796 -0.578 #&gt; [2,] 0.600 -0.584 0.429 #&gt; [3,] -0.693 -0.160 0.695 B #&gt; [,1] [,2] [,3] #&gt; [1,] 1.94e-07 -0.5365 -0.835 #&gt; [2,] -4.34e-07 0.8438 0.139 #&gt; [3,] 1.00e+00 0.0136 -0.533 R #&gt; [1] 1.000 0.519 0.091 15.3 Ejemplo: test psicológico Supongamos que se desea investigar asociaciones entre mediciones psicológicas y medidas de desempeño académico y contamos con datos con 600 observaciones de ocho variables. Las variables psicológicas son locus_of_control, self_concept y motivation. Las variables académicas son pruebas estandarizadas en lectura (read), redacción (write), matemáticas (math) y ciencia (science). Además, la variable female es una variable indicadora de que una estudiante es mujer. library(ggplot2) library(GGally) library(CCA) psychademic %&gt;% head %&gt;% knitr::kable() locus_of_control self_concept motivation read write math science sex -0.84 -0.24 4 54.8 64.5 44.5 52.6 female -0.38 -0.47 3 62.7 43.7 44.7 52.6 female 0.89 0.59 3 60.6 56.7 70.5 58.0 male 0.71 0.28 3 62.7 56.7 54.7 58.0 male -0.64 0.03 4 41.6 46.3 38.4 36.3 female 1.11 0.90 2 62.7 64.5 61.4 58.0 female Las variables psicológicas son: (psych_variables &lt;- attr(psychademic, &quot;psychology&quot;)) #&gt; [1] &quot;locus_of_control&quot; &quot;self_concept&quot; &quot;motivation&quot; Las variables académicas son: (academic_variables &lt;- attr(psychademic, &quot;academic&quot;)) #&gt; [1] &quot;read&quot; &quot;write&quot; &quot;math&quot; &quot;science&quot; &quot;sex&quot; Veamos las correlaciones entre las variables psicológicas: ggpairs(psychademic, psych_variables, title = &quot;Within Psychological Variables&quot;) Y entre las académicas: ggpairs(psychademic, academic_variables, title = &quot;Within Academic Variables&quot;) Veamos las correlaciones entre pares de ambos grupos de variables: ggduo( psychademic, psych_variables, academic_variables, types = list(continuous = &quot;smooth_lm&quot;), title = &quot;Between Academic and Psychological Variable Correlation&quot;, xlab = &quot;Psychological&quot;, ylab = &quot;Academic&quot; ) También podemos analizar las correlaciones numéricamente: datos &lt;- psychademic %&gt;% mutate_at(.funs = as.numeric, .vars = vars(-sex)) %&gt;% mutate(sex = ifelse(sex == &#39;female&#39;, 1, 0)) matcor(datos[,psych_variables], datos[,academic_variables]) #&gt; $Xcor #&gt; locus_of_control self_concept motivation #&gt; locus_of_control 1.000 0.171 0.245 #&gt; self_concept 0.171 1.000 0.289 #&gt; motivation 0.245 0.289 1.000 #&gt; #&gt; $Ycor #&gt; read write math science sex #&gt; read 1.0000 0.629 0.6793 0.691 -0.0417 #&gt; write 0.6286 1.000 0.6327 0.569 0.2443 #&gt; math 0.6793 0.633 1.0000 0.650 -0.0482 #&gt; science 0.6907 0.569 0.6495 1.000 -0.1382 #&gt; sex -0.0417 0.244 -0.0482 -0.138 1.0000 #&gt; #&gt; $XYcor #&gt; locus_of_control self_concept motivation read write #&gt; locus_of_control 1.000 0.1712 0.2450 0.3736 0.3589 #&gt; self_concept 0.171 1.0000 0.2890 0.0607 0.0194 #&gt; motivation 0.245 0.2890 1.0000 0.2105 0.2541 #&gt; read 0.374 0.0607 0.2105 1.0000 0.6286 #&gt; write 0.359 0.0194 0.2541 0.6286 1.0000 #&gt; math 0.337 0.0536 0.1949 0.6793 0.6327 #&gt; science 0.325 0.0698 0.1159 0.6907 0.5691 #&gt; sex 0.113 -0.1260 0.0979 -0.0417 0.2443 #&gt; math science sex #&gt; locus_of_control 0.3373 0.3246 0.1134 #&gt; self_concept 0.0536 0.0698 -0.1260 #&gt; motivation 0.1949 0.1159 0.0979 #&gt; read 0.6793 0.6907 -0.0417 #&gt; write 0.6327 0.5691 0.2443 #&gt; math 1.0000 0.6495 -0.0482 #&gt; science 0.6495 1.0000 -0.1382 #&gt; sex -0.0482 -0.1382 1.0000 LLevamos a cabo CCA: cc1 &lt;- cc(datos[,psych_variables], datos[,academic_variables]) # correlaciones canónicas cc1$cor #&gt; [1] 0.464 0.167 0.104 Coeficientes canónicos: cc1[3:4] #&gt; $xcoef #&gt; [,1] [,2] [,3] #&gt; locus_of_control -1.254 -0.619 -0.663 #&gt; self_concept 0.351 -1.191 0.823 #&gt; motivation -0.421 0.675 0.670 #&gt; #&gt; $ycoef #&gt; [,1] [,2] [,3] #&gt; read -0.0446 -0.00504 0.02123 #&gt; write -0.0359 0.04198 0.09164 #&gt; math -0.0234 0.00410 0.00923 #&gt; science -0.0051 -0.08479 -0.10992 #&gt; sex -0.6321 1.08910 -1.79242 Nota: Las correlaciones anteriores son entre las variables observadas y las variables canónicas, a estas se les conoce como cargas canónicas. Las variables canónicas son en realidad un tipo de variable latente. En general, el número de dimensiones canónicas es igual al número de variables en el conjunto más pequeño; sin embargo, la cantidad de dimensiones significativas puede ser aún menor. Las dimensiones canónicas, también conocidas como variables canónicas, son variables latentes que son análogas a los factores obtenidos en el análisis de factores. En este ejemplo particular, hay tres dimensiones canónicas, de las cuales solo las dos primeras son estadísticamente significativas. ev &lt;- (1 - cc1$cor^2) n &lt;- dim(datos)[1] p &lt;- length(datos[,psych_variables]) q &lt;- length(datos[,academic_variables]) k &lt;- min(p, q) m &lt;- n - 3/2 - (p + q)/2 w &lt;- rev(cumprod(rev(ev))) d1 &lt;- d2 &lt;- f &lt;- vector(&quot;numeric&quot;, k) for (i in 1:k) { s &lt;- sqrt((p^2 * q^2 - 4)/(p^2 + q^2 - 5)) si &lt;- 1/s d1[i] &lt;- p * q d2[i] &lt;- m * s - p * q/2 + 1 r &lt;- (1 - w[i]^si)/w[i]^si f[i] &lt;- r * d2[i]/d1[i] p &lt;- p - 1 q &lt;- q - 1 } pv &lt;- pf(f, d1, d2, lower.tail = FALSE) (dmat &lt;- cbind(WilksL = w, F = f, df1 = d1, df2 = d2, p = pv)) #&gt; WilksL F df1 df2 p #&gt; [1,] 0.754 11.71 15 1635 7.79e-28 #&gt; [2,] 0.962 2.94 8 1186 2.97e-03 #&gt; [3,] 0.989 2.16 3 594 9.19e-02 Cuando las variables en el modelo tienen desviaciones estándar muy diferentes, los coeficientes estandarizados permiten una comparación más fácil entre las variables. Coeficientes canónicos estandarizados para variables psicológicas: s1 &lt;- diag(sqrt(diag(cov(datos[,psych_variables])))) s1 %*% cc1$xcoef #&gt; [,1] [,2] [,3] #&gt; [1,] -0.841 -0.415 -0.444 #&gt; [2,] 0.248 -0.840 0.581 #&gt; [3,] -0.432 0.693 0.688 Coeficientes canónicos estandarizados para variables académicas: s2 &lt;- diag(sqrt(diag(cov(datos[,academic_variables])))) s2 %*% cc1$ycoef #&gt; [,1] [,2] [,3] #&gt; [1,] -0.4506 -0.0509 0.2145 #&gt; [2,] -0.3489 0.4083 0.8914 #&gt; [3,] -0.2202 0.0386 0.0869 #&gt; [4,] -0.0495 -0.8229 -1.0669 #&gt; [5,] -0.3150 0.5428 -0.8933 Los coeficientes canónicos estandarizados se interpretan de manera análoga a la interpretación de los coeficientes de regresión estandarizados. Por ejemplo, para la variable de lectura, un aumento de la desviación estándar en lectura se refleja en una disminución de 0.45 desviaciones estándar de en la primera variable canónica del conjunto 2 cuando las otras variables en el modelo se mantienen constantes. 15.4 Ejemplo: fenómenos meteorológicos Ejemplo de (Marc MeNugget)[https://menugget.blogspot.mx/2012/03/canonical-correlation-analysis-for.html]. Veamos ahora el uso del Análisis de Correlación Canónica (CCA) para encontrar patrones entre pares de observaciones en campos climáticos. El método produce resultados similares a los de componentes principales, pero los patrones reflejan la correlación máxima en vez de la covarianza máxima. Además, el resultado del modelo es una combinación de modelos lineales que se pueden usar para un modelo de predicción de un campo climático. Descargamos los campos de anomalías mensuales de presión superficial del mar (SLP) y temperatura de la superficie del mar (TSM) para la región del Pacífico ecuatorial de la NOAA. library(curl) url_pres &lt;- &quot;ftp://ftp.cdc.noaa.gov/Datasets.other/hadslp/slp.mnmean.nc&quot; if(!file.exists(&quot;datos/slp.mnmean.nc&quot;)){ curl_fetch_disk(url_pres, path = &quot;datos/slp.mnmean.nc&quot;, handle = new_handle(CONNECTTIMEOUT = 60)) } url_temp &lt;- &quot;http://www.esrl.noaa.gov/psd/thredds/fileServer/Datasets/kaplan_sst/sst.mon.anom.nc&quot; if(!file.exists(&quot;datos/sst.mon.anom.nc&quot;)){ curl_fetch_disk(url = url_temp, path = &quot;datos/sst.mon.anom.nc&quot;, handle = new_handle(CONNECTTIMEOUT = 60)) } Leemos los datos: library(maps) library(mapproj) library(ncdf4) library(CCP) library(irlba) nc &lt;- nc_open(filename = &quot;datos/slp.mnmean.nc&quot;) slp.lon &lt;- ncvar_get(nc, &quot;lon&quot;) slp.lat &lt;- ncvar_get(nc, &quot;lat&quot;) slp.t &lt;- ncvar_get(nc, &quot;time&quot;) slp.raw &lt;- ncvar_get(nc, &quot;slp&quot;) nc_close(nc) slp.t &lt;- as.Date(slp.t, origin=&quot;1800-01-01&quot;) temp &lt;- which(slp.lon &gt; 180) slp.lon[temp] &lt;- slp.lon[temp] - 360 slp.grd &lt;- expand.grid(slp.lon, slp.lat) colnames(slp.grd) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) slp &lt;- matrix(c(slp.raw), nrow=length(slp.t), ncol&lt;-length(slp.lon)*length(slp.lat), byrow=TRUE) row.names(slp) &lt;- as.character(slp.t) Calculamos anomalías en la presión superficial del mar: anomaly &lt;- function(y, x, level=&quot;daily&quot;){ y &lt;- as.matrix(y) if(level==&quot;monthly&quot;){levs=unique(x$mon)} if(level==&quot;daily&quot;){levs=unique(x$yday)} levs_lookup=vector(&quot;list&quot;, length(levs)) names(levs_lookup)&lt;-levs for(i in 1:length(levs)){ if(level==&quot;monthly&quot;){levs_lookup[[i]]&lt;-which(x$mon == names(levs_lookup[i]))} if(level==&quot;daily&quot;){levs_lookup[[i]]&lt;-which(x$yday == names(levs_lookup[i]))} } for(j in 1:length(levs)){ y[levs_lookup[[j]],] &lt;- t(t(as.matrix(y[levs_lookup[[j]],])) - apply(as.matrix(y[levs_lookup[[j]],]), 2, mean, na.rm=TRUE)) } y } slp &lt;- anomaly(slp, as.POSIXlt(slp.t), level=&quot;monthly&quot;) Leemos los datos de temperaturas: nc &lt;- nc_open(filename = &quot;datos/sst.mon.anom.nc&quot;) sst.lon &lt;- ncvar_get(nc, &quot;lon&quot;) sst.lat &lt;- ncvar_get(nc, &quot;lat&quot;) sst.t &lt;- ncvar_get(nc, &quot;time&quot;) sst.raw &lt;- ncvar_get(nc, &quot;sst&quot;) nc_close(nc) sst.t &lt;- as.Date(sst.t, origin=&quot;1800-01-01&quot;) temp &lt;- which(sst.lon &gt; 180) sst.lon[temp] &lt;- sst.lon[temp] - 360 sst.grd &lt;- expand.grid(sst.lon, sst.lat) colnames(sst.grd) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) sst &lt;- matrix(c(sst.raw), nrow=length(sst.t), ncol&lt;-length(sst.lon)*length(sst.lat), byrow=TRUE) row.names(sst) &lt;- as.character(sst.t) Tomamos un subconjunto de los datos de acuerdo a fechas: t.min &lt;- as.Date(&quot;1980-01-01&quot;) t.max1 &lt;- as.Date(&quot;1998-12-01&quot;) t.max2 &lt;- as.Date(&quot;2018-04-01&quot;) slp.t.incl &lt;- which(slp.t &gt;= t.min &amp; slp.t &lt;= t.max2) sst.t.incl &lt;- which(sst.t &gt;= t.min &amp; sst.t &lt;= t.max2) Nos concentramos primero en la región del pacífico: lon.lim &lt;- c(-180, -70) lat.lim &lt;- c(-30, 30) slp.grd.incl &lt;- which(slp.grd[,1] &lt; -70 &amp; slp.grd[,1] &gt; -180 &amp; slp.grd[,2] &lt; 30 &amp; slp.grd[,2] &gt; -30) sst.grd.incl &lt;- which(sst.grd[,1] &lt; -70 &amp; sst.grd[,1] &gt; -180 &amp; sst.grd[,2] &lt; 30 &amp; sst.grd[,2] &gt; -30) Centramos cada una de las matrices: X &lt;- slp[slp.t.incl, slp.grd.incl] Y &lt;- sst[sst.t.incl, sst.grd.incl] Y[is.na(Y)] &lt;- 0 X_c &lt;- scale(X, center=TRUE, scale=FALSE) Y_c &lt;- scale(Y, center=TRUE, scale=FALSE) Analicemos en las medias de ambas variables a través del tiempo: X_m &lt;- matrix(apply(X,1,mean),nrow=nrow(X_c), ncol=1) rownames(X_m) &lt;- rownames(X) Y_m &lt;- matrix(apply(Y,1,mean),nrow=nrow(Y_c), ncol=1) rownames(Y_m) &lt;- rownames(Y) zran &lt;- range(X_m, Y_m) zlim &lt;- c(-max(abs(zran)), max(abs(zran))) pal &lt;- color.palette(c(&quot;red&quot;, &quot;yellow&quot;, &quot;white&quot;, &quot;cyan&quot;, &quot;blue&quot;), c(10,1,1,10)) colorvalues1 &lt;- val2col(X_m, zlim, col=pal(ncol)) colorvalues2 &lt;- val2col(Y_m, zlim, col=pal(ncol)) Creamos polígonos para graficar: #slp spacing &lt;- 5 slp.poly &lt;- vector(mode=&quot;list&quot;, dim(slp.grd)[1]) for(i in seq(slp.poly)){ x=c(slp.grd[i,1]-spacing/2, slp.grd[i,1]+spacing/2, slp.grd[i,1]+spacing/2, slp.grd[i,1]-spacing/2) y=c(slp.grd[i,2]-spacing/2, slp.grd[i,2]-spacing/2, slp.grd[i,2]+spacing/2, slp.grd[i,2]+spacing/2) slp.poly[[i]] &lt;- data.frame(x=x, y=y) } #sst spacing &lt;- 5 sst.poly &lt;- vector(mode=&quot;list&quot;, dim(sst.grd)[1]) for(i in seq(sst.poly)){ x=c(sst.grd[i,1]-spacing/2, sst.grd[i,1]+spacing/2, sst.grd[i,1]+spacing/2, sst.grd[i,1]-spacing/2) y=c(sst.grd[i,2]-spacing/2, sst.grd[i,2]-spacing/2, sst.grd[i,2]+spacing/2, sst.grd[i,2]+spacing/2) sst.poly[[i]] &lt;- data.frame(x=x, y=y) } Gráfica de anomalías de presión a nivel del mar: #mapproj settings project=&quot;fisheye&quot; orientation=c(mean(lat.lim), mean(lon.lim), 0) PAR=1 par(mai=c(0.1, 0.1, 0.1, 0.1)) map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim) for(i in seq(slp.grd.incl)){ polygon(mapproject(x=slp.poly[[slp.grd.incl[i]]][,1], y=slp.poly[[slp.grd.incl[i]]][,2]), col=colorvalues1[i], border=colorvalues1[i], lwd=0.3) } map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col=&quot;black&quot;) map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col=&quot;grey&quot;, lwd=1) box() Mapa de anomalías par(mai=c(0.1, 0.1, 0.1, 0.1)) map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim, xaxs=&quot;i&quot;, yaxs=&quot;i&quot;) for(i in seq(sst.grd.incl)){ polygon(mapproject(x=sst.poly[[sst.grd.incl[i]]][,1], y=sst.poly[[sst.grd.incl[i]]][,2]), col=colorvalues2[i], border=colorvalues2[i], lwd=0.3) } map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col=&quot;black&quot;) map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col=&quot;grey&quot;, lwd=1) box() Ahora hacemos el análisis de correlación canónica. Primero calculamos la covarianza entre \\(X\\) y \\(Y\\): cov4gappy &lt;- function(F1, F2=NULL){ if(is.null(F2)){ F1 &lt;- as.matrix(F1) F1_val&lt;-replace(F1, which(!is.na(F1)), 1) F1_val&lt;-replace(F1_val, which(is.na(F1_val)), 0) n_pairs=(t(F1_val)%*%F1_val) F1&lt;-replace(F1, which(is.na(F1)), 0) cov_mat &lt;- (t(F1)%*%F1)/n_pairs cov_mat &lt;- replace(cov_mat, which(is.na(cov_mat)), 0) } if(!is.null(F2)){ if(dim(F1)[1] == dim(F2)[1]){ F1 &lt;- as.matrix(F1) F2 &lt;- as.matrix(F2) F1_val&lt;-replace(F1, which(!is.na(F1)), 1) F1_val&lt;-replace(F1_val, which(is.na(F1_val)), 0) F2_val&lt;-replace(F2, which(!is.na(F2)), 1) F2_val&lt;-replace(F2_val, which(is.na(F2_val)), 0) n_pairs=(t(F1_val)%*%F2_val) F1&lt;-replace(F1, which(is.na(F1)), 0) F2&lt;-replace(F2, which(is.na(F2)), 0) cov_mat &lt;- (t(F1)%*%F2)/n_pairs cov_mat &lt;- replace(cov_mat, which(is.na(cov_mat)), 0) } else { print(&quot;ERROR; matrices columns not of the same lengths&quot;) } } cov_mat } exp.mat&lt;-function(MAT, EXP, tol=NULL){ MAT &lt;- as.matrix(MAT) matdim &lt;- dim(MAT) if(is.null(tol)){ tol=min(1e-7, .Machine$double.eps*max(matdim)*max(MAT)) } if(matdim[1]&gt;=matdim[2]){ svd1 &lt;- svd(MAT) keep &lt;- which(svd1$d &gt; tol) res &lt;- t(svd1$u[,keep]%*%diag(svd1$d[keep]^EXP, nrow=length(keep))%*%t(svd1$v[,keep])) } if(matdim[1]&lt;matdim[2]){ svd1 &lt;- svd(t(MAT)) keep &lt;- which(svd1$d &gt; tol) res &lt;- svd1$u[,keep]%*%diag(svd1$d[keep]^EXP, nrow=length(keep))%*%t(svd1$v[,keep]) } return(res) } F1 &lt;- as.matrix(X_c) F2 &lt;- as.matrix(Y_c) F1 &lt;- as.matrix(F1) F1_val&lt;-replace(F1, which(!is.na(F1)), 1) F1_val&lt;-replace(F1_val, which(is.na(F1_val)), 0) n_pairs=(t(F1_val)%*%F1_val) F1&lt;-replace(F1, which(is.na(F1)), 0) cov_mat1 &lt;- (t(F1)%*%F1)/n_pairs cov_mat1 &lt;- replace(cov_mat1, which(is.na(cov_mat1)), 0) F2 &lt;- as.matrix(F2) F2_val&lt;-replace(F2, which(!is.na(F2)), 1) F2_val&lt;-replace(F2_val, which(is.na(F2_val)), 0) n_pairs=(t(F2_val)%*%F2_val) F2&lt;-replace(F2, which(is.na(F2)), 0) cov_mat2 &lt;- (t(F2)%*%F2)/n_pairs cov_mat2 &lt;- replace(cov_mat2, which(is.na(cov_mat2)), 0) L1 &lt;- irlba(cov_mat1, nu=5, nv=5) L2 &lt;- irlba(cov_mat2, nu=5, nv=5) A1_coeff &lt;- replace(F1, which(is.na(F1)), 0)%*%L1$u[,1:5] A1_norm &lt;- F1_val%*%(L1$u[,1:5]^2) A1=A1_coeff/A1_norm A2_coeff &lt;- replace(F2, which(is.na(F2)), 0)%*%L2$u[,1:5] A2_norm &lt;- F2_val%*%(L2$u[,1:5]^2) A2=A2_coeff/A2_norm x &lt;- A1[,1:5] %*% exp.mat(diag(L1$d[1:5]), -0.5) y &lt;- A2[,1:5] %*% exp.mat(diag(L2$d[1:5]), -0.5) Hacemos descomposición en valores singulares: xdim &lt;- dim(t(x)) ydim &lt;- dim(t(y)) mindim=min(xdim[2], ydim[2]) xcols &lt;- 1:xdim[2] ycols &lt;- (xdim[2]+1):(xdim[2]+ydim[2]) Sxx &lt;- cov4gappy(t(x)) Sxy &lt;- cov4gappy(t(x),t(y)) Syy &lt;- cov4gappy(t(y)) svd1 &lt;- svd(exp.mat(Syy, -0.5) %*% t(Sxy) %*% exp.mat(Sxx, -0.5)) A &lt;- exp.mat(Sxx, -0.5) %*% svd1$v B &lt;- exp.mat(Syy, -0.5) %*% svd1$u Veamos las variables canónicas: V &lt;- t(x) %*% A W &lt;- t(y) %*% B rho=NA*1:mindim for(i in 1:mindim){ rho[i] &lt;- cor(as.vector(W[,i]), as.vector(V[,i]), use=&quot;pairwise&quot;) } # A test of CCA significance # cca.sig &lt;- p.asym(rho, xdim[1], xdim[2], ydim[2], tstat = &quot;Pillai&quot;) Repetimos los mapas de las variables canónicas (primer par): eof.slp &lt;- eof.mca(slp[slp.t.incl, slp.grd.incl], centered=TRUE, nu=20) eof.sst &lt;- eof.mca(sst[sst.t.incl, sst.grd.incl], centered=TRUE, nu=20) dim(eof.slp$A) #&gt; [1] 228 20 dim(eof.sst$A) #&gt; [1] 460 20 #CCA model will be built on a smaller time subset of the PCs (eof$A) eof.slp.match &lt;- which(as.Date(rownames(eof.slp$A)) &gt;= t.min &amp; as.Date(rownames(eof.slp$A)) &lt;= t.max1) eof.sst.match &lt;- which(as.Date(rownames(eof.sst$A)) &gt;= t.min &amp; as.Date(rownames(eof.sst$A)) &lt;= t.max1) #The BPCCA model based on SLP and SST EOFS bpcca &lt;- bp.cca(eof.slp, eof.sst, n_pcx_incl=eof.slp$n_sig, n_pcy_incl=eof.sst$n_sig, rowx_incl=eof.slp.match, rowy_incl=eof.sst.match) #&gt; Pillai-Bartlett Trace, using F-approximation: #&gt; stat approx df1 df2 p.value #&gt; 1 to 5: 1.08708 12.34 25 1110 0.00e+00 #&gt; 2 to 5: 0.44913 6.91 16 1120 2.44e-15 #&gt; 3 to 5: 0.16898 4.39 9 1130 1.17e-05 #&gt; 4 to 5: 0.03486 2.00 4 1140 9.22e-02 #&gt; 5 to 5: 0.00922 2.12 1 1150 1.45e-01 ###Map of CCA slp.cca.modes &lt;- eof.slp$u[,1:bpcca$n_pcx_incl] %*% bpcca$A sst.cca.modes &lt;- eof.sst$u[,1:bpcca$n_pcy_incl] %*% bpcca$B MODE=1 zran &lt;- range(slp.cca.modes[,MODE], sst.cca.modes[,MODE]) zlim &lt;- c(-max(abs(zran)), max(abs(zran))) heights=c(3,2) widths=c(4,4,0.5) pal=color.palette(c(&quot;red&quot;, &quot;yellow&quot;, &quot;white&quot;, &quot;cyan&quot;, &quot;blue&quot;), c(10,1,1,10)) ncol=100 res=200 colorvalues1 &lt;- val2col(slp.cca.modes[,MODE], zlim, col=pal(ncol)) #color levels for the polygons colorvalues2 &lt;- val2col(sst.cca.modes[,MODE], zlim, col=pal(ncol)) #color levels for the polygons #mapproj settings project=&quot;fisheye&quot; orientation=c(mean(lat.lim), mean(lon.lim), 0) PAR=1 par(omi=c(0.5, 0.5, 0.5, 0.5), ps=12) par(mai=c(0.1, 0.1, 0.1, 0.1)) map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim) for(i in seq(slp.grd.incl)){ polygon(mapproject(x=slp.poly[[slp.grd.incl[i]]][,1], y=slp.poly[[slp.grd.incl[i]]][,2]), col=colorvalues1[i], border=colorvalues1[i], lwd=0.3) } map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col=&quot;black&quot;) map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col=&quot;grey&quot;, lwd=1) par(mai=c(0.1, 0.1, 0.1, 0.1)) map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, ylim=lat.lim, xlim=lon.lim, xaxs=&quot;i&quot;, yaxs=&quot;i&quot;) for(i in seq(sst.grd.incl)){ polygon(mapproject(x=sst.poly[[sst.grd.incl[i]]][,1], y=sst.poly[[sst.grd.incl[i]]][,2]), col=colorvalues2[i], border=colorvalues2[i], lwd=0.3) } map(&quot;world&quot;,project=project, orientation=orientation, par=PAR, fill=FALSE, add=TRUE, col=&quot;black&quot;) map.grid(c(-180, 180, -90, 90), nx=36, ny=18, labels=FALSE, col=&quot;grey&quot;, lwd=1) 15.5 Tarea Como parte de un estudio más amplio de los efectos de la estructura organizacional en la “satisfacción laboral”, Dunham investigó en qué medida las medidas de satisfacción laboral están relacionadas con las características del trabajo. Utilizando un instrumento de encuesta, Dunham obtuvo medidas de \\(p = 5\\) características del trabajo y \\(q = 7\\) variables de satisfacción laboral para \\(n = 784\\) ejecutivos de la rama corporativa de una gran corporación comercial minorista. ¿Las medidas de satisfacción laboral están asociadas con las características del trabajo? La respuesta puede tener implicaciones para el diseño del trabajo. library(tidyverse) worksat &lt;- read_csv(&quot;datos/worksat.csv&quot;) worksat %&gt;% str() #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 784 obs. of 13 variables: #&gt; $ ID : int 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ SupervisorSatisfaction(Y1) : num -0.1529 -1.2486 0.8356 0.0264 0.4826 ... #&gt; $ CareerFutureSatisfaction(Y2): num -2.04 -1.083 1.439 0.441 0.652 ... #&gt; $ FinancialSatisfaction(Y3) : num -2.083 -1.295 0.392 -0.591 0.872 ... #&gt; $ WorkloadSatisfaction(Y4) : num -0.553 -0.633 0.488 0.986 -1.424 ... #&gt; $ CompanyIdentification(Y5) : num -1.7599 0.0725 0.4898 -0.0888 -0.7677 ... #&gt; $ WorkTypeSatisfaction(Y6) : num -2.027 -0.38 -1.992 1.477 -0.404 ... #&gt; $ GeneralSatisfaction(Y7) : num -2.304 1.19 1.591 0.793 -0.772 ... #&gt; $ FeedbackQuality(X1) : num -0.347 -0.737 -0.382 0.432 -1.029 ... #&gt; $ TaskSignificance(X2) : num -0.75 -1.038 -1.557 0.187 -0.627 ... #&gt; $ TaskVariety(X3) : num -1.33 0.626 0.604 0.833 0.229 ... #&gt; $ TaskIdentity(X4) : num -2.706 0.2 -1.308 1.335 0.177 ... #&gt; $ Autonomy(X5) : num -1.815 0.673 -0.994 0.138 -0.508 ... #&gt; - attr(*, &quot;spec&quot;)=List of 2 #&gt; ..$ cols :List of 13 #&gt; .. ..$ ID : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; #&gt; .. ..$ SupervisorSatisfaction(Y1) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ CareerFutureSatisfaction(Y2): list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ FinancialSatisfaction(Y3) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ WorkloadSatisfaction(Y4) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ CompanyIdentification(Y5) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ WorkTypeSatisfaction(Y6) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ GeneralSatisfaction(Y7) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ FeedbackQuality(X1) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ TaskSignificance(X2) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ TaskVariety(X3) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ TaskIdentity(X4) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; .. ..$ Autonomy(X5) : list() #&gt; .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; #&gt; ..$ default: list() #&gt; .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; #&gt; ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; Calcula la matriz de correlaciones muestrales basada en las 784 observaciones. Encuentra los eigenvalores de \\(\\Sigma_{11}^{-1/2} \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\Sigma_{11}^{-1/2}\\) y verifica que estos eigenvalores son los mismos eigenvalores de \\(\\Sigma_{22}^{-1/2} \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} \\Sigma_{22}^{-1/2}\\). Calcula todas las correlaciones canónicas y las variables canónicas. Reporta los coeficientes de las variables canónicas e interpreta el primer par de variables canónicas. Primero analiza qué variables contribuyen más a cada variable canónica. Para proporcionar interpretaciones de \\(U_1\\) y \\(V_1\\), calcula las correlaciones muestrales entre \\(U_1\\) y las variables que la componen y entre \\(V_1\\) y sus variables respectivas. Además, haz una tabla que muestre las correlaciones entre las variables de un conjunto y la primera variable canónica del otro conjunto. Sean \\(Z^{(l)}\\) y \\(Z^{(2)}\\) las variables estandarizadas correspondientes a \\(X^{(1)}\\) y \\(X^{(2)}\\), respectivamente. ¿Qué proporción de la varianza muestral total de \\(Z^{(1)}\\) se explica por la variable canónica \\(U_1\\)? ¿Qué proporción de la varianza muestral total de \\(Z^{(2)}\\) se explica por la variable canónica \\(V_1\\)? Discute tus respuestas. 15.5.0.1 Referencias {-}+ Dunham, R. B. (1977). Reactions to job characteristics: Moderating effects of the organization. Academy of Management Journal, 20(1), 42-65. "],
["conglomerados-clustering-1.html", "Clase 16 Conglomerados (clustering) 1 16.1 Introducción 16.2 Enfoques: combinatorio y basado en modelos. 16.3 K-medias 16.4 Selección de número de clusters. 16.5 Dificultades en segmentación/clustering.", " Clase 16 Conglomerados (clustering) 1 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 16.1 Introducción Mediante clustering (o análisis de conglomerados) buscamos encontrar agrupaciones de casos, de manera que casos dentro de un grupo (cluster, o conglomerado) estén cercanos entre ellos, mientras que puntos en distintos clusters están a distancia más grande - todo esto de acuerdo a alguna medida de distancia. Entonces podemos: Descubrir estructuras interesantes de agrupamiento en los datos que nos ayuden a entenderlos. Resumir grupos (que pueden ser grandes), por casos representativos y/o promedio. Así la tarea de entender los casos (que pueden ser miles, por ejemplo) se reduce a entender los grupos (que pueden ser decenas o cientos, por ejemplo). Usar grupos como insumo para otras tareas (por ejemplo, distintos modelos predictivos según cluster, distintas estrategias de tratamiento según cluster, etc.) Generalmente el proceso de clustering involucra tres pasos: Selección de métrica de distancia entre puntos. Aplicación de un algoritmo de clustering. Selección de número de clusters. Para variables numéricas, una elección usual es la distancia euclideana (cuando las variables tienen las mismas unidades, o escala similar), o la distancia euclideana para las variables estandarizadas. Ejemplo Consideramos las ocurrencias de temblores cerca de Fiji (The data set give the locations of 1000 seismic events of MB &gt; 4.0. The events occurred in a cube near Fiji since 1964.) quakes_1 &lt;- quakes[, c(&#39;lat&#39;,&#39;long&#39;)] quakes_1$id &lt;- 1:nrow(quakes_1) ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() ¿Podemos separar en grupos estos datos? Podemos pensar en varias maneras de hacer esto. Un enfoque simple que nos puede dar una agrupación interesante es pensar en agrupar de manera que obtengamos grupos relativamente compactos o concentrados alrededor de un valor (en estos datos, ¿qué otras cosas se te ocurren? ¿qué otros patrones interesantes sería interesante agrupar?). 16.2 Enfoques: combinatorio y basado en modelos. Los enfoques basados en modelos (por ejemplo, modelos de mezclas) se basan en la introducción de variables latentes que explican diferencias en las distribuciones de las variables observadas. Veremos métodos combinatorios, que trabajan directamente sobre los datos (sin modelos), intentanto segmentar en grupos a través de los cuales minimizamos alguna medida objetivo. En este contexto, el problema de segmentación/clustering se plantea como sigue: Suponemos que buscamos \\(K\\) grupos. Una asignación \\(C\\) es una función que asigna a cada observación \\(x_i\\) un grupo \\(C(i)\\in \\{1,\\ldots, k\\}\\). Tenemos una distancia o disimilitud \\(d(x,y)\\) entre puntos, y una función \\(W(C_k)\\) que mide qué tan disperso es el grupo \\(C_k\\) en términos de la distancia \\(d\\). Buscamos entonces encontrar una solución \\(C^*(i)\\) que minimice \\[W(C) = \\sum_{k=1}^K W(C_k),\\] es decir, buscamos que las distancias dentro de cada grupo (within groups) \\(k\\) sean lo más chicas posibles. Resolver este problema enumerando todas las posibles asignaciones \\(C\\) no es factible, pues el número de posibles asignaciones es típicamente gigantesco aún para un conjunto de datos muy chico. La idea entonces es buscar heurísticas que den soluciones razonables a este problema de minimización. 16.3 K-medias Éste es posiblemente el algoritmo (o familia de algoritmos) más popular de segmentación, y se escala razonablemente bien a problemas relativamente grandes. Supongamos que tenemos datos numéricos con escala o unidades similares (si no, podemos estandarizar). En k-medias, un buen agrupamiento es uno en el que la variación dentro de los grupos es chica. En primer lugar fijamos el número \\(K\\) de grupos que buscamos. Supongamos entonces que \\(C_1\\cup\\cdots\\cup C_K\\) es una partición de los datos, y sea \\(W(C_k)\\) nuestra medida de variación dentro de los clusters. Entonces buscaremos resolver \\[ \\min_{C_1,\\ldots, C_K} \\sum_{k=1}^K W(C_k) \\] Una medida usual es la siguiente: \\[W(C_k)=\\sum_{i\\in C_k} ||x_i-\\bar{x}_k||^2,\\] donde \\(\\bar{x}_k=\\frac{1}{|C_k|}\\sum_{i\\in C_k} x_i\\) es el centroide del grupo \\(C_k\\). Esto mide qué tan compacto es un grupo considerando la suma de distancias a su centroide. El problema que queremos resolver es entonces \\[\\begin{equation} \\min_{C_1,\\ldots, C_K} \\sum_{k=1}^K \\sum_{i\\in C_k} ||x_i-\\bar{x}_k||^2 \\tag{16.1} \\end{equation}\\] Nótese que cada caso \\(x_k\\) contribuye un sumando a la función objetivo. Este problema es demasiado grande para resolver por fuerza bruta (por ejemplo, enlistando todas las posibles agrupaciones). Podemos desarrollar una heurística considerando primero el problema ampliado \\[\\begin{equation} \\min_{C_1,\\ldots, C_K, m_1,\\ldots, m_K} \\sum_{k=1}^K \\sum_{i\\in C_k}||x_i-m_k||^2 \\tag{16.2} \\end{equation}\\] cuya solución es la misma que el problema original. La razón es que la función objetivo del problema ampliado alcanza un mínimo menor o igual al del problema original, pero dada cualquier solución \\(C_1^*,C_2^*,\\ldots,C_K^*\\), tenemos que para toda \\(k\\), las distancias al cuadrado son mínimas al centroide de los datos: \\[ \\sum_{i \\in C_k} ||x_i - \\bar{x}_k||^2 \\leq \\sum_{i \\in C_k} ||x_i - m_k||^2, \\] por lo que \\(C_1^*,C_2^*,\\ldots,C_K^*\\) también es solución del problema original. Ahora desarrollamos la heurística de k-medias. Comenzando con el problema ampliado, notamos que si fijamos puntos centrales iniciales \\[ m_1,\\ldots, m_K \\] podemos minimizar (con centros fijos) \\[ \\min_{C_1,\\ldots, C_K} \\sum_{k=1}^K \\sum_{i\\in C_k}||x_i-m_k||^2 \\] asignando cada \\(x_i\\) al cluster \\(C_k\\) con la \\(m_k\\) asociada más cercana a \\(x_i\\). Una vez que tenemos clusters dados, podemos ahora minimizar (con clusters fijos) \\[ \\min_{m_1,\\ldots, m_K} \\sum_{k=1}^K \\sum_{i\\in C_k}||x_i-m_k||^2 \\] cuya solución es \\[ m_k = \\bar{x}_k = \\frac{1}{n_k}\\sum_{i\\in C_k} x_i. \\] Ahora podemos iterar: agrupar en clusters, encontrar centroide, etc. Esto sugiere el siguiente algoritmo: Algoritmo de k-medias K-means Sea \\(k\\) el número de grupos que buscamos. Escogemos \\(k\\) puntos en los datos de entrenamiento al azar. En el paso \\(s=1,2,\\ldots\\): Dadas los centros \\(m_k\\) (que pensamos fijas), encontramos una nueva asignación \\(C_k\\) a clusters que minimice \\[ 2\\sum_{k=1}^K \\sum_{i\\in C_k} ||x_i - m_k||^2, \\] y esto se hace asignando cada observación al centro \\(m_k\\) que esté más cercano. (cálculo de centroides) Dada una asignación a clusters, encontramos nuevos centros promediando en cada cluster : \\[ m_k = \\frac{1}{|C_k|}\\sum_{i\\in C_k} x_i. \\] 3.Repetimos. Nos detenemos cuando los centroides se quedan casi fijos de una iteración a la siguiente (y en consecuencia la cantidad a minimizar no decrece más). Observaciones: Este algoritmo converge (cada paso reduce la función de costo), pero no tiene garantía de obtener un mínimo global. Conviene correr varias veces, para distintos arranques aleatorios, y escoger la solución con función objetivo más chica. Cuando no es posible correrlo múltiples veces, puede ser que la solución obtenida esté muy lejos de una óptima. Hay distintas maneras de implementar este algoritmo, algunas más eficientes que otras. Ejemplo Describiremos iteraciones para \\(K=5\\) para el conjunto de datos, con una implementación simple: quakes_1 &lt;- quakes[, c(&#39;lat&#39;,&#39;long&#39;)] quakes_1$id &lt;- 1:nrow(quakes_1) ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() Seleccionamos muestra de datos al azar (centroides): set.seed(2512) K &lt;- 5 centros &lt;- sample_n(quakes_1, K) %&gt;% mutate(k = 1:K) %&gt;% select(-id) ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() + geom_point(data = centros, aes(x=long, y=lat), size=7, colour=&#39;red&#39;) Agrupamos: agrupar &lt;- function(datos, centros){ datos_larga &lt;- datos %&gt;% gather(variable, valor, -id) centros_larga &lt;- centros %&gt;% gather(variable, valor_m, -k) dat &lt;- full_join(datos_larga, centros_larga) %&gt;% mutate(dif_cuad = (valor-valor_m)^2) %&gt;% group_by(id, k) %&gt;% summarise(dist_cuad = sum(dif_cuad)) %&gt;% group_by(id) %&gt;% arrange(id, k) %&gt;% summarise(k = which.min(dist_cuad)) dat &lt;- dat %&gt;% left_join(datos) dat } agrup &lt;- agrupar(quakes_1, centros) #&gt; Joining, by = &quot;variable&quot; #&gt; Joining, by = &quot;id&quot; ggplot(agrup, aes(x=long, y=lat, colour=factor(k))) + geom_point() Recalculamos centros: recalcular_centros &lt;- function(datos_agrup){ datos_agrup %&gt;% gather(variable, valor, -id, -k) %&gt;% group_by(k, variable) %&gt;% summarise(valor = mean(valor)) %&gt;% spread(variable, valor) } centros &lt;- recalcular_centros(agrup) ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() + geom_point(data = centros, aes(x=long, y=lat), size=7, colour=&#39;red&#39;) Y ahora calculamos distancia dentro de clusters: wss &lt;- function(agrupacion, centros){ wss &lt;- agrupacion %&gt;% group_by(k) %&gt;% gather(variable, valor,-id, -k) %&gt;% left_join(centros %&gt;% gather(variable, centro, -k), by = c(&quot;k&quot;, &quot;variable&quot;)) %&gt;% group_by(k) %&gt;% summarise(dist_2 = sum((valor - centro)^2)) sum(wss$dist_2) } wss(agrup, centros) #&gt; [1] 11022 Agrupamos: agrup &lt;- agrupar(quakes_1, centros) ggplot(agrup, aes(x=long, y=lat, colour=factor(k))) + geom_point() Recalculamos centros: centros &lt;- recalcular_centros(agrup) wss(agrup, centros) #&gt; [1] 9657 ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() + geom_point(data = centros, aes(x=long, y=lat), size=7, colour=&#39;red&#39;) Una iteración más da: agrup &lt;- agrupar(quakes_1, centros) centros &lt;- recalcular_centros(agrup) wss(agrup, centros) #&gt; [1] 9115 ggplot(agrup, aes(x=long, y=lat, colour=factor(k))) + geom_point() Ejercicio: Corre varias veces este ejemplo con distinta semilla. ¿Obtienes mejores o peores soluciones que la de arriba? ¿Qué tan “naturales” crees que son los grupos que obtuvimos? ¿Qué defectos potenciales le ves a este agrupamiento? Usando la funcion kmeans La función kmeans de R automáticamente produce varias corridas con distintos inicios aleatorios y selecciona la que produzca el mínimo más bajo. set.seed(2800) k_medias &lt;- kmeans(quakes_1[, c(&#39;long&#39;,&#39;lat&#39;)], centers = 5, nstart=30) # escoger varios comienzos aleatorios str(k_medias) #&gt; List of 9 #&gt; $ cluster : int [1:1000] 1 1 1 2 1 2 4 5 5 2 ... #&gt; $ centers : num [1:5, 1:2] 181 184 170 167 181 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; .. ..$ : chr [1:2] &quot;long&quot; &quot;lat&quot; #&gt; $ totss : num 62065 #&gt; $ withinss : num [1:5] 1868 2465 314 682 1406 #&gt; $ tot.withinss: num 6736 #&gt; $ betweenss : num 55330 #&gt; $ size : int [1:5] 326 353 69 136 116 #&gt; $ iter : int 3 #&gt; $ ifault : int 0 #&gt; - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; grupo &lt;- k_medias$cluster quakes_1$grupo &lt;- grupo ggplot(quakes_1, aes(x=long, y=lat, colour=factor(grupo))) + geom_point() Observaciones: En la salida la cantidad k_medias$tot.withinss #&gt; [1] 6736 nos da el valor de (16.1) para la solución obtenida. El algoritmo usado por defecto en R es el de Hartigan-Wong, que es más eficiente que la implementación simple que vimos arriba. 16.4 Selección de número de clusters. Variación dentro de clusters para distintas soluciones Podemos medir la calidad de la segmentación según el mínimo alcanzado de la función objetivo: la suma de cuadrados dentro de los clusters (withinss), que nos dice qué tan compactos son. Primero vemos un ejemplo simple con tres clusters claros: set.seed(2800) df &lt;- data.frame(x=c(rnorm(100,-50,10), rnorm(100,0,10), rnorm(70,30,2) )) qplot(df$x) ajustes_km &lt;- lapply(1:20, function(k){ kmedias &lt;- kmeans(df, centers = k, nstart = 20) kmedias }) tot_within &lt;- sapply(ajustes_km, function(aj){ aj$tot.withinss}) datos_codo &lt;- data_frame(no_clusters = 1:length(tot_within), tot_within = tot_within) %&gt;% mutate(sol_3 = no_clusters==3) ggplot(datos_codo, aes(x = no_clusters, y = tot_within)) + geom_line() + geom_point(aes(colour = sol_3), size=3) Agregar un cluster adicional hace más complejo nuestro resumen, así que incrementamos el número de clusters sólo cuando tenemos una mejora considerable en la solución. En este caso particular, un buen lugar para cortar es el codo (en 3 clusters), pues añadir un cluster más no mejora la solución considerablemente. A veces el punto de corte no es tan claro, aunque vemos que en nuestro ejemplo de terremotos la estructura más clara es la de 2 grupos: set.seed(2800) df &lt;- quakes_1[, c(&#39;lat&#39;,&#39;long&#39;)] ajustes_km &lt;- lapply(1:20, function(k){ kmedias &lt;- kmeans(df, centers = k, nstart = 20) kmedias }) tot_within &lt;- sapply(ajustes_km, function(aj){ aj$tot.withinss}) qplot(1:length(tot_within), tot_within, geom=&#39;line&#39;) + geom_point(size=3) Las soluciones son: grupos_df &lt;- lapply(ajustes_km[1:8], function(aj){ data_frame(num = max(aj$cluster), cluster = aj$cluster, id= 1:length(aj$cluster))}) %&gt;% bind_rows() grupo_df_2 &lt;- left_join(grupos_df, quakes_1) ggplot(grupo_df_2, aes(x=lat, y=long, colour=factor(cluster))) + facet_wrap(~num) + geom_point() La gráfica de codo con la función objetivo es una manera de guiar la selección de número de clusters, aún cuando no siempre da respuestas muy claras. En cualquier caso, rara vez seleccionamos el número de clusters usando solamente el criterio de la gráfica de la función objetivo. 16.4.1 Criterios específicos Más frecuentemente, la selección de número de grupos se hace tomando en cuenta el uso posterior que se va a hacer de los clusters. Por ejemplo: En segmentación de clientes/usuarios, casi siempre queremos un grupo no muy grande de grupos (2-10, máximo decenas), pues puede ser difícil diseñar productos o estrategias particulares si tenemos muchos segmentos.Lo mismo sucede en aplicaciones científicas, por ejemplo clasificar objetos como estrellas, galaxias, etc. Existen algunas aplicaciones de clustering donde buscamos muchos grupos, por ejemplo, para agrupar objetos (noticias, tweets, por ejemplo) en clusters muy compactos (de alta similitud), y así detectar duplicados o relaciones entre fuentes de noticias, usuarios de twitter, etc. Hay más opciones además de algoritmos clásicos como k-means para este tipo de problemas La estrategia típica es entonces producir varias agrupaciones, y compararlas según sus virtudes para uso posterior. Ejemplo Consideremos segmentar personas según sus actitudes hacia las responsabilidades que tiene o no el gobierno en cuanto al bienestar de las personas. European Social Survey (ESS) data from the 2008 (fourth) round in the United Kingdom. The data are from a questionnaire on “what the responsibilities of governments should or should not be.” gvjbevn: Job for everyone, governments’ responsibility (0-10). gvhlthc: Health care for the sick, governments’ responsibility (0-10). gvslvol: Standard of living for the old, governments’ responsibility (0-10). gvslvue: Standard of living for the unemployed, governments’ responsibility (0-10). gvcldcr: Child care services for working parents, governments’ responsibility (0-10). gvpdlwk: Paid leave from work to care for sick family, governments’ responsibility (0-10). sbprvpv: Social benefits/services prevent widespread poverty (1-5). sbeqsoc: Social benefits/services lead to a more equal society (1-5). sbcwkfm: Social benefits/services make it easier to combine work and family (1-5). ess4_gb &lt;- read_csv(&#39;datos/ess4_gb.csv&#39;) dat &lt;- ess4_gb %&gt;% select(idno, gvjbevn:sbcwkfm) nombres &lt;- data_frame(var = c(&quot;gvjbevn&quot;, &quot;gvhlthc&quot;, &quot;gvslvol&quot;, &quot;gvslvue&quot;, &quot;gvcldcr&quot;, &quot;gvpdlwk&quot;, &quot;sbprvpv&quot;, &quot;sbeqsoc&quot;, &quot;sbcwkfm&quot;), nombre = c(&#39;trabajo_a_todos&#39;,&#39;cuidados_salud_enfermos&#39;,&#39;garantizar_nivel_mayores&#39;,&#39;garantizar_nivel_desempleados&#39;,&#39;ayuda_padres_trabajadores&#39;,&#39;ausencia_cuidar_enfermos&#39;,&#39;beneficios_pobreza&#39;,&#39;beneficios_igualdad&#39;,&#39;beneficios_fam_trabajo&#39;)) head(dat) %&gt;% knitr::kable() idno gvjbevn gvhlthc gvslvol gvslvue gvcldcr gvpdlwk sbprvpv sbeqsoc sbcwkfm 110701 0 10 8 5 5 4 2 4 2 110702 5 10 10 10 5 10 4 4 4 110703 8 9 10 0 8 10 4 4 3 110704 8 10 10 3 8 6 3 2 2 110705 7 10 8 8 9 8 4 2 2 110708 0 10 10 5 7 7 2 2 2 En este caso particular tenemos unas variables que están en escala 1-5 y otras 1-10. Esta variabilidad distinta sólo es escala de las respuestas, así que normalizamos dividiendo cada pregunta por su máximo (dividir entre 10 las preguntas en escala de 1 a 10 y entre 5 las de 1 a 5. Podríamos también estandarizar): dat_2 &lt;- dat %&gt;% gather(var, valor, gvjbevn:sbcwkfm) %&gt;% left_join(nombres) %&gt;% select(-var) %&gt;% group_by(nombre) %&gt;% mutate(valor_escalado = valor/max(valor, na.rm=T)) %&gt;% select(-valor) %&gt;% spread(nombre, valor_escalado) #&gt; Joining, by = &quot;var&quot; dat_3 &lt;- filter(dat_2, apply(dat_2, 1, function(x){!any(is.na(x))})) dat_3 #&gt; # A tibble: 2,108 x 10 #&gt; idno ausencia_cuidar_enfer… ayuda_padres_trabajad… beneficios_fam_tra… #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 110701 0.4 0.5 0.4 #&gt; 2 110702 1 0.5 0.8 #&gt; 3 110703 1 0.8 0.6 #&gt; 4 110704 0.6 0.8 0.4 #&gt; 5 110705 0.8 0.9 0.4 #&gt; 6 110708 0.7 0.7 0.4 #&gt; # ... with 2,102 more rows, and 6 more variables: #&gt; # beneficios_igualdad &lt;dbl&gt;, beneficios_pobreza &lt;dbl&gt;, #&gt; # cuidados_salud_enfermos &lt;dbl&gt;, garantizar_nivel_desempleados &lt;dbl&gt;, #&gt; # garantizar_nivel_mayores &lt;dbl&gt;, trabajo_a_todos &lt;dbl&gt; ajustes_km &lt;- lapply(1:10, function(k){ kmedias &lt;- kmeans(dat_3[,-1], centers = k, nstart = 20, iter.max=40) kmedias }) tot_within &lt;- sapply(ajustes_km, function(aj){ aj$tot.withinss}) qplot(1:length(tot_within), tot_within, geom=&#39;line&#39;) + geom_point() En esta gráfica no vemos un codo claro. Veamos primero la solución de dos grupos: sol_cl &lt;- ajustes_km[[2]] table(sol_cl$cluster) #&gt; #&gt; 1 2 #&gt; 1116 992 Ahora veamos cómo resumir grupos para entender qué tipo de casos están en cada uno de ellos. Consideramos las variables originales escaladas: cluster_df &lt;- data.frame(idno = dat_3$idno, cluster = sol_cl$cluster) dat_4 &lt;- dat_3 %&gt;% gather(variable, valor, ausencia_cuidar_enfermos:trabajo_a_todos) %&gt;% left_join(cluster_df) Y resumimos dentro de cada grupo cada una de las variables. Elecciones populares son la media y el error estándar de la media (desviación estándar dividida entre la raíz del número de casos): resumen_1 &lt;- dat_4 %&gt;% group_by(cluster, variable) %&gt;% summarise(media = mean(valor), ee = sd(valor)/sqrt(length(valor))) resumen_1 #&gt; # A tibble: 18 x 4 #&gt; # Groups: cluster [?] #&gt; cluster variable media ee #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 ausencia_cuidar_enfermos 0.816 0.00476 #&gt; 2 1 ayuda_padres_trabajadores 0.804 0.00475 #&gt; 3 1 beneficios_fam_trabajo 0.490 0.00474 #&gt; 4 1 beneficios_igualdad 0.554 0.00556 #&gt; 5 1 beneficios_pobreza 0.506 0.00528 #&gt; 6 1 cuidados_salud_enfermos 0.928 0.00323 #&gt; # ... with 12 more rows ## adicionalmente, invertimos las 3 preguntas en escala de 1 a 5, pues 1 representa mayor acuerdo. filtro &lt;- resumen_1$variable %in% c(&#39;beneficios_fam_trabajo&#39;,&#39;beneficios_igualdad&#39;,&#39;beneficios_pobreza&#39;) resumen_1$media[filtro] &lt;- 1-resumen_1$media[filtro] resumen_1$variable &lt;- reorder(resumen_1$variable, resumen_1$media, mean) ggplot(resumen_1, aes(x=variable, y=media, ymin=media-ee, ymax=media+ee, colour=factor(cluster), group=cluster)) + geom_point() + coord_flip() + geom_line() + geom_linerange() Y notamos dos grupos claros que esperaríamos ver. ¿Cómo la describirías? Esta puede ser una solución aceptable (por ejemplo, si vamos a usar los grupos en otro modelo, como parte de diseños de estrategias de comunicación, etc.) Intentemos ahora con 5 grupos: sol_cl &lt;- ajustes_km[[5]] table(sol_cl$cluster) #&gt; #&gt; 1 2 3 4 5 #&gt; 450 417 573 306 362 Todos los grupos tienen tamaño razonable. No queremos tener grupos muy chicos, pues entonces es difícil caracterizarlos o entenderlos: si hay 15 personas en un grupo, cualquier resumen de este grupo estaría sujeto a variación muestral alta. Consideramos las variables originales: cluster_df &lt;- data_frame(idno = dat_3$idno, cluster = sol_cl$cluster) dat_4 &lt;- dat_3 %&gt;% gather(variable, valor, ausencia_cuidar_enfermos:trabajo_a_todos) %&gt;% left_join(cluster_df) resumen_5 &lt;- dat_4 %&gt;% group_by(cluster, variable) %&gt;% summarise(media = mean(valor), ee = sd(valor)/sqrt(length(valor))) ## adicionalmente, invertimos las 3 preguntas en escala de 1 a 5, pues 1 representa mayor acuerdo. filtro &lt;- resumen_5$variable %in% c(&#39;beneficios_fam_trabajo&#39;,&#39;beneficios_igualdad&#39;,&#39;beneficios_pobreza&#39;) resumen_5$media[filtro] &lt;- 1-resumen_5$media[filtro] resumen_5$variable &lt;- reorder(resumen_5$variable, resumen_5$media, mean) ggplot(resumen_5, aes(x=variable, y=media, ymin=media-ee, ymax=media+ee, colour=factor(cluster), group=cluster)) + geom_point() + coord_flip() + geom_line() + geom_linerange() Y en estos casos es especialmente útil perfilar los grupos, es decir, mostrar las diferencias en las medias de cada grupo con respecto a la media general: resumen_perfil_5 &lt;- resumen_5 %&gt;% group_by(variable) %&gt;% mutate(perfil = media - mean(media)) ggplot(resumen_perfil_5, aes(x = variable, y = perfil, colour = factor(cluster), group = cluster)) + geom_point() + coord_flip() + geom_line() + facet_wrap(~cluster) + geom_hline(yintercept=0, colour=&#39;gray&#39;) ¿Cómo les llamarías a cada uno de estos grupos? Observación: heterogeneidad en uso de escalas Los datos en escalas de acuerdo, que son usados frecuentemente en encuestas sociales y de negocios, tienen dificultades adicionales: Desde el punto de vista estadístico, usamos una medición ordinal como si fuera numérica. Esto sugiere utilizar técnicas de clustering más complicadas adaptadas a datos ordinales. Pero en realidad éste es un aspecto menor en el análisis de este tipo de datos. La dificultad grande en el análisis de este tipo de datos es la heterogeneidad en el uso de la escala. Esto quiere decir que no todas las personas usan escalas 1-10 (o 1-5, o 1-100, o Totalmente de acuerdo-Totalmente en desacuerdo) de la misma manera. Hay algunos que usan todo el rango de la escala, otros que se concentran en la mitad, etc. y muchas veces eso no tienen que ver sólo con el verdadero nivel de acuerdo o desacuerdo de la persona, sino también con cómo usa el lenguaje cada persona. El verdadero problema es entonces en la medición, no que tratemos como numérica a una variable que no lo es. Los datos de una persona no son realmente directamente comparables con los de otra persona. Hay maneras de lidiar con esto: por ejemplo, centrar los niveles de respuesta de cada persona, usar modelos que intentan medir la heterogeneidad de uso en la escala y separar los niveles de acuerdo, pero lo mejor en estos casos siempre es (si es posible) mejorar la medición. 16.5 Dificultades en segmentación/clustering. Aunque la idea conceptual de clustering es más o menos clara, en la práctica es una tarea difícil. Vamos a empezar apuntando dificultades que se comunmente se encuentran: La estructura de grupos naturales que buscamos no es necesariamente de clusters compactos. No existen grupos naturales En dimensiones altas (digamos &gt; 30) muchas veces todos los puntos están a distancias similares entre ellos, especialmente cuando hay variables que aportan la separación entre grupos. Dificultades en la selección de medida de distancia (o disimilitud). 16.5.1 Estructuras no compactas En algunos casos se dice que k-medias no tiene supuestos, otros dicen que tienen supuestos de clusters esféricos, etc. k-medias es un algoritmo, no es un modelo. Así que en realidad no tiene supuestos en el sentido típico. Lo importante es entender la cantidad que estamos minimizando. Si lo que realmente queremos hacer es minimizar esta cantidad, entonces k-medias nos devuelve una solución razonable. Pero hay veces que no queremos minimizar esta cantidad. Un ejemplo clásico es el siguiente: theta &lt;- runif(200,0,2*pi) r &lt;- c(runif(100,0,0.3), runif(100,0.8,1)) df &lt;- data.frame(x=r*cos(theta), y=r*sin(theta)) df$grupo &lt;- kmeans(df, centers=2, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() ¿Por qué falla k-medias? Porque la estructura de grupos que estábamos buscando no es una donde los clusters están definidos por distancia baja a su centro. Aquí realmente queremos otra cosa más complicada, como clusters definidos por cantidades invariantes (en este caso, distancia al origen). Sin embargo, si lo que nos interesa es distancia baja a un centroide, entonces esta solución es razonable para k=2: theta &lt;- runif(200,0,2*pi) r &lt;- c(runif(100,0,0.3), runif(100,0.8,1)) df &lt;- data.frame(x=r*cos(theta), y=r*sin(theta)) df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() Como ejemplo, pensemos que el espacio es un espacio de “gustos por películas”. Aún cuando podría ser muy interesante descubrir esta estructura concéntrica, ¡el grupo exterior contiene personas con gustos diametralmente opuestos! El problema de hacer dos clusters, uno central y otro concéntrico puede ser un poco artificial. 16.5.2 Existencia o no de grupos “naturales” Otro punto que se discute usualmente es si hay o no grupos naturales, que se refiere a grupos bien compactos y diferenciados entre sí, como en el ejemplo inicial ggplot(filter(iris, Species %in% c(&#39;setosa&#39;,&#39;versicolor&#39;)), aes(x=Sepal.Length, y=Petal.Width)) + geom_point() Pero es común, por ejemplo, encontrar cosas como siguen: set.seed(90902) df &lt;- data.frame(x = rnorm(500,0,1), y = rnorm(500,0,1)) df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() Nótese que k-medias logró encontrar una buena solución, y esta solución puede ser muy útil para nuestros fines (agrupa puntos “similares”). Sin embargo, en esta situación debemos reconocer que los tamaños, las posiciones, y el número de grupos es fundamentalmente arbitrario, y una “buena” solución depende de nuestros fines. Si corremos otra vez el algoritmo, vemos que los grupos encontrados son similares: df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() Sin embargo, si tomamos otra muestra distinta set.seed(909021) df &lt;- data.frame(x = rnorm(500,0,1), y = rnorm(500,0,1)) df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() La solución es bastante diferente. Esta diferencia no se debe al comienzo aleatorio del algoritmo. Se debe más bien a que los grupos se están definiendo por variación muestral, y pequeñas diferencias en las muestras. **En esta situación, debemos entender lo arbitrario de la solución, y considerar si una solución así es útil para nuestros fines. Esto no le quita necesariamente utilidad a la segmentación resultante, pero hay que recordar que los grupos que encontramos son en ciertos aspectos arbitrarios. Ejemplo También en esta situación puede ser que el criterio para segmentar no es uno apropiado para un algoritmo de segmentación. Por ejemplo, supongamos que un fabricante de zapatos nos pide segmentar a sus clientes en términos de su estatura y su tamaño de pie. Observamos: set.seed(909021) x &lt;- rnorm(200, 160, 15 ) df &lt;- data.frame(estatura=x, pie= 6 + 2*(x-160)/30 + rnorm(200,0,0.5)) ggplot(df, aes(x=estatura, y=pie)) + geom_point() ¿Dónde cortamos los grupos? Aunque cualquier algoritmo no supervisado nos va a dar una respuesta, muy posiblemente sería buena idea encontrar puntos de cortes definidos de otra manera (por ejemplo algo tan simple como cuantiles!). Algo similar ocurre en la segmentación por actitudes/ideología: no hay “buenos” y “malos” o “saludables” y “descuidados”, sino continuos a lo largo de estas actitudes. Grupos en dimensión alta En dimensión más alta (50 variables, 10 casos) observamos cosas como la siguiente: mat_1 &lt;- rbind(matrix(rnorm(10*50), ncol=50)) mat_1[1:10,1] &lt;- mat_1[1:10,1] + 10 dist(mat_1, method = &#39;euclidean&#39;) #&gt; 1 2 3 4 5 6 7 8 9 #&gt; 2 9.9 #&gt; 3 9.3 9.2 #&gt; 4 9.9 10.0 9.4 #&gt; 5 9.3 10.6 10.8 9.1 #&gt; 6 10.7 11.7 10.5 9.9 10.4 #&gt; 7 8.2 11.8 10.1 11.2 11.1 12.4 #&gt; 8 9.6 10.0 10.0 9.8 9.6 10.0 11.3 #&gt; 9 9.9 9.5 9.3 9.9 9.4 10.6 11.1 9.4 #&gt; 10 8.2 10.1 9.9 9.9 10.0 9.9 10.3 10.8 9.4 donde todos los puntos están a más o menos la misma distancia, aún cuando existe una estructura de grupos natural. En dimensión baja, la situación se ve muy diferente: mat_1 &lt;- rbind(matrix(rnorm(10*2), ncol=2)) mat_1[1:5,1] &lt;- mat_1[1:5,1] + 10 dist(mat_1, method = &#39;euclidean&#39;) #&gt; 1 2 3 4 5 6 7 8 9 #&gt; 2 1.43 #&gt; 3 2.42 3.76 #&gt; 4 1.05 1.66 2.31 #&gt; 5 2.19 1.36 4.59 2.86 #&gt; 6 9.00 9.12 10.28 10.04 7.88 #&gt; 7 11.85 11.95 13.06 12.89 10.69 2.85 #&gt; 8 8.23 8.26 9.63 9.26 6.99 0.95 3.69 #&gt; 9 10.72 10.72 12.09 11.75 9.44 1.83 1.38 2.50 #&gt; 10 10.86 10.65 12.49 11.85 9.30 2.82 2.70 3.00 1.63 ggplot(data.frame(mat_1), aes(x=X1, y=X2)) + geom_point() Y cualquier técnica razonable que usemos lograría encontrar estos grupos. Muchas veces pueden encontrarse mejores soluciones aplicando alguna técnica de reducción de dimensionalidad antes de hacer clustering. Dificultades en la selección de métrica Muchas veces no hay una métrica natural para el problema. En este caso, muchas veces escogemos distancia euclideana (con variables estandarizadas o no, dependiendo de sus escalas). La inclusión de variables categóricas plantea distintas alternativas que dan resultados distintos. En dimensión alta, incluso con variables numéricas, no siempre es claro que peso debería tener cada variable. Los métodos básicos de clustering generalmente producen una solución bien definida en problemas dimensión baja con clusters razonablemente bien definidos, donde hay una métrica de distancia natural. En otros casos, como: Dimensión alta con muchas variables ruidosas o que no aportan en la definición de los clusters. Estructuras relativamente dispersas que quisiéramos agrupar Clusters no bien definidos. Dificultad en escoger una métrica única apropiada para el problema, el resultado puede depender mucho del algoritmo, el criterio del analista, y la muestra de entrenamiento. Eso no quiere decir que la segmentación de casos que produce el algoritmo no sea útil, más bien que es difìcil obtener grupos naturales "],
["conglomerados-clustering-2.html", "Clase 17 Conglomerados (clustering) 2 17.1 Clustering jerárquico", " Clase 17 Conglomerados (clustering) 2 .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } library(tidyverse) 17.1 Clustering jerárquico "],
["referencias.html", "Referencias", " Referencias "]
]
