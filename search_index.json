[
["index.html", "Estadística Aplicada III Temario y referencias", " Estadística Aplicada III Andreu Boada de Atela 2018-02-01 Temario y referencias Todas las notas y material del curso estarán en este repositorio. Análisis exploratorio de datos Análisis de datos categóricos Regresión logística Regularización y selección de modelos Análisis de conglomerados: clustering jerárquico, k-medias Componentes principales Análisis de factores Análisis canónico Análisis discriminante Introducción a estadística espacial Evaluación Tareas semanales (20%) Examen parcial (30% práctico, 20% teórico) Un examen final (30% práctico) Software: R y Rstudio R Sitio de R (CRAN) Rstudio Interfaz gráfica para trabajar en R. Recursos para aprender R Referencia principal Johnson, R., y Wichern, D. Applied multivariate statistical analysis. Upper Saddle River, N. J.: Pearson Prentice Hall, 2007. Otras referencias Pattern Recognition and Machine Learning The Elements of Statistical Learning, Hastie, Tibshirani y Friedman Agresti, Alan. Categorical data analysis. 3rd ed. Hoboken, N. J.: John Wiley &amp; Sons, 2013. Greenacre, Michael J., ed. Multiple correspondence analysis and related methods. Boca Raton, Fla.: Chapman &amp; Hall/CRC, 2006 Thompson, Bruce. Canonical correlation analysis : uses and interpretation. Thousand Oaks, Calif.: Sage Publications, 1984 Jackson, J. Edward. A user’s guide to principal components. New York: John Wiley &amp; Sons, 1991. Harrell, Frank E. Regression modeling strategies : with applications to linear models, logistic regression, and survival analysis. New York: Springer, c2001. McLachlan, Geoffrey J. Discriminant analysis and statistical pattern recognition. New York: John Wiley &amp; Sons, 1992. Tareas Enviar tareas por correo electrónico a: andreuboadadeatela@gmail.com con el asunto “EAPLICADA3-Tarea-[XX]-[clave única 1]-[clave única 2]” donde [XX] es el número de la tarea, [clave única 1] y [clave única 2] son tu clave y la de tu compañero con quien vas a trabajar durante el semestre. "],
["intro.html", "Clase 1 Introducción 1.1 ¿Por qué un análisis multivariado? 1.2 La paradoja de Simpson 1.3 Modelos log-lineales 1.4 Interpretación de parámetros 1.5 Otros ejemplos 1.6 Tarea", " Clase 1 Introducción .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } La investigación científica es un proceso de aprendizaje iterativo. Para explicar un fenómeno físico o social primero se deben especificar los objetivos de una investigación y luego probar los objetivos a través de la recopilación y el análisis de datos pertinentes. A su vez, el análisis de los datos recopilados (experimentalmente o mediante observación) generalmente sugerirá una explicación modificada del fenómeno. A lo largo de este proceso iterativo, generalmente se añaden o se excluyen variables del análisis. Por lo tanto, la complejidad de la mayoría de los fenómenos requieren que el investigador recolecte muchas variables de las observaciones. En este curso veremos una introducción a los modelos estadísticos que son multivariados, es decir, modelos en los cuales los datos corresponden a mediciones en muchas variables. 1.1 ¿Por qué un análisis multivariado? Las razones por las cuales se utilizan modelos multivariados son generalmente (Johnson, Wichern, and others 2014): Factores de “confusión”. Un factor de confusión (o confound variable) es una variable que puede correlacionarse con otra variable de interés. Las correlaciones espurias son un posible tipo de confusión, donde el factor de confusión hace que una variable sin importancia real parezca ser importante. Pero las confusiones pueden ocultar variables reales importantes tan fácilmente como pueden producir falsas. Un ejemplo de esto, conocido como la “paradoja” de Simpson, consiste de datos en los cuales la dirección de una aparente asociación entre un predictor y la variable respuesta se puede revertir al considerar un factor de confusión. 1.2 La paradoja de Simpson La paradoja de Simpson, también conocida como el efecto Yule-Simpson, ocurre cuando existe una asociación entre dos variables en varios grupos pero la dirección de esta asociación se invierte cuando los datos se combinan para formar un solo grupo. En un análisis de los scores de SAT (examen de posicionamiento para la universidad) en Estados Unidos en 1997 se encontró que había una relación negativa entre el salario promedio anual de los maestros y el score total promedio de los alumnos que presentaron el SAT: library(tidyverse) sat &lt;- read_csv(&quot;datos/sat.csv&quot;) ggplot(sat, aes(x = teacher_salary, y = total_score)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + xlab(&quot;Salario anual promedio (en miles) de maestros&quot;) + ylab(&quot;Score total promedio de SAT&quot;) Podemos revisar el resultado de la regresión lineal (haciendo uso del paquete stargazer): library(stargazer) out1 &lt;- lm(formula = total_score ~ teacher_salary, data = sat) stargazer(out1, type = &#39;html&#39;, style = &quot;all&quot;, single.row = T, title = &quot;Regresión lineal del promedio de sueldo de maestros vs SAT promedio&quot;) Regresión lineal del promedio de sueldo de maestros vs SAT promedio Dependent variable: total_score teacher_salary -5.540*** (1.630) t = -3.390 p = 0.002 Constant 1,159.000*** (57.700) t = 20.100 p = 0.000 Observations 50 R2 0.193 Adjusted R2 0.177 Residual Std. Error 67.900 (df = 48) F Statistic 11.500*** (df = 1; 48) (p = 0.002) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Podemos observar que el coeficiente de la variable salario es \\(-5.54\\) y es significartivo según la prueba de hipótesis correspondiente. Desafortunadamente, la asociación entre salario y score de SAT parece ser negativa: a medida que aumenta el salario, se predice que el score SAT promedio disminuye. Afortunadamente para los maestros, una vez que se cuenta la variable de la fracción de alumnos que presentan el SAT, vemos una relación positiva estadísticamente significativa: out2 &lt;- lm(formula = total_score ~ teacher_salary + perc_take_sat, data = sat) stargazer(out2, type = &#39;html&#39;, style = &quot;all&quot;, single.row = T, title = &quot;Incluyendo el factor de confusión&quot;) Incluyendo el factor de confusión Dependent variable: total_score teacher_salary 2.180** (1.030) t = 2.120 p = 0.040 perc_take_sat -2.780*** (0.228) t = -12.200 p = 0.000 Constant 988.000*** (31.900) t = 31.000 p = 0.000 Observations 50 R2 0.806 Adjusted R2 0.797 Residual Std. Error 33.700 (df = 47) F Statistic 97.400*** (df = 2; 47) (p = 0.000) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Podemos ver que el coeficiente de la regresión lineal correspondiente al salario de los maestros se invierte. Gráficamente podemos visualizar este efecto: Vemos que dentro de cada grupo, la pendiente es positiva (o al menos no negativa). También podríamos agregar etiquetas: Otro ejemplo de este fenómeno es cuando la Universidad de California, Berkeley fue demandada por discrimanción hacia mujeres que habían solicitado admisión a un posgrado en 1973. De acuerdo con las estadísticas de admisión, los hombres que presentaron la solicitud tenían mayor probabilidad de ser admitidos que las mujeres, y la diferencia era tan sustancial que uno podría concluir que efectivamente había existido dicha discriminación. Sin embargo, al examinar los datos individualmente por departamento, parecía que no había una diferencia significativa en contra de las mujeres. ucb &lt;- UCBAdmissions %&gt;% as.tibble() ucb %&gt;% head(10) %&gt;% knitr::kable() Admit Gender Dept n Admitted Male A 512 Rejected Male A 313 Admitted Female A 89 Rejected Female A 19 Admitted Male B 353 Rejected Male B 207 Admitted Female B 17 Rejected Female B 8 Admitted Male C 120 Rejected Male C 205 Los datos contienen el número de solicitudes y admisiones por género a seis escuelas de postgrado diferentes. Analicemos una tabla de contingencia entre la variable género y la variable admitido: tab &lt;- ucb %&gt;% group_by(Gender, Admit) %&gt;% summarise(p = sum(n)) %&gt;% spread(Admit, p) tab %&gt;% knitr::kable() Gender Admitted Rejected Female 557 1278 Male 1198 1493 Los solicitantes masculinos tenían una tasa de aceptación del 44.52%, en comparación con solo el 30.35% de las mujeres, condicionando en la variable de género: tab %&gt;% gather(Admit, n, -Gender) %&gt;% mutate(prop = round(prop.table(n) * 100,2)) %&gt;% select(-n) %&gt;% spread(Admit, prop) %&gt;% knitr::kable() Gender Admitted Rejected Female 30.4 69.7 Male 44.5 55.5 Incluso podemos proporcionar una prueba estadística para apoyar la afirmación de que hubo sesgo en las admisiones. En R, se puede realizar una prueba de proporciones a través de la función prop.test(): prop.test(tab %&gt;% ungroup() %&gt;% select(-Gender) %&gt;% as.matrix()) #&gt; #&gt; 2-sample test for equality of proportions with continuity #&gt; correction #&gt; #&gt; data: tab %&gt;% ungroup() %&gt;% select(-Gender) %&gt;% as.matrix() #&gt; X-squared = 90, df = 1, p-value &lt;2e-16 #&gt; alternative hypothesis: two.sided #&gt; 95 percent confidence interval: #&gt; -0.170 -0.113 #&gt; sample estimates: #&gt; prop 1 prop 2 #&gt; 0.304 0.445 A partir de la prueba de hipótesis realizada anteriormente, se puede concluir que hay una diferencias significativa entre la proporción de hombres admitidos y la proporción de mujeres admitidas en los programas de posgrado. Sin embargo, si hacemos el mismo análisis por departamento, veremos que las diferencias ya no son tan significativas: ucb %&gt;% spread(Admit, n) %&gt;% mutate(total = Admitted + Rejected, porc = round(Admitted/total*100,2)) %&gt;% select(Gender,Dept,porc) %&gt;% spread(Gender, porc) %&gt;% knitr::kable() Dept Female Male A 82.41 62.1 B 68.00 63.0 C 34.06 36.9 D 34.93 33.1 E 23.92 27.8 F 7.04 5.9 Condicionando por departamento ahora vemos que las mujeres realmente tienen mayores tasas de admisión en cuatro de los seis departamentos (A, B, D, F). ¿Cómo puede ser esto? En realidad las diferencias tienen que ver con el porcentaje de solicitantes (hombres y mujeres) que son admitidos por departamentos, es decir, hay departamentos más competitivos que otros. Consideremos un modelo log-lineal. Sea \\(p_{ijk}\\) la proporción de la población en la celda \\((i,j,k)\\). Por ejemplo, \\(p_{112}\\) es la proporción de solicitantes que son admitidos, son hombres y se postulan para el Departamento B. tab &lt;- xtabs(n ~ ., ucb) llout &lt;- loglin(tab, list(1:2,c(1,3),2:3),param=TRUE) #&gt; 9 iterations: deviation 0.0492 Los efectos globales de la variable de admitidos son: llout$param$Admit %&gt;% knitr::kable() x Admitted -0.321 Rejected 0.321 Los efectos globales de la variable Departamento, por ejemplo, nos dicen qué departamentos tienden a tener más solicitantes (el A, el C y el D), aunque en realidad esto no sea tan relevante: llout$param$Dept %&gt;% knitr::kable() x A 0.154 B -0.765 C 0.540 D 0.430 E -0.029 F -0.330 Los efectos globales de la variable género también nos dicen que el número de solicitantes hombres es mayor que el número de solicitantes que son mujeres: llout$param$Gender %&gt;% knitr::kable() x Female -0.329 Male 0.329 Analicemos ahora los términos de interacciones entre variables. Como la variable que nos interesa es la de admisión, veamos la interacción de Admit-Gender y Admit-Department: llout$param$Admit.Gender %&gt;% knitr::kable() Female Male Admitted 0.025 -0.025 Rejected -0.025 0.025 llout$param$Admit.Dept %&gt;% knitr::kable() A B C D E F Admitted 0.637 0.615 0.006 -0.01 -0.232 -1.02 Rejected -0.637 -0.615 -0.006 0.01 0.232 1.02 En términos de asociación con la variable de admisión, la relación con la variable de departamento es mucho más fuerte que con la de género, lo que significa que la mayoría de los parámetros estimados son mucho más grandes en el primer caso. En otras palabras, el departamento es la variable más importante, no el género. Más aún, los resultados anteriores también muestran que existe una interacción Admit-Female positiva, es decir, que a las mujeres les va un poco mejor que a los hombres en cuanto a la admisión. 1.3 Modelos log-lineales Denotemos los 3 factores por \\(X^{(s)}\\), \\(s=1,2,3\\). En este caso particular, \\(X^{(1)}\\) es la variable de admisión Admit, y toma valores de \\(1\\) y \\(2\\), admitido y no admitido respectivamente. \\(X^{(2)}\\) la variable género tomaría valores \\(1\\) y \\(2\\) para hombre y mujer respectivamente, mientras que \\(X^{(3)}\\), la variable de departamento, toma valores del \\(1\\) al \\(6\\) para representar los departamentos A al F. En esta notación no estamos usando variables indicadoras. Estamos trabajando con variables estrictamente categóricas, cuyos valores son meramente etiquetas. Sea \\(X_r^{(s)}\\) el valor de \\(X^{(s)}\\) para el \\(i\\)-ésimo solicitante en la muestra, para \\(r=1,2,\\ldots,n\\). Aquí el número de observaciones es el número de solicitantes $n=$4526. Nuestros datos son los conteos en todas las categorías: \\[ N_{ijk} = \\mbox{nú}\\;\\mbox{mero de solicitantes }r\\mbox{ tales que }X_r^{(1)}=i, X_r^{(2)}=j,\\mbox{ y } X_r^{(3)}=k. \\] A esto le llamamos una tabla de contigencias en tres variables. Cada valor \\(N_{ijk}\\) es una celda de la tabla. Sea \\(p_{ijk}\\) la proporción poblacional de un solicitante elegido al azar en la celda \\((i,j,k)\\), es decir, \\[ p_{ijk} = P(X^{(1)}=i \\mbox{ y } X^{(2)}=j \\mbox{ y } X^{(3)}=k) = E(N_{ijk})/n. \\] Como se mencionó, nos interesan las relaciones entre las factores, en forma de independencia, tanto plena como parcial. De hecho, es común que un analista ajuste sucesivamente modelos más refinados a los datos, asumiendo cada uno una estructura de dependencia más compleja que la anterior. Esto se desarrollará en detalle a continuación. Considere primero el modelo que asume la independencia total: \\[ \\begin{eqnarray*} p_{ijk} &amp; = &amp; P(X^{(1)}=i \\mbox{ y } X^{(2)}=j \\mbox{ y } X^{(3)}=k) \\\\ &amp;=&amp; P(X^{(1)}=i) \\cdot P(X^{(2)}=j) \\cdot P(X^{(3)}=k). \\end{eqnarray*} \\] Tomando logaritmo de ambos lados, vemos que la independencia de los tres factores es equivalente a escribir una ecuación de la forma \\[ \\mbox{log}(p_{ijk}) = a_i + b_j + c_k, \\] donde \\(a_i,b_j,c_k\\) son cantidades estimadas. Por ejemplo, \\[ b_j = \\mbox{log}\\left(P(X^{(2)}=j)\\right). \\] El punto es que el modelo es similar a un modelo de regresión lineal sin interacciones. El análogo de que no haya interacción entre las variables aquí está representado por el supuesto de independencia. Por ejemplo, si suponemos que Departamento es independiente de Admisión y Género, pero que Admisión y Género no son independientes entre sí, el modelo incluiría un término de interacción \\(i-j\\): \\[ p_{ijk} = P(X^{(1)}=i, X^{(2)}=j)\\cdot P(X^{(3)} = k), \\] por lo que el modelo sería \\[ \\mbox{log}(p_{ijk}) = a_{ij} + b_k. \\] La mayoría de los modelos formales reescriben esto como \\[ a_{ij} = u + v_i + w_j + r_{ij}, \\] de tal forma que el término de interacción \\(P(X^{(1)}=i \\mbox{ y } X^{(2)}=j)\\) es una suma de un “efecto global” \\(u\\), “efectos principales” \\(v_i\\) y \\(w_j\\) y “efectos de interacción” \\(r_{ij}\\), nuevamente de forma análoga a la regresión lineal. Sin embargo, tenga en cuenta que esto en realidad nos da demasiados parámetros. Para el término de interacción \\(a_{ij}\\) del modelo, tenemos \\(2 \\times 3 = 6\\) probabilidades reales, pero \\(1 + 2 + 2 + 2\\times 2 = 9\\) parámetros (1 para \\(u\\), 2 para \\(v_i\\) y así sucesivamente). Por esta razón, generalmente los modelos tienen restricciones de la forma \\[ \\displaystyle{\\sum_i{v_i}}=0. \\] Es posible enumerar todas las restricciones, aunque en la mayoría de los modelos aún con restricciones el número de parámetros puede ser muy grande. ¿Qué modelo es más apropiado en el ejemplo anterior? \\(p_{ijk} = P(X^{(1)}=i) \\cdot P(X^{(2)}=j, X^{(3)}=k)\\) \\(p_{ijk} = P(X^{(1)}=i, X^{(2)}=j, X^{(3)}=k)\\) \\(p_{ijk} = P(X^{(1)}=i)\\cdot P(X^{(2)}=j)\\cdot P(X^{(3)}=k)\\) \\(p_{ijk} = P(X^{(1)}=i, X^{(3)}=k) \\cdot P(X^{(2)}=j)\\) Otro posible modelo tendría Admitido y Género condicionalmente independientes, dado Departamento, lo que significa que en cualquier género, la proporción de admitidos y su género, no están relacionados. Escribimos el modelo de esta manera \\[ \\begin{eqnarray*} p_{ijk} &amp;=&amp; P(X^{(1)}=i, X^{(2)}=j, X^{(3)}=k) \\\\ &amp;=&amp; P(X^{(1)}=i, X^{(2)}=j|X^{(3)}=k) \\cdot P(X^{(3)}=k) \\\\ &amp;=&amp; P(X^{(1)}=i|X^{(3)}=k) \\cdot P(X^{(2)}=j|X^{(3)}=k) \\cdot P(X^{(3)}=k), \\end{eqnarray*} \\] y el modelo sería de la forma \\[ \\mbox{log}(p_{ijk}) = u + a_i + f_{ik} + b_j + h_{jk} + c_k. \\] ¿Cuántos parámetros tendría este modelo? 30 35 40 45 1.4 Interpretación de parámetros Consideremos los modelos: \\[ \\begin{eqnarray*} \\mbox{(1)} &amp; \\qquad &amp; \\mbox{log}(p_{ijk}) = a_{ij} + b_k, \\\\ \\mbox{(2)} &amp; \\qquad &amp; \\mbox{log}(p_{ijk}) = u + a_i + f_{ik} + b_j + h_{jk} + c_k. \\end{eqnarray*} \\] La independencia que representa el modelo (1) tiene una interpretación muy diferente a las independencias representadas por el modelo (2). 1.4.1 Ejemplo: dos monedas Supongamos que tenemos una gran caja con monedas de dos tipos. Las monedas de tipo 1 tienen probabilidad \\(p\\) de salir águila, y las monedas tipo 2 tienen probabilidad \\(q\\) de salir águila. Una proporción \\(r\\) de las monedas es del tipo 1. Seleccionamos una moneda al azar de la caja, lanzamos esa moneda \\(M\\) veces, y observamos \\(N\\) águilas. ¿Cuál es la distribución de \\(N\\)? \\[ p_N(k) = r \\dbinom{M}{k}p^k (1-p)^{M-k} + (1-r) \\dbinom{M}{k}q^k (1-q)^{M-k}, \\] donde \\(k=0,1,\\ldots,M\\). Es fácil ver por qué a esta distribución se le conoce como modelo de mezcla. Esta función de distribución de probabilidad es una mezcla de dos funciones de distribución de probabilidad binomiales, con proporciones de mezcla \\(r\\) y \\(1-r\\). Si \\(M\\) es una variable aleatoria con soporte (o rango) \\(R\\) y \\(\\left\\{g_t\\right\\}_{t\\in R}\\) es una colección de funciones de densidad, entonces se dice que \\(h\\) es una función de densidad de mezcla si \\[ h = \\displaystyle{\\sum_{k\\in R}p_M(k)g_k}, \\] cuando \\(M\\) es una variable aleatoria discreta, o si \\[ h = \\displaystyle{\\int_{t\\in R}f_M(t)g_t(u)\\;\\;dt}, \\] cuando \\(M\\) es una variable aleatoria continua. En el ejemplo de la moneda: \\(Y = N\\) \\(S = \\{1,2\\}\\) \\(p_M(1) = r, \\quad p_M(2) = 1-r\\) \\(g_1\\) es la fn de densidad de una va \\(\\mbox{Binomial}(M,p)\\) \\(g_2\\) es la fn de densidad de una va \\(\\mbox{Binomial}(M,q)\\) Dada la elección de moneda, el resultado de la moneda 1 es independiente del resultado de la oneda 2. Sin embargo, los resultados de las monedas no son independientes si no se sabe qué moneda fue seleccionada. Este ejemplo es como el del modelo (2) mencionado arriba. Por otro lado, se sabe que la estatura y el peso están asociadas linealmente, pero ambas son independientes de la preferenvia por algún sabor de helado, una situación similar a la del modelo (1). Es importante considerar las interacciones que están consideradas en el modelo, por ejemplo: en el modelo (1) hay una interacción entre los factores \\(i,j\\) pero no hay interacción con el factor \\(k\\), por lo que \\(i\\) y \\(j\\) se modelan como completamente independientes de \\(k\\) y no entre sí, mientras que en el modelo (2) \\(i\\) y \\(j\\) tienen una interacción con \\(k\\) y no entre sí, por lo que \\(i\\) y \\(j\\) no son independientes entre sí, pero sí los son dado \\(k\\). Consideremos ahora el modelo \\[ \\mbox{log}(p_{ijk}) = u + a_i + f_{ik} + b_j + h_{jk} + l_{ij} + c_k, \\] ¿cuál de las siguientes afirmaciones es cierta? \\(i\\) y \\(j\\) son dependientes dado \\(k\\). \\(i\\) y \\(j\\) son independientes dado \\(k\\). \\(i\\) y \\(k\\) son dependientes dado \\(j\\). \\(i\\) y \\(k\\) son independientes dado \\(j\\). Si hubiéramos incluido un término \\(m_{ijk}\\), que ahora haría que el modelo estuviera lleno (o saturado), entonces sería posible que los factores \\(i\\) y \\(j\\) estuvieran altamente relacionados para algunos valores de \\(k\\), y menos relacionados para otros. Claramente, cuantas más variables tengamos, y cuanto mayor sea el orden de las interacciones que incluimos, más difícil será interpretar el modelo. 1.5 Otros ejemplos 1.5.1 Discriminación de residentes hispanos con discapacidades La mayoría de los estados en los Estados Unidos proporcionan servicios y apoyo a personas con discapacidades (por ejemplo, discapacidad intelectual, parálisis cerebral, autismo, etc.) y sus familias. La agencia a través de la cual el estado de California sirve al desarrollo de la población discapacitada es el Departamento de Servicios de Desarrollo de California (DDS). Una de las responsabilidades de DDS es asignar fondos que respalden a más de 250,000 residentes con discapacidades de desarrollo (denominados “consumidores”). Hace algunos años, se hizo una alegación de discriminación presentando un análisis univariado que examinaba los gastos anuales promedio de los consumidores por etnia. El análisis reveló que el gasto anual promedio en consumidores hispanos (Hispanic) era aproximadamente un tercera parte (1/3) del gasto promedio en consumidores blancos no hispanos (White non-Hispanic). Este hallazgo fue el catalizador para una mayor investigación; posteriormente, los legisladores estatales y los gerentes de departamento buscaron servicios de consultoría de un estadístico. El conjunto de datos utilizado en el análisis contiene seis variables: ID: identificador único por consumidor, Categoría de edad: es una variable importante porque, aunque la edad suele ser causa de discriminación legal, en este caso, para la población específica de Hispanos no americanos la edad no se consideraría relacionada con los casos de discriminación. El propósito de estas ayudas es que los que viven con alguna discapacidad puedan vivir igual que los que no tienen ninguna discapacidad. Es lógico, por lo tanto, que mientras las personas tienen mayor edad requieran de mayor ayuda económica. Los seis grupos de edad utilizados en este ámbito son: 0 a 5 años de edad, 6 a 12, 13 a 17, 18 a 21, 22 a 50, y más de 51. Edad: edad del consumidor, Género: se incluye en los datos como una variable a considerar porque el género es otro factor sujeto a discriminación, Gastos: el gasto anual que el gobierno le dedica a un consumidor para apoyar a estos individuos y sus familias. El gasto se dedica a: ayuda a la familia, servicios psicológicos, gastos médicos, transporte y costos relacionados a la vivienda como la renta. Origen étnico: es la variable más importante ya que con respecto a esta variable se presentaron las supuestas alegaciones por discriminación. Los primeros 10 renglones de la tabla se muestran a continuación: dds &lt;- read_csv(&quot;datos/californiaDDSData.csv&quot;) dds %&gt;% head(10) %&gt;% knitr::kable() Id Age Cohort Age Gender Expenditures Ethnicity 10210 13-17 17 Female 2113 White not Hispanic 10409 22-50 37 Male 41924 White not Hispanic 10486 0-5 3 Male 1454 Hispanic 10538 18-21 19 Female 6400 Hispanic 10568 13-17 13 Male 4412 White not Hispanic 10690 13-17 15 Female 4566 Hispanic 10711 13-17 13 Female 3915 White not Hispanic 10778 13-17 17 Male 3873 Black 10820 13-17 14 Female 5021 White not Hispanic 10823 13-17 13 Male 2887 Hispanic Se puede ver que la columna “Age Cohort” tiene la categoría de edad a la cual correponde la observación. También podemos utilizar la función glimpse() del paquete tibble: glimpse(dds) #&gt; Observations: 1,000 #&gt; Variables: 6 #&gt; $ Id &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, ... #&gt; $ `Age Cohort` &lt;chr&gt; &quot;13-17&quot;, &quot;22-50&quot;, &quot;0-5&quot;, &quot;18-21&quot;, &quot;13-17&quot;, &quot;13-17... #&gt; $ Age &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15... #&gt; $ Gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Fema... #&gt; $ Expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, ... #&gt; $ Ethnicity &lt;chr&gt; &quot;White not Hispanic&quot;, &quot;White not Hispanic&quot;, &quot;Hisp... Podemos ver que el conjunto de datos contiene una muestra de exactamente 1000 observaciones que fueron seleccionadas aleatoriamente. Veamos una tabla del gasto promedio por etnicidad: dds %&gt;% group_by(Ethnicity) %&gt;% summarise(Gasto_promedio = round(mean(Expenditures),0)) %&gt;% knitr::kable() Ethnicity Gasto_promedio American Indian 36438 Asian 18392 Black 20885 Hispanic 11066 Multi Race 4457 Native Hawaiian 42782 Other 3316 White not Hispanic 24698 Podemos comparar también con el promedio de todos los consumidores: mean(dds$Expenditures) #&gt; [1] 18066 Es común hacer gráficas de barras para representar medias, aunque en realidad, esto no es lo más recomendable: media_por_etnia &lt;- dds %&gt;% group_by(Ethnicity) %&gt;% summarise(Media_etnia = mean(Expenditures)) media_por_etnia$Etnia &lt;- reorder(media_por_etnia$Ethnicity, -media_por_etnia$Media_etnia, FUN = median) ggplot(media_por_etnia, aes(x = Etnia, y = Media_etnia)) + geom_bar(stat = &#39;identity&#39;) + theme(axis.text.x = element_text(angle=25)) ¿Se puede concluir que existe evidencia de discriminación contra los Hispanos en comparación con los blancos no hispanos? Sí No Con respecto a discriminación de género, usualmente concluiríamos que no hay evidencia de discriminación: dds %&gt;% group_by(Gender) %&gt;% summarise(Gasto_promedio = round(mean(Expenditures),0)) %&gt;% knitr::kable() Gender Gasto_promedio Female 18130 Male 18001 Las necesidades de los consumidores aumentan conforme envejecen, lo que resulta en mayores gastos para personas de mayor edad: dds$Age_Cohort &lt;- ordered(dds$`Age Cohort`, levels=c(&quot;0-5&quot;,&quot;6-12&quot;,&quot;13-17&quot;,&quot;18-21&quot;,&quot;22-50&quot;,&quot;51+&quot;)) dds %&gt;% group_by(Age_Cohort) %&gt;% summarise(Gasto_promedio = round(mean(Expenditures),0)) %&gt;% knitr::kable() Age_Cohort Gasto_promedio 0-5 1415 6-12 2227 13-17 3923 18-21 9889 22-50 40209 51+ 53522 El problema de que haya discrimnación o no se puede analizar más a fondo viendo, por ejemplo, qué porcentaje de consumidores pertenecen a cada etnia, tal como sucedió en el ejemplo de la controversia de Berkeley: dds %&gt;% group_by(Ethnicity) %&gt;% summarise(num_cons = n(), porc_cons = paste0(num_cons/10,&#39;%&#39;)) %&gt;% knitr::kable() Ethnicity num_cons porc_cons American Indian 4 0.4% Asian 129 12.9% Black 59 5.9% Hispanic 376 37.6% Multi Race 26 2.6% Native Hawaiian 3 0.3% Other 2 0.2% White not Hispanic 401 40.1% Podemos observar que los dos grandes grupos pertenecen a las 2 etnias del problema de discriminación que estamos analizando de blancos no hispanos vs hispanos. Examinemos de nuevo las medias y el porcentaje de consumidores de estos dos grupos: dds_blancos_hispanos &lt;- dds %&gt;% filter(Ethnicity %in% c(&quot;Hispanic&quot;,&quot;White not Hispanic&quot;)) dds_blancos_hispanos %&gt;% group_by(Ethnicity) %&gt;% summarise(media_gasto = mean(Expenditures), porc_consum = n()/10) %&gt;% knitr::kable() Ethnicity media_gasto porc_consum Hispanic 11066 37.6 White not Hispanic 24698 40.1 Tiende a haber un consenso general de que hay una diferencia significativa en la cantidad promedio de gastos entre el grupo de blancos no hispanos y el de hispanos. ¿Por qué podría haber diferencias en los promedios? ¿Se puede determinar si realmente existe discriminación? Algunas razones que sugieren normalmente son: Los hispanos tienen más apoyo familiar, y por lo tanto, es menos probable que busquen asistencia financiada por el gobierno, Los hispanos están menos informados sobre cómo buscar ayuda. Ambas razones son difíciles de modelar y podrían apoyar alegaciones de discriminación, en vez de negarlas. Analicemos ahora diferencias para cada grupo de edad entre hispanos y blancos: dds_blancos_hispanos %&gt;% group_by(Ethnicity, Age_Cohort) %&gt;% summarise(media_gasto = mean(Expenditures)) %&gt;% spread(Ethnicity, media_gasto) #&gt; # A tibble: 6 x 3 #&gt; Age_Cohort Hispanic `White not Hispanic` #&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0-5 1393 1367 #&gt; 2 6-12 2312 2052 #&gt; 3 13-17 3955 3904 #&gt; 4 18-21 9960 10133 #&gt; 5 22-50 40924 40188 #&gt; 6 51+ 55585 52670 ¿Se puede concluir que el típico hispano recibe menos fondos (es decir, gastos) que el típico blanco? Dado que la cantidad típica de gastos para los hispanos (en todos excepto un grupo de edad) es más alta que la cantidad típica de gastos para los blancos que no son hispanos en cada grupo de edad (excepto en uno), la hipótesis de discriminación sería refutada. Si un consumidor hispano fuera a reclamar discriminación porque es hispano (frente a blancos no hispanos), podría hacerlo con base en el promedio general de gastos para todos los consumidores de su grupo. Podemos entender mejor por qué esta aparente asociación desaparece cuando consideramos la variable de grupo de edad si analizamos el porcentaje de consumidores en cada categoría de edad para los grupos de hispanos y blancos: dds_blancos_hispanos %&gt;% group_by(Ethnicity) %&gt;% mutate(num_etnia = n()) %&gt;% ungroup() %&gt;% group_by(Ethnicity, Age_Cohort) %&gt;% summarise(porc_grupo_edad = round(n()/first(num_etnia)*100,2)) %&gt;% spread(Ethnicity, porc_grupo_edad) #&gt; # A tibble: 6 x 3 #&gt; Age_Cohort Hispanic `White not Hispanic` #&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0-5 11.7 4.99 #&gt; 2 6-12 24.2 11.5 #&gt; 3 13-17 27.4 16.7 #&gt; 4 18-21 20.7 17.2 #&gt; 5 22-50 11.4 33.2 #&gt; 6 51+ 4.52 16.5 Veamos estas medias como un promedio ponderado por grupo de edad: \\[ \\bar{X}_k = \\displaystyle{\\sum_{i=1}^{m}w_{ki}\\bar{X}_{ki}}, \\] donde \\(\\bar{X}_k\\) es la media del \\(k\\)-ésimo grupo étnico, \\(w_{ki}\\) es el porcentaje del \\(k\\)-ésimo grupo étnico en el \\(i\\)-ésimo grupo de edad, y \\(\\bar{X}_{ki}\\) es la media de gasto del \\(k\\)-ésimo grupo étnico en el \\(i\\)-ésimo grupo de edad. Los pesos \\(w_{ki}\\) para la población hispana son más altos para los 4 grupos de edad más jóvenes y más bajas para los 2 grupos de edad más viejos, en comparación con la población blanca no hispana. En otras palabras, la población total de consumidores hispanos es relativamente más joven en comparación con la población de consumidores blancos no hispanos. Dado que los gastos para los consumidores más jóvenes son más bajos, el promedio general de los gastos para los hispanos (frente a los blancos no hispanos) es menor. Factores de causa común. Incluso cuando no hay factores de confusión, un fenómeno puede realmente surgir de múltiples causas. Puede ocurrir que exista una correlación entre dos variables, sin embargo, es posible que esto no te diga nada cuando estos dos factores tienen como causa común a un tercer factor. Además, cuando la causalidad es múltiple, una causa puede ocultar a otra. 1.5.2 Consumo de chocolate y premios Nobel En un artículo reciente se publicó un resultado que demuestra una correlación estadísticamente significativa entre el consumo de chocolate per capita y el número de premios Nobel del país por 10 millones de habitantes. El artículo se puede consultar aquí: Messerli, 2012. En el artículo está publicada esta gráfica: Los datos de consumo de chocolate per cápita provienen de fuentes de datos distintas: confectionerynews.com, theobroma-cacao.de, y caobisco. Por otro lado, los datos del número de premios Nobel por cada 10 millones de habitantes están publicados en Wikipedia. Mientras que Messerli advierte en su artículo que la existencia de una correlación no implica causalidad, esto no impidió que los medios populares publicaran historias con estos titulares: “Eating Chocolate May Help You Win Nobel Prize” - CBS News There’s A Shocking Connection Between Eating More Chocolate And Winning The Nobel Prize - Business Insider “Why Chocolate Makes You Smart (or Peaceful)” - Psychology Today “Study links eating chocolate to winning Nobels” - USA Today Como describimos anteriormente, se tienen datos para varios años del consumo per capita de chocolate (en kg). chocolate_nobel &lt;- read_csv(&quot;datos/chocolate_nobel.csv&quot;) chocolate_nobel %&gt;% sample_n(10) %&gt;% knitr::kable() Country Year Cons_per_capita Nobel_Laureates Population_2017 Laureates_per_10_million US 2004 5.30 335 3.24e+08 10.325 Ireland 2006 7.64 2 4.67e+06 4.281 Switzerland 2011 10.55 21 8.48e+06 24.776 Hungary 2009 3.58 8 9.72e+06 8.229 Austria 2007 8.22 18 8.74e+06 20.606 Germany 2011 11.60 91 8.21e+07 11.082 Netherlands 2012 5.40 19 1.70e+07 11.153 Spain 2007 3.27 2 4.64e+07 0.431 Finland 2009 6.87 3 5.52e+06 5.432 US 2012 5.50 335 3.24e+08 10.325 Podemos ver una gráfica del consumo de chocolate vs el número de premios nobel: ggplot(chocolate_nobel, aes(x=Cons_per_capita, y = Laureates_per_10_million)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) El artículo original era más una nota sarcástica, que un artículo de investigación. Muchos artículos, blogs y medios mostraron que esta aparente correlación no tiene sentido. Estas críticas muestran que el número de Nobel para 10 millones de habitantes también está “correlacionado” con el PIB per cápita, el índice de desarrollo humano, el consumo de todo tipo de bienes de lujo, etc. Factores de interacción. Incluso cuando las variables no están correlacionadas por completo, la importancia de cada una puede depender de la otra. Por ejemplo, las plantas se benefician tanto de la luz como del agua. Pero en ausencia de cualquiera, el otro no es en absoluto beneficioso. Tales interacciones ocurren en una gran cantidad de sistemas. Por lo tanto, la inferencia efectiva sobre una variable generalmente dependerá de la consideración de otras variables. Inferencia causal. A pesar de su importancia central, todavía no existe un enfoque unificado para hacer inferencia causal en las ciencias o en estadística. Incluso hay personas que argumentan que la causa realmente no existe, que es una ilusión psíquica. Por ejemplo, en sistemas dinámicos complejos todo parece causar todo lo demás, por lo que el término “causa” pierde valor intuitivo. Sin embargo, existe un acuerdo general: la inferencia causal siempre depende de supuestos no verificables. Otra forma de decir esto es que siempre nos será posible imaginar alguna forma en la que la inferencia sobre la causa sea incorrecta, sin importar qué tan cuidadosamente se haya realizado el diseño o el análisis. En este curso nuestros análisis jamás van a pretender hacer alguna inferencia sobre la causa de los fenómenos observados; únicamente se harán afirmaciones sobre las asociaciones, interacciones y relaciones entre las variables en los datos observados. La mayoría de nuestros análisis se van a enfocar en datos que fueron obtenidos sin que activamente se controlara o manipulara cualquiera de las variables en las cuales se hicieron las mediciones. Los diseños estadísticos en los cuales se controla alguna de las variables observadas en los datos se estudian en el curso de Diseño de experimentos. Generalmente vamos a suponer que los factores (o variables) observados son aleatorios. Esto quiere decir que nuestro análisis va a estar basado en el supuesto de que los datos provienen de una muestra aleatoria de la población de interés en un momento determinado del tiempo. En muchas ramas de la estadística, contar con datos temporales es muy importante. Por ejemplo, en el curso de Análisis de supervivencia se estudia el uso de modelos estadísticos en aplicaciones en las cuáles se desea estimar la distribución de un período entre dos eventos, como la duración del empleo (tiempo transcurrido entre el contrato y el abandono de la empresa), del tiempo de vida de un paciente, la diferencia en algún beneficio terapéutico sobre la prolongación de la vida para un nuevo tratamiento con respecto al tratamiento tradicional, o el tiempo de falla en un sistema mecánico. Hoy en día, el uso de herramientas computacionales ha adquirido importancia en la ciencia estadística. Esto resultó en el desarrollo de nuevas técnicas computacionales para fines estadísticos, tales como el uso de muestreo para estimar cantidades estadísticas o parámetros (bootstrap), la simulación de variables aleatorias, la simulación de modelos probabilísticos, la simulación de modelos multivariados, y la inferencia de gráficas estadísticas. Estos temas se ven en el curso de Estadística computacional. La técnica de simulación ha sido esencial en los últimos años. Se ha desarrollado una clase de métodos de simulación para poder calcular la distribución posterior, estos se conocen como cadenas de Markov via Monte Carlo (MCMC por sus siglas en inglés). El desarrollo de los métodos MCMC es lo que ha propiciado el desarrollo de la estadística bayesiana en años recientes. En el curso de Estadística bayesiana se estudia la teoría básica que sirve de fundamento para la estadística bayesiana: la teoría de decisión, la probabilidad subjetiva, la utilidad, la inferencia como problema de decisión, y la inferencia paramétrica bayesiana. El curso de Regresión avanzada está diseñado para estudiar inferencia bayesiana y el uso de modelos estadísticos bayesianos en el análisis de aplicaciones reales actuales. El enfoque es en modelos lineales generalizados, modelos dinámicos y modelos jerárquicos o multinivel. El uso de la estadística computacional con el fin de hacer predicciones, aprovecha la optimización numérica para estudiar métodos que son útiles para reconocer patrones. En el curso de Aprendizaje estadístico se estudian modelos lineales para reconocimiento de patrones, clasificación, y predicción, la regresión múltiple y descenso en gradiente, las redes neuronales (y deep learning), máquinas de soporte vectorial, los árboles y bosques aleatorios. En el curso de Métodos analíticos se ven otras técnicas de minería de datos, tales como el análisis de market basket, local sensitivity hashing (LHS), la minería de flujos de datos, los algoritmos de recomendación y la minería de texto. Finalmente, debido a la importancia antes mencionada del uso de varias variables en los modelos estadísticos actuales, hay nuevas técnicas estadísticas para estudiar fenómenos multivariados desde una perspectiva bayesiana, como las redes bayesianas, los modelos gráficos no dirigidos, las redes markovianas, los modelos para datos faltantes, modelos de variables latentes, como los modelos de rasgos latentes (LTM), los modelos de perfiles latentes (LPM), los modelos de clases latentes (LCM), y los modelos markovianos de estados ocultos (HMM). Todas estas técnicas se ven en el curso de Estadística multivariada. En este curso nuestro enfoque tendrá una persepctiva frecuentista. 1.6 Tarea Recordemos que la devianza la definimos como \\(-2\\) multiplicado por la log-verosimilitud: \\[ D = -2\\, \\mbox{log}{\\left(p(X|\\hat{\\theta})\\right)} \\] donde \\(X\\) son los datos observados y \\(\\hat{\\theta}\\) es el parámetro a estimar. Generalmente nos interesa disminuir la devianza. Con los datos del ejemplo de discriminación a hispanos modela la probabilidad \\(p_{ijk}\\) de cada categoría \\((i,j,k)\\) correspondiente al gasto, raza y categoría de edad, respectivamente. Puedes utilizar la función loglin vista en clase o la función loglm del paquete MASS. Esta última te permite especificar el modelo en forma de función. Puedes ver la ayuda así: library(MASS) ?loglm Filtra los datos para aquellas observaciones que sólo sean de hispanos o blancos no hispanos y llena la siguiente tabla utilizando factores de interacción como lo vimos anteriormente con las variables G (categoría del gasto del gobierno en discapacitados), H (Hispano o Blanco no hispano), y E (Categoría de edad). Para esto deberás crear una variable de categoría de gasto del gobierno. Puede hacerlo utilizando cuantiles con la función cut2 del paquete Hmisc. Puedes usar el número de grupos que creas que es más adecuado. Por ejemplo, el modelo “G + H + E” representa el modelo de independencias. Para el modelo “GH + GE + HE” hay interacciones entre: GH: gasto en discapacitados y si es hispano, GE: gasto y edad, y HE: hispano y edad. El último modelo tendría parámetros \\(u\\), \\(a_i\\), \\(b_{ij}\\), \\(c_j\\), \\(d_{ik}\\), \\(e_{jk}\\), y \\(f_k\\) tales que \\[ \\mbox{log}(p_{ijk}) = u + a_i + b_{ij} + c_j + d_{ik} + e_{jk} + f_k. \\] Modelo Devianza Grados de libertad Número de parámetros G + H + E GH + E GE + H G + HE GH + GE GH + HE GH + GE + HE Di qué modelo es mejor, tanto en términos del ajuste de la devianza y su interpretabilidad y explica por qué. ¿Hay algún modelo que no esté en la lista que sea el más apropiado para ajustar los datos? Manda tu tarea por correo electrónico a: andreuboadadeatela@gmail.com con el asunto “EAPLICADA3-Tarea-[XX]-[clave única 1]-[clave única 2]” donde [XX] es el número de la tarea (en este caso es la tarea 01), y [clave única 1] y [clave única 2] son tu clave y la de tu compañero con quien vas a trabajar durante el semestre. Referencias "],
["Rintro.html", "Clase 2 Temas selectos de R 2.1 ¿Qué ventajas tiene R? 2.2 Flujo básico de trabajo para el análisis de datos en R. 2.3 Introducción a R como lenguaje de programación, y la plataforma interactiva de RStudio. 2.4 Estructuras de datos 2.5 R Markdown 2.6 Proyectos de RStudio 2.7 Otros aspectos importantes de R 2.8 Tarea", " Clase 2 Temas selectos de R .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } El lenguaje de programación R ha surgido como un avance en el desarrollo de software para análisis estadítico. Hace unos años era común el uso de productos de software proprietario, tales como GAUSS, RATS, EVIEWS, SPSS, SAS, Matlab, Minitab, Stata y software que en principio ni siquiera es apropiado para el análisis de datos, como Microsoft Excel. Estos programas generalmente son demasiado costosos y tienen un rendimiento bajo. Si es necesario hacer un análisis más complejo, entonces los archivos se vuelven demasiado grandes y el todo el proceso se vuelve infactible. 2.1 ¿Qué ventajas tiene R? R es la herramienta más sobresaliente para la estadística, el análisis de datos y el aprendizaje estadístico. Es más que un paquete estadístico; es un lenguaje de programación, por lo que puede crear sus propios objetos, funciones y paquetes. Hablando de paquetes, hay más de 12,000 innovadores paquetes aportados por los usuarios, y que están disponibles en CRAN (The Comprehensive R Archive Network), eso sin mencionar Bioconductor. Para tener una idea de qué paquetes hay disponibles, puedes leer posts en R-bloggers y ver el . Muchos paquetes son enviados por miembros prominentes de sus respectivos campos. Al igual que todos los programas, los programas de R documentan explícitamente los pasos de su análisis y esto facilita la reproducibilidad del análisis estadístico. Además, provee de herramientas para probar rápidamente muchas ideas y corregir fácilmente los problemas que puedan surgir. R puedes usarlo fácilmente en cualquier lugar. Es independiente de la plataforma, por lo que puede usarlo en cualquier sistema operativo. Y es gratis, por lo que puede usarlo en cualquier empleador sin tener que persuadir a su jefe para comprar una licencia. 2.1.1 R es gratuito y de código abierto R está disponible bajo una licencia de código abierto, lo que significa que cualquiera puede descargar y modificar el código. Esta libertad a menudo se conoce como la de software libre (“free as in speech”). R también está disponible de manera gratuita (“free as in beer”). En términos prácticos, esto significa que podemos descargar y usar R gratis. Otro beneficio, aunque un poco más indirecto, es que cualquiera puede acceder al código fuente, modificarlo y mejorarlo. Como resultado, muchos programadores excelentes han contribuido con mejoras y correcciones al código de R. Por esta razón, R es muy estable y confiable. Cualquier libertad también tiene asociadas ciertas obligaciones. En el caso de R, estas obligaciones se describen en las condiciones de la licencia bajo la cual se publica: Licencia Pública General de GNU (GPL), Versión 2. Estas obligaciones te pertienen si solamente haces uso de R. Sin embargo, si haces cambios en su código fuente R y lo redistribuyes, entonces estos cambios se deben poner a disposición de todos los usuarios. 2.1.2 R tiene una comunidad comprometida Muchas personas que usan R eventualmente comienzan a ayudar a los nuevos usuarios y proponen el uso de R en sus lugares de trabajo y círculos profesionales. Por ejemplo, si tienes dudas sobre algún aspecto de R, podrás encontrar ayuda en Stack Overflow. R-Ladies CDMX es parte de R-Ladies Global, una organización mundial que busca generar una comunidad fuerte para compartir dudas, habilidades y apoyo sobre #RStats en una comunidad con perspectiva de género. 2.2 Flujo básico de trabajo para el análisis de datos en R. En el análisis de datos nos interesan técnicas cuantitativas cómo: recolectar, organizar, entender, interpretar y extraer información de colecciones de datos predominantemente numéricos. Estas tareas se resumen en el proceso de análisis del siguiente diagrama: Primero debe importar los datos en R. Esto generalmente significa llevar los datos almacenados en un archivo, una base de datos, o uan Web API, a un data frame de R. Limpiar y transformar los datos es necesario, para que la forma en que se almacenan los datos coincida con la semántica de los datos. En términos generales, cada columna debe ser una variable y cada rengón una observación. La visualización es una actividad fundamentalmente humana. Una buena visualización te puede mostrar cosas que no esperabas y puede ayudarte a plantear nuevas preguntas acerca de los datos. Una buena visualización también puede ayudar a determinar si se está haciendo una pregunta equivocada sobre los datos, o si es encesario recolectar más datos, o bien, obtener datos de fuentes distintas. Las visualizaciones pueden sorprenderte, pero requieren de un ser humano para interpretarlas. Por otro lado, los modelos son una herramienta para complementar las visualizaciones. Los modelos los utilizamos como un instrumento matemático y computacional para responder preguntas precisas acerca de los datos. Por último, la comunicación de los resultados es una parte absolutamente crítica para cualquier proyecto de análisis de datos. 2.3 Introducción a R como lenguaje de programación, y la plataforma interactiva de RStudio. Notas basadas en el material de Teresa Ortiz y Sonia Mendizábal y en el libro “R for Data Science” escrito por Hadley Wickham y Garret Grolemund (Wickham and Grolemund 2016). Hay cuatro cosas que necesitan para ejecutar el código en este taller: R, RStudio, una colección de paquetes de R, llamada tidyverse, y otros paquetes que vamos a ir instalando progresivamente. Los paquetes son la unidad fundamental del código reproducible en R. Incluyen funciones que se pueden utilizar en un ámbito general, su documentación que describe cómo usarlas y datos de ejemplo con código de ejemplo como ayuda para los usuarios. 2.3.1 ¿Cómo entender R? Hay una sesión de R corriendo. La consola de R es la interfaz entre R y nosotros. En la sesión hay objetos. Todo en R es un objeto: vectores, tablas, funciones, etc. Operamos aplicando funciones a los objetos y creando nuevos objetos. 2.3.2 ¿Por qué R? R funciona en casi todas las plataformas (Mac, Windows, Linux e incluso en Playstation 3). R promueve la investigación reproducible. R está actualizado gracias a que tiene una activa comunidad. R se puede combinar con otras herramientas. R permite integrar otros lenguajes (C/C++, Java, Julia, Python) y puede interactuar con muchas fuentes de datos: bases de datos compatibles con ODBC y paquetes estadísticos. 2.3.2.1 Descargar R: versión 3.4.3 Sigue las instrucciones del instalador: OSX: http://cran.stat.ucla.edu/bin/macosx/R-3.4.3.pkg Windows: http://cran.stat.ucla.edu/bin/windows/base/R-3.4.3-win.exe 2.3.2.2 Descargar RStudio: versión 1.1.414 OSX: https://download1.rstudio.org/RStudio-1.1.414.dmg Windows: https://download1.rstudio.org/RStudio-1.1.414.exe RStudio es libre y gratis. Es un ambiente de desarrollo integrado para R: incluye una consola, un editor de texto y un conjunto de herramientas para administrar el espacio de trabajo cuando se utiliza R. Algunos shortcuts útiles en RStudio son: En el editor command/ctrl + enter: enviar código a la consola ctrl + 2: mover el cursor a la consola En la consola flecha hacia arriba: recuperar comandos pasados ctrl + flecha hacia arriba: búsqueda en los comandos ctrl + 1: mover el cursor al editor Más alt + shift + k: muestra los shortcuts disponibles. Para que el código sea reproducible es importante que RStudio únicamente guarde lo relevante para hacer los cálculos, es decir, los scripts y no los cálculos en sí. Con tus scripts de R (y los datos), siempre podemos volver a crear las variables de ambiente. Sin embargo, es casi imposible recuperar un script únicamente a partir de tus variables de ambiente. Por lo tanto, se recomienda ampliamente configurar RStudio para que jamás guarde el ambiente en memoria. 2.3.2.3 Paquetes de R Una de las ventajas de R es la gran comunidad que aporta al desarrollo por medio de paquetes que dan funcionalidad adicional. Esta es la mejor manera de usar R para análisis de datos. Existen dos formas de instalar paquetes: Desde RStudio: Desde la consola: install.packages(&#39;tidyverse&#39;) Una vez instalados los paquetes, se cargan a la sesión de R mediante library. Por ejemplo, para cargar el paquete readr que instalamos anteriormente, hacemos: library(&#39;tidyverse&#39;) print(read_csv) #&gt; function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), #&gt; na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;, #&gt; trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, #&gt; n_max), progress = show_progress()) #&gt; { #&gt; tokenizer &lt;- tokenizer_csv(na = na, quoted_na = TRUE, quote = quote, #&gt; comment = comment, trim_ws = trim_ws) #&gt; read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, #&gt; locale = locale, skip = skip, comment = comment, n_max = n_max, #&gt; guess_max = guess_max, progress = progress) #&gt; } #&gt; &lt;environment: namespace:readr&gt; Como el paquete readr está cargado en la sesión podemos llamar a la función read_csv que se usará más adelante. Importante: Los paquetes se instalan una vez únicamente después de descargar una nueva versión de R. Las librerías se cargan en cada sesión de R nueva. 2.3.2.4 Ayuda en R Existen diferentes formas de pedir ayuda en R. help.start(): ayuda en general help(fun) o ?fun: ayuda sobre la función fun apropos(&quot;fun&quot;): lista de funciones que contiene la palabra fun example(fun): muestra un ejemplo de la función fun help(read_csv) ?read_csv2 2.4 Estructuras de datos Todo lo que existe en R es un objeto. En R se puede trabajar con distintas estructuras de datos, algunas son de una sola dimensión y otras permiten más, como indica el siguiente diagrama: 2.4.1 Vectores Los vectores son estructuras de datos de una dimensión. Un vector se define con la función c(), que concatena diferentes elementos del mismo tipo, esto determina el tipo del vector. Nota: En R, la asignación de un nombre al vector, o en general a cualquier objeto, se realiza con el símbolo &lt;-. Se recomienda usar el shortcut alt - genera &lt;-. Los vectores en R pueden ser de diferentes tipos o clases, a continuación se presentan algunos casos. En R, la clase de cada vector se extrae con la función class(). Vectores numéricos: a &lt;- c(1,2.5,3,4.5,5,6.9) a #&gt; [1] 1.0 2.5 3.0 4.5 5.0 6.9 # clase del vector class(a) #&gt; [1] &quot;numeric&quot; Vectores lógicos: bool &lt;- c(T, F, TRUE, FALSE) bool #&gt; [1] TRUE FALSE TRUE FALSE # clase del vector class(bool) #&gt; [1] &quot;logical&quot; Vectores de caracteres: fruits &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;) fruits #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; class(fruits) #&gt; [1] &quot;character&quot; Para la manipulación de caracteres es recomendable el paquete stringr que permite realizar operaciones sobre este tipo de elementos. Más adelante se presenta un ejemplo. La selección de elementos de un vector se realiza con [ ] para indicar la posición. A diferencia de otros lenguajes de programación las posiciones en R incian en 1. # elemento en la posición 1 fruits[1] #&gt; [1] &quot;apple&quot; # elemento en la posición 1 y 5 fruits[c(1,5)] #&gt; [1] &quot;apple&quot; &quot;lemon&quot; En R es posible extraer un valor del vector indexándolo con posiciones negativas: # omitir el elemento en la primera posición fruits[-1] #&gt; [1] &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; Una característica particular de vectores en R, es que cada elemento puede ser nombrado. Para hacer esto se usa la función names(). Por ejemplo, al vector fruits agregemos el nombre en español de la fruta para como el nombre de cada elemento. names(fruits) &lt;- c(&#39;manzana&#39;, &#39;platano&#39;, &#39;naranja&#39;, &#39;piña&#39;, &#39;limón&#39;, &#39;kiwi&#39;) fruits #&gt; manzana platano naranja piña limón kiwi #&gt; &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; # cada elemento tiene un nombre asignado fruits[5] #&gt; limón #&gt; &quot;lemon&quot; Para eliminar los nombres asignados a cada elemento, se asigna NULL a los nombres del vector: #&gt; NULL #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; Los tipos que pueden tener los vectores se muestran en la siguiente figura. Veamos que regresan los siguientes comandos: typeof(TRUE) typeof(1L) typeof(1.5) typeof(&quot;a&quot;) Cada vector tiene 3 propiedades: x &lt;- 1:5 Tipo typeof(x) #&gt; [1] &quot;integer&quot; Longitud length(x) #&gt; [1] 5 Atributos attributes(x) #&gt; NULL Existe la función is.vector(x) para determinar si un objeto es un vector: is.vector(1:3) #&gt; [1] TRUE ¿Qué regresa ìs.vector(factor(1:3))? TRUE FALSE NA Ninguna de las anteriores Ejemplo Del vector de seis frutas diferentes llamado fruits, localiza únicamente las frutas que tengan la letra w. # Cargamos la librería library(stringr) fruits &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;) fruits #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; Esto es posible con la función str_detect(), que regresa un vector booleano para cada elemento del vector donde encontró el patron w. str_detect(fruits, pattern = &#39;w&#39;) #&gt; [1] FALSE FALSE FALSE FALSE FALSE TRUE Ahora, seleccionamos únicamente los elementos del vector que tienen la letra w: # Selecciona el elemento con valor TRUE: kiwi fruits[str_detect(fruits, pattern = &#39;w&#39;)] #&gt; [1] &quot;kiwi&quot; 2.4.1.1 Operaciones de vectores En R las operaciones de vectores son componente a componente. Sumas, multiplicaciones y potencias: # Suma del vector longitud 6 y un vector longitud 1 a &lt;- c(1, 2.5, 3, 4.5, 5, 6.9) b &lt;- 1 a + b #&gt; [1] 2.0 3.5 4.0 5.5 6.0 7.9 # Multiplicaciones componente a componente misma longitud a &lt;- c(1, 2.5, 3, 4.5, 5, 6.9) a*a #&gt; [1] 1.00 6.25 9.00 20.25 25.00 47.61 # Multiplicaciones y potencias a &lt;- c(1, 2.5, 3, 4.5, 5, 6.9) c &lt;- (a^2 + 5)*3 c #&gt; [1] 18.0 33.8 42.0 75.8 90.0 157.8 Comparaciones: En este tipo de operación se obtiene un vector lógico dependiendo si la condición se cumple o no. # Comparar el vector dado un valor específico a &gt; 3 #&gt; [1] FALSE FALSE FALSE TRUE TRUE TRUE a[a &gt; 3] # únicamente elementos que cumple la condicion de ser mayores a 3 #&gt; [1] 4.5 5.0 6.9 fruits != &#39;apple&#39; #&gt; [1] FALSE TRUE TRUE TRUE TRUE TRUE fruits[fruits != &#39;apple&#39;] # únicamente elementos que no son apple #&gt; [1] &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; &quot;kiwi&quot; # Comparar el vector dado otro vector de la misma dimensión x &lt;- c(1, 2, 3, 4, 5, 6) a == x #&gt; [1] TRUE FALSE TRUE FALSE TRUE FALSE a[a == x] # unicamente los elementos iguales y en la misma posición entre a y x #&gt; [1] 1 3 5 Funciones predeterminadas: Algunas funciones predeterminadas del paquete básico de R son muy útiles para la manipulación de vectores y el análisis de datos. A continuación se enlistan algunasde las más comúnes: length: número de elementos en el vector class: clase del vector summary: resumen de información del vector unique: valores unicos del vector table: tabla de frecuencias para cada elemento del vector sum: suma de los elementos del vector mean: promedio de elementos del vector sd: desviación estándar de los elementos del vector cumsum: suma acumulada de elemento anterior del vector Aplica las funciones comúnes enlistadas antes en el vector x &lt;- c(1, 2, 3, 4, 5, 6) 2.4.1.2 Otros tipos de vectores: Existen tipos de vectores con características importantes: Secuencias: los vectores de secuencias se pueden crear con la función seq() o con :, de la siguiente forma: # secuecia de 1 al 10 1:10 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 # secuecia de pares de 0 al 10 seq(0, 10, by = 2) #&gt; [1] 0 2 4 6 8 10 Vectores de fechas: se pueden hacer operaciones y algunas funciones definidas de fechas. El paquete lubridate permite manejar fechas con mayor facilidad. Se incia la secuencia el 08 de agosto de 2016 y se asigna la clase de fecha con la función as.Date(). Se generan en total 10 fechas length.out = 10 y con una distancua semanal by=&quot;1 week&quot;, es decir, se tiene la fecha de 10 semanas consecutivas: library(lubridate) tenweeks &lt;- seq( as.Date(&quot;2016-08-08&quot;), length.out = 10, by=&quot;1 week&quot;) tenweeks #&gt; [1] &quot;2016-08-08&quot; &quot;2016-08-15&quot; &quot;2016-08-22&quot; &quot;2016-08-29&quot; &quot;2016-09-05&quot; #&gt; [6] &quot;2016-09-12&quot; &quot;2016-09-19&quot; &quot;2016-09-26&quot; &quot;2016-10-03&quot; &quot;2016-10-10&quot; class(tenweeks) #&gt; [1] &quot;Date&quot; Se pueden hacer algunas operaciones como se ejemplifica en el siguiente código. # Aumenta un día a cada fecha tenweeks + 1 #&gt; [1] &quot;2016-08-09&quot; &quot;2016-08-16&quot; &quot;2016-08-23&quot; &quot;2016-08-30&quot; &quot;2016-09-06&quot; #&gt; [6] &quot;2016-09-13&quot; &quot;2016-09-20&quot; &quot;2016-09-27&quot; &quot;2016-10-04&quot; &quot;2016-10-11&quot; # Aumenta un día a cada fecha tenweeks #&gt; [1] &quot;2016-08-08&quot; &quot;2016-08-15&quot; &quot;2016-08-22&quot; &quot;2016-08-29&quot; &quot;2016-09-05&quot; #&gt; [6] &quot;2016-09-12&quot; &quot;2016-09-19&quot; &quot;2016-09-26&quot; &quot;2016-10-03&quot; &quot;2016-10-10&quot; weekdays(tenweeks) # Día de la semana de cada fecha #&gt; [1] &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; #&gt; [8] &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; Vectores de factores: este tipo de vector es usado comúnmente para variables categóricas. En R existe la clase factor que se asigna con la función homónima factor() o as.factor(). Un vector de factores tiene dos elementos importantes, levels o niveles y labels o etiquetas. Los niveles determinan las categorías únicas del vector y pueden ser etiquetadas, como se muestra en le siguiente código para un vector de frutas. En este ejemplo se tienen valores de frutas repetidos, se asigna un orden de niveles específicos y etiquetas específicas para cada nivel. fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;, &quot;apple&quot;) # Vector de caracteres a vector de factores fruits.fac &lt;- factor(fruits, levels = c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;kiwi&quot;), labels = c(&#39;manzana&#39;, &#39;platano&#39;, &#39;naranja&#39;, &#39;piña&#39;, &#39;limón&#39;, &#39;kiwi&#39;) ) fruits.fac #&gt; [1] platano manzana platano naranja piña limón kiwi manzana #&gt; Levels: manzana platano naranja piña limón kiwi # Clase class(fruits.fac) #&gt; [1] &quot;factor&quot; # Niveles etiquetados levels(fruits.fac) #&gt; [1] &quot;manzana&quot; &quot;platano&quot; &quot;naranja&quot; &quot;piña&quot; &quot;limón&quot; &quot;kiwi&quot; # Niveles únicos as.numeric(fruits.fac) #&gt; [1] 2 1 2 3 4 5 6 1 # Agregar un nuevo valor fruits.fac[7] &lt;- &#39;melon&#39; #&gt; Warning in `[&lt;-.factor`(`*tmp*`, 7, value = &quot;melon&quot;): invalid factor level, #&gt; NA generated fruits.fac #&gt; [1] platano manzana platano naranja piña limón &lt;NA&gt; manzana #&gt; Levels: manzana platano naranja piña limón kiwi Importante: En R los vectores no pueden combinar diferentes tipos de elementos. El tipo de elementos es lo que define la clase del vector. Es por esto que en el ejemplo, al sustituir la posición 7 por melon se obtiene un NA, melón no está incluído en los niveles definidos del vector. Existen también los factores ordenados. Por ejemplo, consideremos los datos de flores de iris de Fisher: library(forcats) iris %&gt;% sample_n(10) %&gt;% knitr::kable() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 13 4.8 3.0 1.4 0.1 setosa 125 6.7 3.3 5.7 2.1 virginica 89 5.6 3.0 4.1 1.3 versicolor 24 5.1 3.3 1.7 0.5 setosa 2 4.9 3.0 1.4 0.2 setosa 68 5.8 2.7 4.1 1.0 versicolor 72 6.1 2.8 4.0 1.3 versicolor 42 4.5 2.3 1.3 0.3 setosa 105 6.5 3.0 5.8 2.2 virginica 109 6.7 2.5 5.8 1.8 virginica Este conjunto de datos multivariados fue presentado por el estadístico y biólogo británico Ronald Fisher en su artículo de 1936 “El uso de mediciones múltiples en problemas taxonómicos como un ejemplo de análisis discriminante lineal”. Edgar Anderson recopiló los datos para cuantificar la variación morfológica de las flores de iris de tres especies relacionadas. Los datos fueron recolectadas en la Península de Gaspé. (Fisher 1936) El conjunto de datos consiste de 50 observaciones de cada una de las tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midieron cuatro características de cada muestra: la longitud y el ancho de los sépalos y pétalos, en centímetros. Con base en la combinación de estas cuatro características, Fisher desarrolló un modelo discriminante lineal para distinguir las especies entre sí. Supongamos que queremos analizar la distribución del ancho del sépalo por especie de flor de iris: Esto ocurre porque los factores están ordenados alfabéticamente: levels(iris$Species) #&gt; [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Sería mejor que las especies estuvieran ordenadas por la mediana de la distribución para poder hacer mejores comparaciones. Notemos el uso de la función fct_reorder del paquete forcats. library(forcats) iris$Species_ord &lt;- fct_reorder(iris$Species, iris$Sepal.Width, fun = median) levels(iris$Species_ord) #&gt; [1] &quot;versicolor&quot; &quot;virginica&quot; &quot;setosa&quot; 2.4.2 Data Frames Un data.frame es un conjunto de vectores del mismo tamaño agrupado en una tabla. Son estructuras rectangulares donde cada columna tiene elementos de la misma clase, pero columnas distintas pueden tener diferentes clases. Por ejemplo: tabla &lt;- data.frame( n = 1:6, frutas = fruits[1:6], valor = c(1, 2.5, 3, 4.5, 5, 6.9) ) tabla #&gt; n frutas valor #&gt; 1 1 banana 1.0 #&gt; 2 2 apple 2.5 #&gt; 3 3 banana 3.0 #&gt; 4 4 orange 4.5 #&gt; 5 5 pineapple 5.0 #&gt; 6 6 lemon 6.9 Similar a las funciones de vectores, en data.frames existen funciones predeterminadas que ayudan a su manipulación. head permite ver los primeros 6 elemento del data.frame: head(mtcars) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 #&gt; Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 str describe el tipo de variables en el data.frame: str(mtcars) #&gt; &#39;data.frame&#39;: 32 obs. of 11 variables: #&gt; $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... #&gt; $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... #&gt; $ disp: num 160 160 108 258 360 ... #&gt; $ hp : num 110 110 93 110 175 105 245 62 95 123 ... #&gt; $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... #&gt; $ wt : num 2.62 2.88 2.32 3.21 3.44 ... #&gt; $ qsec: num 16.5 17 18.6 19.4 17 ... #&gt; $ vs : num 0 0 1 1 0 1 0 1 1 1 ... #&gt; $ am : num 1 1 1 0 0 0 0 0 0 0 ... #&gt; $ gear: num 4 4 4 3 3 3 3 4 4 4 ... #&gt; $ carb: num 4 4 1 1 2 1 4 2 2 4 ... dim muestra la dimensión (renglones, columnas) del data.frame: dim(mtcars) #&gt; [1] 32 11 colnames y names muestran los nombres de las columnas del data.frame: names(mtcars) #&gt; [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; #&gt; [11] &quot;carb&quot; rownames muestra el nombre de los renglones del data.frame: rownames(mtcars) #&gt; [1] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; #&gt; [4] &quot;Hornet 4 Drive&quot; &quot;Hornet Sportabout&quot; &quot;Valiant&quot; #&gt; [7] &quot;Duster 360&quot; &quot;Merc 240D&quot; &quot;Merc 230&quot; #&gt; [10] &quot;Merc 280&quot; &quot;Merc 280C&quot; &quot;Merc 450SE&quot; #&gt; [13] &quot;Merc 450SL&quot; &quot;Merc 450SLC&quot; &quot;Cadillac Fleetwood&quot; #&gt; [16] &quot;Lincoln Continental&quot; &quot;Chrysler Imperial&quot; &quot;Fiat 128&quot; #&gt; [19] &quot;Honda Civic&quot; &quot;Toyota Corolla&quot; &quot;Toyota Corona&quot; #&gt; [22] &quot;Dodge Challenger&quot; &quot;AMC Javelin&quot; &quot;Camaro Z28&quot; #&gt; [25] &quot;Pontiac Firebird&quot; &quot;Fiat X1-9&quot; &quot;Porsche 914-2&quot; #&gt; [28] &quot;Lotus Europa&quot; &quot;Ford Pantera L&quot; &quot;Ferrari Dino&quot; #&gt; [31] &quot;Maserati Bora&quot; &quot;Volvo 142E&quot; La forma de indexar data.frames es similar a la de un vector [ ], pero en este caso es posible indexar renglones y columnas: # por posiciones de renglones mtcars[1:4, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 # por posiciones de columnas mtcars[1:4, c(1, 4, 6)] #&gt; mpg hp wt #&gt; Mazda RX4 21.0 110 2.62 #&gt; Mazda RX4 Wag 21.0 110 2.88 #&gt; Datsun 710 22.8 93 2.32 #&gt; Hornet 4 Drive 21.4 110 3.21 # por nombre de renglones específico mtcars[c(&#39;Mazda RX4&#39;, &#39;Mazda RX4 Wag&#39;), ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 mtcars[str_detect(rownames(mtcars), &quot;Mazda&quot; ), ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 También se puede seleccionar o filtrar el data.frame dado una condición: mtcars[mtcars$cyl == 6, ] # Selecciona los carros con número de cilindros mayor a 6 #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 #&gt; Merc 280 19.2 6 168 123 3.92 3.44 18.3 1 0 4 4 #&gt; Merc 280C 17.8 6 168 123 3.92 3.44 18.9 1 0 4 4 #&gt; Ferrari Dino 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6 rbind permite unir dos data.frames por renglones, si y solo si, tiene el mismo número de columnas: rbind(mtcars[str_detect(rownames(mtcars), &quot;Mazda&quot; ), ], mtcars[str_detect(rownames(mtcars), &quot;Hornet&quot;), ]) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 cbind permite unir dos data.frames por columna, si y solo si, tiene el mismo número de renglones: tabla &lt;- data.frame( n = 1:6, frutas = c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pineapple&quot;, &quot;lemon&quot;, &quot;apple&quot;), valor = runif(6) ) tabla #&gt; n frutas valor #&gt; 1 1 apple 0.8746 #&gt; 2 2 banana 0.1749 #&gt; 3 3 orange 0.0342 #&gt; 4 4 pineapple 0.3204 #&gt; 5 5 lemon 0.4023 #&gt; 6 6 apple 0.1957 tabla.color &lt;- data.frame( peso = rnorm(6), color = c(&#39;rojo&#39;, &#39;amarillo&#39;, &#39;naranje&#39;, &#39;amarillo&#39;, &#39;amarillo&#39;, &#39;rojo&#39;) ) tabla.color #&gt; peso color #&gt; 1 -0.244 rojo #&gt; 2 -0.283 amarillo #&gt; 3 -0.554 naranje #&gt; 4 0.629 amarillo #&gt; 5 2.065 amarillo #&gt; 6 -1.631 rojo cbind(tabla, tabla.color) #&gt; n frutas valor peso color #&gt; 1 1 apple 0.8746 -0.244 rojo #&gt; 2 2 banana 0.1749 -0.283 amarillo #&gt; 3 3 orange 0.0342 -0.554 naranje #&gt; 4 4 pineapple 0.3204 0.629 amarillo #&gt; 5 5 lemon 0.4023 2.065 amarillo #&gt; 6 6 apple 0.1957 -1.631 rojo Nota: Una forma de seleccionar una columna es con el símbolo $ (pesitos) y el nombre de la columna. Ejercicio: Del data.frame mtcars realiza lo siguiente: Calcula el promedio de cilindros cyl en los datos. Calcula el número de autos con peso wt mayor a 2. Extrae la información de los coches Merc. Calcula el promedio de millas por galón mpg de los autos Merc. 2.4.3 Listas La lista es una estructura de datos de una dimensión que permite distintas clases de elementos en el objeto. La función list() permite crear objetos de esta clase. Por ejemplo: lista &lt;- list( n = 100, x = &#39;hello&#39;, frutas = fruits, tabla = tabla, ejemlista = list(a = 15:20, b = 1:5) ) lista #&gt; $n #&gt; [1] 100 #&gt; #&gt; $x #&gt; [1] &quot;hello&quot; #&gt; #&gt; $frutas #&gt; [1] &quot;banana&quot; &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; &quot;pineapple&quot; &quot;lemon&quot; #&gt; [7] &quot;kiwi&quot; &quot;apple&quot; #&gt; #&gt; $tabla #&gt; n frutas valor #&gt; 1 1 apple 0.8746 #&gt; 2 2 banana 0.1749 #&gt; 3 3 orange 0.0342 #&gt; 4 4 pineapple 0.3204 #&gt; 5 5 lemon 0.4023 #&gt; 6 6 apple 0.1957 #&gt; #&gt; $ejemlista #&gt; $ejemlista$a #&gt; [1] 15 16 17 18 19 20 #&gt; #&gt; $ejemlista$b #&gt; [1] 1 2 3 4 5 La lista anterior contiene numeros, caracteres, vectores, data.frames e incluso otra lista con distintas secuencias. Se puede indexar una lista de varias formas: Usando [ ]: extrae el objeto como una lista, incluyendo el nombre asignado: lista[1] #&gt; $n #&gt; [1] 100 Usando [[ ]]: extrae únicamente el objeto respetando la clase de éste y sin incluir nombres: lista[[1]] #&gt; [1] 100 Usando $ mas el nombre: extrae únicamente el objeto: lista$ejemlista$a #&gt; [1] 15 16 17 18 19 20 2.5 R Markdown R Markdown es un sistema para crear documentos, en los cuales se combina tu código de R, los resultados y el texto que escribes como comentario en forma de prosa. Algunas ventajas y características de R Markdown son: cualquier R markdown Rmd es totalmente reproducible admite docenas de formatos de salida, como archivos PDF, Word, presentaciones de diapositivas y más. es muy útil para los tomadores de decisiones, quienes quieren enfocarse en las conclusiones, no en el código detrás del análisis. permite colaborar con otras personas de estadística que estén interesadas en tus conclusiones y cómo llegaste a ellas 2.5.1 ¿Qué es R Markdown? R Markdown integra código de R, comandos de TeX y muchas herramientas externas. Cuando construyes el documento, R Markdown envía un archivo con formato .Rmd a otro paquete llamado knitr, http://yihui.name/knitr/, que ejecuta el código de todos los chunks y crea un nuevo archivo de markdown con formato md que ya incluye el código y los resultados. Este archivo de markdown generado por knitr después es procesado por pandoc, http://pandoc.org/, que es el que crea el archivo final. La ventaja de este flujo de trabajo de dos pasos es que te permite crear una amplia gama de formatos de salida. 2.5.2 Estructura básica de R Markdown Éste es un R Markdown, un archivo de texto sin formato que tiene la extensión .Rmd: cat(htmltools::includeText(&quot;rmarkdown/ejemplo.Rmd&quot;)) #&gt; --- #&gt; title: &quot;Ejemplo de R Markdown&quot; #&gt; date: 2018-01-22 #&gt; output: html_document #&gt; --- #&gt; #&gt; Veamos unos datos de diamantes para analizar la distribución #&gt; del quilataje de aquellos diamantes que tiene quilataje #&gt; menor a 2.5: #&gt; #&gt; ```{r setup, include = FALSE} #&gt; library(ggplot2) #&gt; library(dplyr) #&gt; #&gt; smaller &lt;- diamonds %&gt;% #&gt; filter(carat &lt;= 2.5) #&gt; ``` #&gt; #&gt; En el __chunk__ de arriba se hizo el filtro adecuado, ahora #&gt; veamos una muestra de tamaño 10 de los datos: #&gt; #&gt; ```{r, echo = FALSE} #&gt; smaller %&gt;% #&gt; sample_n(10) %&gt;% #&gt; knitr::kable() #&gt; ``` #&gt; #&gt; Los datos corresponde a `r nrow(diamonds)` diamantes. Solamente #&gt; `r nrow(diamonds) - nrow(smaller)` son de más de 2.5 quilates. #&gt; La distribución de los diamantes de menor quilataje se muestra abajo: #&gt; #&gt; ```{r, echo = FALSE} #&gt; smaller %&gt;% #&gt; ggplot(aes(carat)) + #&gt; geom_freqpoly(binwidth = 0.01) #&gt; ``` Contiene tres tipos importantes de contenido: 1 Un encabezado en formato YAML rodeado por ---s.. Chunks de código de R rodeados por ```. Texto mezclado con formato de texto simple como # heading y _italics_. Cuando abres un .Rmd, RStudio muestra una interfaz de tipo notebook en la cual tanto el código como la salida están intercalados. Puedes ejecutars cada chunk de código presionando el botón de “Run” (en la parte superior derecha de la ventana de script), o bien, Cmd/Ctrl + Shift + Enter. RStudio ejecuta el código y muestra los resultados junto con el código. Para generar un informe completo que contenga todo el texto, el código y los resultados, presiona el botón “Knit”, o bien, Cmd/Ctrl + Shift + K. Esto generará un reporte en una nueva ventana y creará un archivo HTML independiente que podrás compartir con los demás. Para comenzar con tu propio archivo .Rmd, selecciona File &gt; New File &gt; R Markdown… en la barra superior. RStudio te mostrará un asistente que puedes usar para crear un archivo de R Markdown con ejemplos básicos. Como R Markdown integara varias herramientas, entonces no es posible que la ayuda esté autocontenida en RStudio. Esto significa que gran parte de la ayuda no la podrás encontrar a través de ?. Hay mucha documentación en línea y un recurso es particularmente útil son los cheatsheets de RStudio, que están disponibles en http://rstudio.com/cheatsheets. 2.6 Proyectos de RStudio Los proyectos de RStudio son útiles para mantener juntos todos los archivos asociados a un análisis (o proyecto) específico: datos de entrada, scripts de R, resultados, gráficas, datos de salida. Ésta es una práctica limpia y ordenada de trabajar y RStudio tiene soporte integrado para esto a través de los proyectos. Hagamos un proyecto. Para esto debes presionar File &gt; New Project, luego: Puedes cerrar el proyecto y después hacer doble click en el archivo .Rproj para volver a abrir el proyecto. Observa que regresas a donde estabas, en el mismo directorio de trabajo, con el mismo historial de comandos, y todos los archivos en los que estaba trabajando siguen abiertos. En resumen, los proyectos de RStudio te brindan un flujo de trabajo sólido que te servirá en el futuro: Creas un proyecto de RStudio para cada proyecto de análisis de datos. Mantienes los archivos de datos ahí mismo para después cargarlos en un script. Mantienes tus scripts organizados en el mismo directorio, y los puedes encontrar fácilmente para editarlos y ejecutarlos. Puedes guardar ahí mismo las salidas del código, como gráficas y datos limpios. Solamente utilizas rutas relativas, no absolutas. Todo lo que necesitas está en un solo lugar y separado de los demás proyectos en los que estés trabajando. 2.7 Otros aspectos importantes de R 2.7.1 Valores faltantes En R los datos faltantes se expresan como NA. La función is.na() regresa un vector lógico sobre los valores que son o no NA. is.na(c(4, 2, NA)) #&gt; [1] FALSE FALSE TRUE El default de R es propagar los valores faltantes, esto es, si se desconoce el valor de una de las componentes de un vector, también se desconoce la suma del mismo, en general, cualquier operación. sum(c(4, 2, NA)) #&gt; [1] NA mean(c(4, 2, NA)) #&gt; [1] NA 3 &gt; NA #&gt; [1] NA (NA == NA) #&gt; [1] NA Sin embargo, muchas funciones tienen un argumento na.rm para removerlos. sum(c(4, 2, NA), na.rm = T) #&gt; [1] 6 mean(c(4, 2, NA), na.rm = T) #&gt; [1] 3 2.7.2 Funciones Todo lo que sucede en R es una función. R es un lenguaje de programación funcional. Es decir, proporciona muchas herramientas para la creación y manipulación de funciones. En R las funciones, al igual que los vectores, se pueden asignar a variables, guardarlas en listas, usarlas como argumentos en otras funciones, crearlas dentro de otras funciones, e incluso regresar como resultado de una función más funciones. Una caja negra Una función puede verse como una caja negra que realiza un proceso o serie de instrucciones condicionadas a un valor de entrada, y cuyo resultado es un valor de salida. En R existen algunas funciones pre cargadas que ya hemos usado, por ejemplo, .la función mean(). input &lt;- c(1:5) output &lt;- mean( input ) output #&gt; [1] 3 Sin embargo, también es posible escribir nuestras propias funciones. Escibir una función En R es posible escribir funciones y es muy recomendable para dar soluciones a problemas simples. Existen ocasiones en las que al programar copias y pegas cierto código varias veces para una meta en especial. En ese momento, es necesario pasar el código a una función. Una función soluciona un problema en particular. La función function() nos permite crear funciones con la siguiente estructura: my_fun &lt;- function( arg1 ){ body return() } En general, esta estructura se respeta en las funciones predeterminadas de R. Creamos una función que sume uno a cualquier número. suma_uno_fun &lt;- function( x ){ y = x + 1 return(y) } Aplicamos la función: suma_uno_fun(5) #&gt; [1] 6 Podemos ver que en nuestra sesión ya existe la función con la función ls(). ls() #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;bool&quot; &quot;c&quot; #&gt; [5] &quot;fruits&quot; &quot;fruits.fac&quot; &quot;input&quot; &quot;iris&quot; #&gt; [9] &quot;lista&quot; &quot;output&quot; &quot;suma_uno_fun&quot; &quot;tabla&quot; #&gt; [13] &quot;tabla.color&quot; &quot;tenweeks&quot; &quot;x&quot; Esta función en lista los objetos existente en la sesión actual. Argumentos de funciones En R los argumentos de las funciones pueden llamarse por posición o nombre. Consideremos la siguiente función en la que se eleva un numero a un exponente determinado. potencia_fun &lt;- function(base, exponente){ base^exponente } Los argumentos pueden indicarse por posición: potencia_fun(2, 3) #&gt; [1] 8 O bien por nombre: potencia_fun(exponente = 2, base = 3) #&gt; [1] 9 Argumentos predeterminados En una función es posible asignar valores predeterminados a los argumentos. Por ejemplo, modificamos la función para asignar un valor predeterminado del exponente. potencia_default_fun &lt;- function(base, exponente = 2){ base^exponente } Al llamar la función, no es necesario definir un valor para el exponente y en automático tomará el valor exponente = 2. potencia_default_fun(2) #&gt; [1] 4 Argumentos nulos Una función puede no tener argumentos y simplemente correr un proceso. En este caso, usaremos la función sample() que elige una muestra aleatoria de tamaño 1 de un vector de 1 a 6 imitando un dado dentro la la función lanza_dado(). lanza_dado &lt;- function() { numero &lt;- sample(1:6, size = 1) numero } Ahora tiraremos dos veces los dados. Primer lanzamiento: lanza_dado() #&gt; [1] 5 Segundo lanzamiento: lanza_dado() #&gt; [1] 5 Alcance de la función Es importante mencionar que las variables que son definidas dentro de la función no son accesibles fuera de la función. Es decir, las funciones en R tienen un ambiente local. Por ejemplo, al correr la siguiente función e intentar imprimir el objeto x regresa un error. xs_fun &lt;- function(a){ x &lt;- 2 a*x } xs_fun(2) #&gt; [1] 4 # print(x) La función crea un ambiente nuevo dentro de la misma, en caso de no encontrar el valor de la variable en el ambiente local, sube un nivel. Este nuevo nivel puede ser el ambiente global. Por ejemplo: y &lt;- 10 ys_fun &lt;- function(a){ a*y } ys_fun(2) #&gt; [1] 20 Si la función está contenida en otra función, primero buscará en el ambiente local, después en el ambiente local de la función superior y luego en el ambiente global. Por ejemplo: y &lt;- 10 mas_uno_fun &lt;- function(a){ c &lt;- 1 y &lt;- 1 ys_add_fun &lt;- function(a){ a*y + c } ys_add_fun(a) } Si llamamos la función con un valor a = 2 al igual que en el ejemplo anterior, ¿por qué da el siguiente resultado y no 21 o 20? mas_uno_fun(a = 2) #&gt; [1] 3 Funciones para funciones O bien funciones para entender las partes de la función. body() body(suma_uno_fun) #&gt; { #&gt; y = x + 1 #&gt; return(y) #&gt; } args() args(mean.default) #&gt; function (x, trim = 0, na.rm = FALSE, ...) #&gt; NULL if() Una función que se usa al programar funciones es if() que permite agregar una condición. divide_fun &lt;- function(num, den){ if(den == 0){ return(&quot;Denominador es cero&quot;) }else{ return(num/den) } } Al ejecutar la función y tener cero en el denominador imprime el string. divide_fun(10, 0) #&gt; [1] &quot;Denominador es cero&quot; Al no tener cero en el denominador la operación se ejecuta. divide_fun(10, 2) #&gt; [1] 5 Todas las operaciones en R son producto de la llamada a una función, esto incluye operaciones como +, operadores que controlan flujo como for, if y while, e incluso operadores para obtener subconjuntos como [ ] y $. x &lt;- 3 y &lt;- 4 `+`(x, y) #&gt; [1] 7 for (i in 1:2) print(i) #&gt; [1] 1 #&gt; [1] 2 `for`(i, 1:2, print(i)) #&gt; [1] 1 #&gt; [1] 2 Cuando llamamos a una función podemos especificar los argumentos con base en la posición, el nombre completo o el nombre parcial: f &lt;- function(abcdef, bcde1, bcde2) { list(a = abcdef, b1 = bcde1, b2 = bcde2) } str(f(1, 2, 3)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 str(f(2, 3, abcdef = 1)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 Podemos abreviar el nombre de los argumentos: str(f(2, 3, a = 1)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 Siempre y cuando la abreviación no sea ambigua: #f(1, 3, b = 1) Los argumentos de las funciones en R se evaluan conforme se necesitan: f &lt;- function(a, b){ a ^ 2 } f(2) #&gt; [1] 4 La función anterior nunca utiliza el argumento b, de tal manera que f(2) no produce ningún error. 2.7.3 Funcionales La familia de funciones apply pertenece a la librería base en R y facilitan la manipulación de datos de forma repetitiva. Las funciones de esta familia son: apply(), lapply(), sapply(), vapply(), mapply(), rapply(), y tapply(). La estructura de los datos de entrada y el formato del resultado o salida determinarán cual función usar. En este taller solo se verán las primeras tres funciones. apply() Esta es la función que manipula arreglos homogéneos, en particular, se revisa el caso de matrices que son arreglos de dos dimensiones. La función tiene los siguientes argumentos: apply(X, MARGIN, FUN, ...) X representa el arreglo de dos dimensiones. MARGIN representa la dimensión sobre la que se va a resumir la información. Donde 1 = renglon o primera dimensión y 2 = columna o segunda dimensión. FUN representa la función que resume la información. Tomemos la siguiente matriz de simulaciones: set.seed(1) mat_norm &lt;- matrix(rnorm(24, mean = 0, sd = 1), nrow = 4, ncol = 6) mat_norm #&gt; [,1] [,2] [,3] [,4] [,5] [,6] #&gt; [1,] -0.626 0.330 0.576 -0.6212 -0.0162 0.9190 #&gt; [2,] 0.184 -0.820 -0.305 -2.2147 0.9438 0.7821 #&gt; [3,] -0.836 0.487 1.512 1.1249 0.8212 0.0746 #&gt; [4,] 1.595 0.738 0.390 -0.0449 0.5939 -1.9894 Deseamos obtener la suma de cada columna de la matriz. El primer método, quizá el mas intuitivo en este momento, es obtener cada elemento o columna, aplicar la función a cada elemento y concatenar: prom_col_m1 &lt;- c(sum(mat_norm[, 1]), sum(mat_norm[, 2]), sum(mat_norm[, 3]), sum(mat_norm[, 4]), sum(mat_norm[, 5]), sum(mat_norm[, 6])) prom_col_m1 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Segundo método: prom_col_m2 &lt;- vector( length = ncol(mat_norm)) for(j in 1:ncol(mat_norm)){ prom_col_m2[j] &lt;- sum(mat_norm[, j]) } prom_col_m2 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Tercer método: prom_col_m3 &lt;- apply(X = mat_norm, MARGIN = 2, FUN = sum) prom_col_m3 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Cuarto método: prom_col_m4 &lt;- colSums(mat_norm) prom_col_m4 #&gt; [1] 0.317 0.735 2.172 -1.756 2.343 -0.214 Ahora, para obtener la suma por renglón usando el tercer método de la función apply(), únicamente es necesario cambiar la dimensión sobre la que voy a resumir con el argumento MARGIN = 1. prom_row_m3 &lt;- apply(mat_norm, 1, sum) prom_row_m3 #&gt; [1] 0.56 -1.43 3.18 1.28 Esto es equivalente al primer método que usamos: prom_row_m1 &lt;- c(sum(mat_norm[1, ]), sum(mat_norm[2, ]), sum(mat_norm[3, ]), sum(mat_norm[4, ])) prom_row_m1 #&gt; [1] 0.56 -1.43 3.18 1.28 La ventaja de usar la función apply() es que se puede usar cualquier función. Por ejemplo, obtener la desviación estándar. apply(mat_norm, 1, sd) #&gt; [1] 0.634 1.172 0.834 1.207 O bien, una crear una función propia (definida por el usuario) con la función function(): cv_vec_m3 &lt;- apply(mat_norm, 1, function(reng){ cv &lt;- mean(reng)/sd(reng) return(cv) }) cv_vec_m3 #&gt; [1] 0.147 -0.204 0.636 0.177 Funciones Anónimas: A este tipo de funciones se les llama funciones anónimas porque no se nombran ni guardan en el ambiente de R y únicamente funcionan dentro del comando que las llama. lapply() La función lapply() aplica una función sobre una lista o un vector y regresa el resultado en otra lista. Vector de ciudades: ciudades_vec &lt;- c(&quot;Aguascalientes&quot;, &quot;Monterrey&quot;, &quot;Guadalajara&quot;, &quot;México&quot;) ciudades_vec #&gt; [1] &quot;Aguascalientes&quot; &quot;Monterrey&quot; &quot;Guadalajara&quot; &quot;México&quot; res_nchar_l &lt;- lapply(ciudades_vec, nchar) res_nchar_l #&gt; [[1]] #&gt; [1] 14 #&gt; #&gt; [[2]] #&gt; [1] 9 #&gt; #&gt; [[3]] #&gt; [1] 11 #&gt; #&gt; [[4]] #&gt; [1] 6 Esta función permite implementar funciones que regresen objetos de diferentes tipos, porque la listas permiten almacenar contenido heterogéneo. La función lapply() permite incluir argumentos de las funciones que implementa. Estos argumentos se incluyen dentro de lapply() después de la función a implementar. Por ejemplo, usamos la función potencia que se creó antes. potencia_fun &lt;- function(base, exponente){ base^exponente } El objetivo es aplicar a cada elemento de la siguiente lista la función potencia y elevarlo al cubo. nums_lista &lt;- list(1, 3, 4) nums_lista #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 3 #&gt; #&gt; [[3]] #&gt; [1] 4 En la función lapply() se agrega el argumento exponente = 3 como último argumento. potencia_lista &lt;- lapply(nums_lista, potencia_fun, exponente = 3) potencia_lista #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 27 #&gt; #&gt; [[3]] #&gt; [1] 64 Una forma de reducir la lista obtenida a un vector es con la función unlist() que vimos antes. unlist(potencia_lista) #&gt; [1] 1 27 64 sapply() La función sapply() es muy similar a lapply(). La única diferencia es la s que surge de simplified apply. Al igual que lapply() aplica una función sobre una lista o un vector pero simplifica el resultado en un arreglo. res_nchar_s &lt;- sapply(ciudades_vec, nchar) res_nchar_s #&gt; Aguascalientes Monterrey Guadalajara México #&gt; 14 9 11 6 Esta función es peligrosa ya que únicamente simplifica la estructura del resultado cuando es posible, de lo contrario, regresará una lista igual que lapply(). 2.7.3.1 Funciones map Un problema con sapply() y lapply() es que puedes no tener control sobre el tipo que obtienes y esto es importante si el código está dentro de una función: x1 &lt;- list( c(0.27, 0.37, 0.57, 0.91, 0.20), c(0.90, 0.94, 0.66, 0.63, 0.06), c(0.21, 0.18, 0.69, 0.38, 0.77) ) x2 &lt;- list( c(0.50, 0.72, 0.99, 0.38, 0.78), c(0.93, 0.21, 0.65, 0.13, 0.27), c(0.39, 0.01, 0.38, 0.87, 0.34) ) threshold &lt;- function(x, cutoff = 0.8) x[x &gt; cutoff] x1 %&gt;% sapply(threshold) %&gt;% str() #&gt; List of 3 #&gt; $ : num 0.91 #&gt; $ : num [1:2] 0.9 0.94 #&gt; $ : num(0) x2 %&gt;% sapply(threshold) %&gt;% str() #&gt; num [1:3] 0.99 0.93 0.87 Las funciones del paquete purrr son útiles porque hacen que los loops sobre vectores sean sencillos: tienen nombres similares, y tienen argumentos consistentes. Hay una función para cada tipo de salida: map() crea una lista. map_lgl() crea un vector lógico. map_int() crea un vector de enteros. map_dbl() crea un vector numérico. map_chr() crea un vector de tipo caracter. Cada función toma un vector como entrada, aplica una función a cada elemento y luego devuelve un nuevo vector que tiene la misma longitud (y tiene los mismos nombres) que la entrada, y el tipo de vector está determinado por el sufijo de la función. Por ejemplo, supongamos que deseamos calcular el número de valores únicos en cada columna de iris. Utilizamos, entonces, la función map_int porque deseamos obtener como resultado un vector de enteros donde cada entero represente el número de valores únicos en cada columna: map_int(.x = iris, .f = function(x){length(unique(x)) }) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 35 23 43 22 3 #&gt; Species_ord #&gt; 3 Para generar cuatro vectores cada uno de tamaño 5 de valores que provienen de una distribución normal con medias \\(\\mu=-10, 0, 10, 100\\) podemos utilizar la función map: mu &lt;- list(-10, 0, 10, 100) mu %&gt;% map(rnorm, n = 5) %&gt;% str() #&gt; List of 4 #&gt; $ : num [1:5] -9.38 -10.06 -10.16 -11.47 -10.48 #&gt; $ : num [1:5] 0.4179 1.3587 -0.1028 0.3877 -0.0538 #&gt; $ : num [1:5] 8.62 9.59 9.61 9.94 11.1 #&gt; $ : num [1:5] 100.8 99.8 99.7 100.7 100.6 Podemos usar map2() para iterar sobre dos vectores en paralelo: sigma &lt;- list(1, 5, 10, 100) map2(mu, sigma, rnorm, n = 5) %&gt;% str() #&gt; List of 4 #&gt; $ : num [1:5] -10.69 -10.71 -9.64 -9.23 -10.11 #&gt; $ : num [1:5] 4.41 1.99 -3.06 1.71 -5.65 #&gt; $ : num [1:5] 24.33 29.804 6.328 -0.441 15.697 #&gt; $ : num [1:5] 86.5 340.2 96.1 169 102.8 Las funciones de map también preservan nombres: z &lt;- list(x = 1:3, y = 4:5) map_int(z, length) #&gt; x y #&gt; 3 2 Hay algunos shortcuts que podemos usar con .f para guardar un poco de tipeo. Supongamos que queremos ajustar un modelo lineal a cada subconjunto en un conjunto de datos. Pensemos en los datos de mtcars divididos en tres subconjuntos (uno para cada valor de cilindro) y se ajusta el mismo modelo lineal en cada subconjunto (millas por galón mpg vs peso wt): models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~lm(mpg ~ wt, data = .)) Con el símbolo ~ se puede declarar una función anónima con un argumento al cual se hace referencia después utilizando el .. Cuando vemos muchos modelos, generalmente deseamos extraer un resumen estadístico como la \\(R^2\\). Para hacer eso necesitamos ejecutar primero summary() y luego extraer el componente llamado r.squared. Podríamos hacer eso usando la abreviatura de funciones anónimas: models %&gt;% map(summary) %&gt;% map_dbl(~.$r.squared) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 Pero purrr tiene un shortcut para extraer estos componentes en una cadena: models %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 También se puede usar un número entero para seleccionar elementos por posición: x &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9)) x %&gt;% map_dbl(2) #&gt; [1] 2 5 8 La función pmap() recibe una lista de argumentos para aplicarlos a una función: params &lt;- tribble( ~mean, ~sd, ~n, 5, 1, 1, 10, 5, 3, -3, 10, 5 ) params %&gt;% pmap(rnorm) #&gt; [[1]] #&gt; [1] 4.26 #&gt; #&gt; [[2]] #&gt; [1] 10.944 0.975 17.328 #&gt; #&gt; [[3]] #&gt; [1] -1.47 18.73 1.76 -10.10 3.11 Hay un paso más en la complejidad: además de variar los argumentos para una función, también puedes variar la función misma y para esto se usa la función invoke_map(): sim &lt;- tribble( ~f, ~params, &quot;runif&quot;, list(min = -1, max = 1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim %&gt;% mutate(sim = invoke_map(f, params, n = 10)) %&gt;% str() #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3 obs. of 3 variables: #&gt; $ f : chr &quot;runif&quot; &quot;rnorm&quot; &quot;rpois&quot; #&gt; $ params:List of 3 #&gt; ..$ :List of 2 #&gt; .. ..$ min: num -1 #&gt; .. ..$ max: num 1 #&gt; ..$ :List of 1 #&gt; .. ..$ sd: num 5 #&gt; ..$ :List of 1 #&gt; .. ..$ lambda: num 10 #&gt; $ sim :List of 3 #&gt; ..$ : num -0.65 0.493 -0.79 0.729 0.229 ... #&gt; ..$ : num 0.372 -2.948 -2.843 -0.676 5.89 ... #&gt; ..$ : int 11 10 8 13 13 12 15 11 5 10 2.7.4 Rendimiento en R “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.” -Donald Knuth Diseña primero, luego optimiza. La optimización del código es un proceso iterativo: Encuentra el cuello de botella (más importante). Intenta eliminarlo (no siempre se puede). Repite hasta que tu código sea lo suficientemente rápido. Diagnosticar Una vez que tienes código que se puede leer y funciona, el perfilamiento (profiling) del código es un método sistemático que nos permite conocer cuanto tiempo se esta usando en diferentes partes del programa. Comenzaremos con la función system.time (no es perfilamiento aún), esta calcula el timepo en segundos que toma ejecutar una expresión (si hay un error, regresa el tiempo hasta que ocurre el error): library(Lahman) Batting %&gt;% sample_n(10) %&gt;% knitr::kable() playerID yearID stint teamID lgID G AB R H X2B X3B HR RBI SB CS BB SO IBB HBP SH SF GIDP 86418 gilesma01 2005 1 ATL NL 152 577 104 168 45 4 15 63 16 3 64 108 1 5 4 4 13 32692 weathro01 1946 1 NYA AL 2 2 0 1 0 0 0 0 0 0 0 0 NA 0 0 NA 0 80489 tynerja01 2000 2 TBA AL 37 83 6 20 2 0 0 8 6 1 4 12 0 1 5 1 1 27504 stephwa01 1937 1 PHI NL 10 23 1 6 0 0 0 2 0 NA 2 3 NA 0 0 NA 1 22480 cohenan01 1928 1 NY1 NL 129 504 64 138 24 7 9 59 3 NA 31 17 NA 2 11 NA NA 53133 lemonch01 1975 1 CHA AL 9 35 2 9 2 0 0 1 1 0 2 6 0 0 1 0 0 27651 chapmsa01 1938 1 PHA AL 114 406 60 105 17 7 17 63 3 4 55 94 NA 4 1 NA NA 18626 weavebu01 1920 1 CHA AL 151 629 102 208 34 8 2 75 19 17 28 23 NA 6 27 NA NA 53314 perezto01 1975 1 CIN NL 137 511 74 144 28 3 20 109 1 2 54 101 6 3 0 6 12 57859 littljo01 1980 1 SLN NL 52 11 1 0 0 0 0 0 0 0 0 3 0 0 1 0 0 system.time(lm(R ~ AB + teamID, Batting)) #&gt; user system elapsed #&gt; 2.952 0.096 3.048 user time: Tiempo usado por el CPU(s) para evaluar esta expresión, tiempo que experimenta la computadora. elapsed time: tiempo en el reloj, tiempo que experimenta la persona. El tiempo de usuario (user) usualmente es menor que el tiempo transcurrido: system.time(readLines(&quot;http://www.jhsph.edu&quot;)) #&gt; user system elapsed #&gt; 0.021 0.004 1.052 library(parallel) system.time(mclapply(2000:2006, function(x){ sub &lt;- subset(Batting, yearID == x) lm(R ~ AB, sub) }, mc.cores = 5)) #&gt; user system elapsed #&gt; 0.061 0.055 0.090 Comparemos la velocidad de dplyr con funciones que se encuentran en R estándar y plyr. # dplyr dplyr_st &lt;- system.time({ Batting %&gt;% group_by(playerID) %&gt;% summarise(total = sum(R, na.rm = TRUE), n = n()) %&gt;% dplyr::arrange(desc(total)) }) # plyr plyr_st &lt;- system.time({ Batting %&gt;% plyr::ddply(&quot;playerID&quot;, plyr::summarise, total = sum(R, na.rm = TRUE), n = length(R)) %&gt;% dplyr::arrange(-total) }) # estándar lento est_l_st &lt;- system.time({ players &lt;- unique(Batting$playerID) n_players &lt;- length(players) total &lt;- rep(NA, n_players) n &lt;- rep(NA, n_players) for(i in 1:n_players){ sub_batting &lt;- Batting[Batting$playerID == players[i], ] total[i] &lt;- sum(sub_batting$R, na.rm = TRUE) n[i] &lt;- nrow(sub_batting) } batting_2 &lt;- data.frame(playerID = players, total = total, n = n) batting_2[order(batting_2$total, decreasing = TRUE), ] }) # estándar rápido est_r_st &lt;- system.time({ batting_2 &lt;- aggregate(. ~ playerID, data = Batting[, c(&quot;playerID&quot;, &quot;R&quot;)], sum) batting_ord &lt;- batting_2[order(batting_2$R, decreasing = TRUE), ] }) dplyr_st #&gt; user system elapsed #&gt; 0.160 0.004 0.164 plyr_st #&gt; user system elapsed #&gt; 8.334 0.008 8.342 est_l_st #&gt; user system elapsed #&gt; 70.80 1.56 72.36 est_r_st #&gt; user system elapsed #&gt; 0.682 0.012 0.694 La función system.time supone que sabes donde buscar, es decir, que expresiones debes evaluar, una función que puede ser más útil cuando uno desconoce cuál es la función que alenta un programa es Rprof(). Rprof es un perfilador de muestreo que registra cambios en la pila de funciones, funciona tomando muestras a intervalos regulares y tabula cuánto tiempo se lleva en cada función. Rprof(&quot;out/lm_rprof.out&quot;, interval = 0.015, line.profiling = TRUE) mod &lt;- lm(R ~ AB + teamID, Batting) Rprof(NULL) Usamos la función `summaryRprof para tabular las salidas de Rprof y calcular cuánto tiempo se toma en cada función. summaryRprof(&quot;out/lm_rprof.out&quot;) #&gt; $by.self #&gt; self.time self.pct total.time total.pct #&gt; &quot;lm.fit&quot; 2.82 89.52 2.82 89.52 #&gt; &quot;.External2&quot; 0.24 7.62 0.26 8.10 #&gt; &quot;as.character&quot; 0.06 1.90 0.06 1.90 #&gt; &quot;anyDuplicated.default&quot; 0.02 0.48 0.02 0.48 #&gt; &quot;parent.frame&quot; 0.02 0.48 0.02 0.48 #&gt; #&gt; $by.total #&gt; total.time total.pct self.time self.pct #&gt; &quot;&lt;Anonymous&gt;&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;block_exec&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;call_block&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;do.call&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;eval.parent&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;eval&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;evaluate_call&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;evaluate::evaluate&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;evaluate&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;in_dir&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;knitr::knit&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;local&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;process_file&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;process_group.block&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;process_group&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;withCallingHandlers&quot; 3.15 100.00 0.00 0.00 #&gt; &quot;handle&quot; 3.13 99.52 0.00 0.00 #&gt; &quot;lm&quot; 3.13 99.52 0.00 0.00 #&gt; &quot;timing_fn&quot; 3.13 99.52 0.00 0.00 #&gt; &quot;withVisible&quot; 3.13 99.52 0.00 0.00 #&gt; &quot;lm.fit&quot; 2.82 89.52 2.82 89.52 #&gt; &quot;.External2&quot; 0.26 8.10 0.24 7.62 #&gt; &quot;model.matrix.default&quot; 0.24 7.62 0.00 0.00 #&gt; &quot;model.matrix&quot; 0.24 7.62 0.00 0.00 #&gt; &quot;as.character&quot; 0.06 1.90 0.06 1.90 #&gt; &quot;model.response&quot; 0.06 1.90 0.00 0.00 #&gt; &quot;anyDuplicated.default&quot; 0.02 0.48 0.02 0.48 #&gt; &quot;parent.frame&quot; 0.02 0.48 0.02 0.48 #&gt; &quot;[.data.frame&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;[&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;anyDuplicated&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;match.arg&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;model.frame.default&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;na.omit.data.frame&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;na.omit&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;set_hooks&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;setHook&quot; 0.02 0.48 0.00 0.00 #&gt; &quot;stats::model.frame&quot; 0.02 0.48 0.00 0.00 #&gt; #&gt; $sample.interval #&gt; [1] 0.015 #&gt; #&gt; $sampling.time #&gt; [1] 3.15 Hay dos métodos para normalizar los datos de Rprof: by.total divide el tiempo que se toma en cada función entre el tiempo total en correr. by.self similar a by.total pero primero resta el tiempo que se toman las funciones en la cima de la pila. Rprof(&quot;out/plyr_rprof.out&quot;) Batting %&gt;% plyr::ddply(&quot;playerID&quot;, plyr::summarise, total = sum(R, na.rm = TRUE), n = length(R)) %&gt;% plyr::arrange(-total) %&gt;% head() #&gt; playerID total n #&gt; 1 henderi01 2295 29 #&gt; 2 cobbty01 2246 24 #&gt; 3 bondsba01 2227 22 #&gt; 4 aaronha01 2174 23 #&gt; 5 ruthba01 2174 22 #&gt; 6 rosepe01 2165 25 Rprof(NULL) summaryRprof(&quot;out/plyr_rprof.out&quot;)$by.self[1:10, ] #&gt; self.time self.pct total.time total.pct #&gt; &quot;FUN&quot; 1.18 13.79 2.06 24.07 #&gt; &quot;lapply&quot; 1.06 12.38 3.22 37.62 #&gt; &quot;eval&quot; 0.56 6.54 8.56 100.00 #&gt; &quot;[[&quot; 0.30 3.50 4.18 48.83 #&gt; &quot;.fun&quot; 0.30 3.50 2.82 32.94 #&gt; &quot;extract_rows&quot; 0.28 3.27 3.46 40.42 #&gt; &quot;as.list&quot; 0.28 3.27 0.54 6.31 #&gt; &quot;[[.data.frame&quot; 0.24 2.80 0.56 6.54 #&gt; &quot;$&quot; 0.24 2.80 0.24 2.80 #&gt; &quot;make_names&quot; 0.22 2.57 0.36 4.21 # Rprof(&quot;out/slow_rprof.out&quot;) # players &lt;- unique(batting$playerID) # n_players &lt;- length(players) # total &lt;- rep(NA, n_players) # n &lt;- rep(NA, n_players) # for(i in 1:n_players){ # sub_batting &lt;- batting[batting$playerID == players[i], ] # total[i] &lt;- sum(sub_batting$R) # n[i] &lt;- nrow(sub_batting) # } # batting_2 &lt;- data.frame(playerID = players, total = total, n = n) # batting_2[order(batting_2$total, decreasing = TRUE), ] # Rprof(NULL) summaryRprof(&quot;out/slow_rprof.out&quot;)$by.self[1:10, ] #&gt; self.time self.pct total.time total.pct #&gt; &quot;[.data.frame&quot; 45.50 54.74 82.74 99.54 #&gt; &quot;==&quot; 24.78 29.81 24.78 29.81 #&gt; &quot;attr&quot; 6.70 8.06 6.70 8.06 #&gt; &quot;NextMethod&quot; 3.42 4.11 3.44 4.14 #&gt; &quot;[[&quot; 1.32 1.59 2.12 2.55 #&gt; &quot;[[.data.frame&quot; 0.22 0.26 0.80 0.96 #&gt; &quot;&lt;Anonymous&gt;&quot; 0.20 0.24 0.46 0.55 #&gt; &quot;all&quot; 0.10 0.12 0.10 0.12 #&gt; &quot;sys.call&quot; 0.10 0.12 0.10 0.12 #&gt; &quot;%in%&quot; 0.08 0.10 0.22 0.26 Estrategias para mejorar desempeño Utilizar apropiadamente funciones de R, o funciones de paquetes que muchas veces están mejor escritas de lo que nosotros podríamos hacer. Hacer lo menos posible. Usar funciones vectorizadas en R (casi siempre). No hacer crecer objetos (es preferible definir su tamaño antes de operar en ellos). Paralelizar. La más simple y muchas veces la más barata: conseguie una máquina más grande (por ejemplo Amazon Web Services). 1 Utilizar apropiadamente funciones de R Si el cuello de botella es la función de un paquete vale la pena buscar alternativas, CRAN task views es un buen lugar para buscar. 2 Hacer lo menos posible Utiliza funciones más específicas, por ejemplo: rowSums(), colSums(), rowMeans() y colMeans() son más rápidas que las invocaciones equivalentes de apply(). Si quieres checar si un vector contiene un valor any(x == 10) es más veloz que 10 %in% x, esto es porque examinar igualdad es más sencillo que examinar inclusión en un conjunto. Este conocimiento requiere que conozcas alternativas, para ello debes construir tu vocabulario, puedes comenzar por lo básico e ir incrementando conforme lees código. Otro caso es cuando las funciones son más rápidas cunado les das más información del problema, por ejemplo: read.csv(), especificar las clases de las columnas con colClasses. factor() especifica los niveles con el argumento levels. 3.1 Usar funciones vectorizadas en R Es común escuchar que en R vectorizar es conveniente, el enfoque vectorizado va más allá que evitar ciclos for: Pensar en objetos, en lugar de enfocarse en las compoentes de un vector, se piensa únicamente en el vector completo. Los ciclos en las funciones vectorizadas de R están escritos en C, lo que los hace más veloces. Las funciones vectorizadas programadas en R pueden mejorar la interfaz de una función pero no necesariamente mejorar el desempeño. Usar vectorización para desempeño implica encontrar funciones de R implementadas en C. Al igual que en el punto anterior, vectorizar requiere encontrar las funciones apropiadas, algunos ejemplos incluyen: _rowSums(), colSums(), rowMeans() y colMeans(). 3.2 Evitar copias Otro aspecto importante es que generalmente conviene asignar objetos en lugar de hacerlos crecer (es más eficiente asignar toda la memoria necesaria antes del cálculo que asignarla sucesivamente). Esto es porque cuando se usan instrucciones para crear un objeto más grande (e.g. append(), cbind(), c(), rbind()) R debe primero asignar espacio a un nuevo objeto y luego copiar al nuevo lugar. Para leer más sobre esto The R Inferno es una buena referencia. Veamos unos ejemplos de vectorización y de asignar objetos. aciertos &lt;- FALSE system.time( for (i in 1:1e+05) { if (runif(1) &lt; 0.3) aciertos[i] &lt;- TRUE }) #&gt; user system elapsed #&gt; 0.363 0.076 0.439 aciertos &lt;- rep(FALSE, 1e+06) system.time( for (i in 1:1e+05) { if (runif(1) &lt; 0.3) aciertos[i] &lt;- TRUE }) #&gt; user system elapsed #&gt; 0.345 0.064 0.409 Usando rbind: crecer_rbind &lt;- function(){ mi.df &lt;- data.frame(a = character(0), b = numeric(0)) for(i in 1:1e3) { mi.df &lt;- rbind(mi.df, data.frame(a = sample(letters, 1), b = runif(1))) } mi.df } system.time(mi.df.1 &lt;- crecer_rbind()) #&gt; user system elapsed #&gt; 1.05 0.00 1.06 Si definimos el tamaño del data.frame obtenemos mejoras: crecer_rbind_2 &lt;- function() { mi.df &lt;- data.frame(a = rep(NA, 1000), b = rep(NA, 1000)) for (i in 1:1000) { mi.df$a[i] &lt;- sample(letters, 1) mi.df$b[i] &lt;- runif(1) } mi.df } system.time(mi.df.1 &lt;- crecer_rbind_2()) #&gt; user system elapsed #&gt; 0.079 0.000 0.079 Finalmente, veamos un enfoque totalmente vectorizado porcolumna_df &lt;- function(){ a &lt;- sample(letters, 1000, replace = TRUE) b &lt;- runif(1000) mi.df &lt;- data.frame(a = a, b = b) mi.df } system.time(mi.df.2 &lt;- porcolumna_df()) #&gt; user system elapsed #&gt; 0.001 0.000 0.002 A pesar de que aumentamos la velocidad conforme aumentamos el nivel de vectorización, este incremento conlleva un costo en memoria. Si comparamos la versión mas lenta con la más rápida, en la última debemos asignar a, b y mi.df. Entonces, no siempre es mejor vectorizar, pues si consumimos la memoria, entonces la versión vectorizada puede enfrentarse al problema de uso de memoria en disco, que tiene aun más grandes penalizaciones en el desempeño que los ciclos que hemos visto. 4 Paralelizar Paralelizar usa varios cores para trabajar de manera simultánea en varias secciones de un problema, no reduce el tiempo computacional pero incrementa el tiempo del usuario pues aprovecha los recursos. Como referencia está Parallel Computing for Data Science de Norm Matloff. 2.8 Tarea Considerando la lista siguiente, cdmx_list &lt;- list( pop = 8918653, delegaciones = c(&quot;Alvaro Obregón&quot;, &quot;Azcapotzalco&quot; ,&quot;Benito Juárez&quot; , &quot;Coyoacán&quot; ,&quot;Cuajimalpa de Morelos&quot; ,&quot;Cuauhtémoc&quot; , &quot;Gustavo A. Madero&quot; , &quot;Iztacalco&quot; ,&quot;Iztapalapa&quot; , &quot;Magdalena Contreras&quot; ,&quot;Miguel Hidalgo&quot; ,&quot;Milpa Alta&quot; , &quot;Tláhuac&quot; ,&quot;Tlalpan&quot; , &quot;Venustiano Carranza&quot; ,&quot;Xochimilco&quot;), capital = TRUE ) obtén la clase de cada elemento con la función lapply(). lapply( , class) La siguiente función extrae la letra de menor posición y mayor posición en orden alfabético. min_max_fun &lt;- function(nombre){ nombre_sinespacios &lt;- gsub(&quot; &quot;, &quot;&quot;, nombre) letras &lt;- strsplit(nombre_sinespacios, split = &quot;&quot;)[[1]] c(minimo = min(letras), maximo = max(letras)) } Es decir, si incluimos las letras abcz la letra mínima es a y la máxima es z. min_max_fun(&quot;abcz&quot;) #&gt; minimo maximo #&gt; &quot;a&quot; &quot;z&quot; El siguiente vector incluye el nombre de las 16 delegaciones de la Ciudad de México. delegaciones &lt;- c(&quot;Alvaro Obregon&quot;, &quot;Azcapotzalco&quot; ,&quot;Benito Juarez&quot; , &quot;Coyoacan&quot; ,&quot;Cuajimalpa de Morelos&quot; ,&quot;Cuauhtemoc&quot; , &quot;Gustavo Madero&quot; , &quot;Iztacalco&quot; ,&quot;Iztapalapa&quot; , &quot;Magdalena Contreras&quot; ,&quot;Miguel Hidalgo&quot; ,&quot;Milpa Alta&quot; , &quot;Tlahuac&quot; ,&quot;Tlalpan&quot; , &quot;Venustiano Carranza&quot; ,&quot;Xochimilco&quot;) Aplica la función sapply() para obtener un arreglo con la letra máxima y mínima de cada nombre. sapply(, ) El siguiente vector incluye el precio de la gasolina en diferentes estados del país en julio de 2017. gas_cdmx &lt;- c(15.82, 15.77, 15.83, 15.23, 14.95, 15.42, 15.55) gas_cdmx #&gt; [1] 15.8 15.8 15.8 15.2 14.9 15.4 15.6 Crea una función que convierta el precio a dolares suponiendo que un dolar equivale a 17.76 pesos. conv_fun &lt;- function(precio){ /17.76 return() } Usando la función lapply() convierte el precio de la gasolina a dolares. gas_cdmx_usd_lista &lt;- lapply(, conv_fun) Usa la función unlist() para convertir la lista a un vector. gas_cdmx_usd &lt;- unlist() print(gas_cdmx_usd) Estadísticos importantes estadisticos &lt;- c(&quot;GAUSS:1777&quot;, &quot;BAYES:1702&quot;, &quot;FISHER:1890&quot;, &quot;PEARSON:1857&quot;) split_estadisticos &lt;- strsplit(estadisticos, split = &quot;:&quot;) split_estadisticos #&gt; [[1]] #&gt; [1] &quot;GAUSS&quot; &quot;1777&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;BAYES&quot; &quot;1702&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;FISHER&quot; &quot;1890&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;PEARSON&quot; &quot;1857&quot; Utiliza la función predefinida tolower() y lapply() para convertir a minúsculas cada letra de la lista split_estadisticos. split_lower &lt;- lapply( , ) print(split_lower) Usando el vector split_estadísticos del ejercicio anterior. str(split_estadisticos) #&gt; List of 4 #&gt; $ : chr [1:2] &quot;GAUSS&quot; &quot;1777&quot; #&gt; $ : chr [1:2] &quot;BAYES&quot; &quot;1702&quot; #&gt; $ : chr [1:2] &quot;FISHER&quot; &quot;1890&quot; #&gt; $ : chr [1:2] &quot;PEARSON&quot; &quot;1857&quot; Crea una función que regrese la primera posición. primera_pos_fun &lt;- function(lista){ } Crea una función que regrese la segunda posición. segunda_pos_fun &lt;- function(lista){ } Usando lapply() crea una lista con los nombres de los estadísticos y otra con la fecha de nacimiento. nombres &lt;- lapply() fechas &lt;- lapply() Usando una función anónima y el vector split_estadísticos en un solo lapply() o sapply() obtén un vector compuesto de la primera posición, es decir el nombre, en minúsculas. Tip: si usas lapply() recuerda usar la función unlist(). nombre_estadisticos &lt;- (split_estadisticos, function(elemento){ tolower() }) nombre_estadisticos En la siguiente lista se presenta el registro de temperatura de tres ciudades a las 07:00 am, 10:00 am, 01:00 pm, 04:00 pm y 07:00 pm. temp_lista &lt;- list( cdmx = c(13, 15, 19, 22, 20), guadalajara = c(18, 18, 22, 26, 27), tuxtla_gtz = c(22, 24, 29, 32, 28) ) str(temp_lista) #&gt; List of 3 #&gt; $ cdmx : num [1:5] 13 15 19 22 20 #&gt; $ guadalajara: num [1:5] 18 18 22 26 27 #&gt; $ tuxtla_gtz : num [1:5] 22 24 29 32 28 Completa la siguiente función que obtiene el promedio entre el valor mínimo y máximo registrados. promedio_extremos_fun &lt;- function(x) { ( min() + max() ) / 2 } Aplica la función a la lista y obtén la temperatura promedio de extremos para cada ciudad usando lapply() y sapply(). lapply(,) sapply(,) Crea una función en la que mientras la velocidad sea mayor a 50 km/hr se reduzca de la siguiente forma: Si es mayor a 80 km/hr se reducen 20 km/hr e imprime ¡Demasido rápido!. Si es menor o igual a 80km/hr se reducen únicamente 5 km/hr. velocidad_act &lt;- 140 while(velocidad_act &gt; ){ if(velocidad_act &gt; ){ print() velocidad_act &lt;- } if(velocidad_act &lt; ){ velocidad_act &lt;- } velocidad_act } Referencias "],
["manipulacion-y-visualizacion-de-datos.html", "Clase 3 Manipulación y visualización de datos 3.1 El principio de datos limpios 3.2 Limpieza de datos 3.3 Separa-aplica-combina 3.4 Muertes por armas de fuego en EUA 3.5 El Cuarteto de Anscombe 3.6 The Grammar of Graphics de Leland Wilkinson 3.7 ggplot 3.8 Un histograma de las muertes en Iraq 3.9 Inglehart–Welzel: un mapa cultural del mundo 3.10 Poniendo todo junto 3.11 Tarea", " Clase 3 Manipulación y visualización de datos .espacio { margin-bottom: 1cm; } .espacio3 { margin-bottom: 3cm; } “Happy families are all alike; every unhappy family is unhappy in its own way.” — Leo Tolstoy “Tidy datasets are all alike; but every messy dataset is messy in its own way.” — Hadley Wickham Comencemos nuevamente cargando el paquete tidyverse: library(tidyverse) La visualización es una herramienta importante para generar información. Sin embargo, es muy raro obtener los datos exactamente en la forma en que se necesitan. Es común tener que crear nuevas variables o hacer resúmenes a partir de algunas variables, o tal vez sólo sea necesario cambiar el nombre de las variables o reordenar las observaciones con el fin de facilitar el análisis de datos. Pipeline La idea de pipeline intenta hacer el desarrollo de código más fácil, en menor tiempo, fácil de leerlo, y por lo tanto, más fácil mantenerlo. En el análisis de datos es común hacer varias operaciones y se vuelve difícil leer y entender el código. La dificultad radica en que usualmente los parámetros se asignan después del nombre de la función usando (). La forma en que esta idea logra hacer las cosas más faciles es con el operador forwad pipe %&gt;%que envía un valor a una expresión o función. Este cambio en el orden funciona como el parámetro que precede a la función es enviado (“piped”) a la función. Es decir, supongamos x es una valor y sea f una función, entonces, x %&gt;% f es igual a f(x). Por ejemplo, sea \\(f(x)\\) la función de probabilidad de la distribución normal con media \\(\\mu = 0\\) y desviación estándar \\(\\sigma = 1\\): \\[ f(x) = \\dfrac{ 1 }{\\sqrt{2\\pi}} e^{- \\frac{1}{2} x^2 } \\] f &lt;- function(x){ exp(-(x^2)/2)/sqrt(2*pi) } # Con el operador de pipe 0 %&gt;% f #&gt; [1] 0.399 que de forma tradicional se realiza: # Forma tradicional f(0) #&gt; [1] 0.399 En resumen %&gt;% funciona como se muestra en la siguiente figura: Nota: Se puede insertar el pipe %&gt;% utilizando: Cmd/Ctrl + Shift + M. ¿Qué hace el siguiente código? ¿Qué hace .? df &lt;- data_frame( x = runif(5), y = rnorm(5) ) df %&gt;% .$x df %&gt;% ggplot(data = ., aes(x = x, y = y)) + geom_point() Tibbles Tibbles son dataframes con algunas modificaciones que permitirán trabajar mejor con los paquetes de limpieza y manipulación de datos tidyr y dplyr. Una diferencia son los tipos de columnas que maneja: lgl: vectores de valores lógicos, vectores que contienen TRUE o FALSE. int: vectores de números enteros. dbl: vectores de números reales. chr: vectores de caracteres, strings. Imprime ds y as.data.frame(ds). ¿Cuál es la diferencia entre ambas? ds &lt;- tbl_df(mtcars) ds as.data.frame(ds) Nota: Para mayor información de este tipo de dataframes consulta la documentación de la libreria tibble. 3.1 El principio de datos limpios Los principios de datos limpios (Tidy Data de Hadley Wickham) proveen una manera estándar de organizar la información: Cada variable forma una columna. Cada observación forma un renglón. Cada tipo de unidad observacional forma una tabla. Nota: La mayor parte de las bases de datos en estadística tienen forma rectangular por lo que únicamente se trataran este tipo de estructura de datos. Una base de datos es una colección de valores numéricos o categóricos. Cada valor pertenece a una variable y a una observación. Una variable contiene los valores del atributo (genero, fabricante, ingreso) de la variable por unidad. Una observación contiene todos los valores medidos por la misma unidad (personas, día, autos, municipios) para diferentes atributos. Ejemplo: Supongamos un experimento con 3 pacientes cada uno tiene resultados de dos tratamientos (A y B): tratamientoA tratamientoB Juan Aguirre - 2 Ana Bernal 16 11 José López 3 1 La tabla anterior también se puede estructurar de la siguiente manera: Juan Aguirre Ana Bernal José López tratamientoA - 16 3 tratamientoB 2 11 1 Si vemos los principios, entonces ¿las tablas anteriores los cumplen? Para responder la pregunta veamos: ¿Cuáles son los valores? En total se tienen 18 valores en el conjunto de datos. ¿Cuáles son las variables? Se tienen tres variables: Persona/nombre: Juan Aguirre, Ana Bernal, y José López Tratamiento: A y B Resultado: -, 2, 16, 11, 3, 1 ¿Cuáles son las observaciones? Existen 6 observaciones. Entonces, siguiendo los principios de datos limpios obtenemos la siguiente estructura: nombre tratamiento resultado Juan Aguirre a - Ana Bernal a 16 José López a 3 Juan Aguirre b 2 Ana Bernal b 11 José López b 1 Una vez que identificamos los problemas de una base de datos podemos proceder a la limpieza. 3.2 Limpieza de datos Algunos de los problemas más comunes en las bases de datos que no están limpias son: Los encabezados de las columnas son valores y no nombres de variables. Más de una variable por columna. Las variables están organizadas tanto en filas como en columnas. Más de un tipo de observación en una tabla. Una misma unidad observacional está almacenada en múltiples tablas. La mayor parte de estos problemas se pueden arreglar con pocas herramientas, a continuación veremos como limpiar datos usando dos funciones del paquete tidyr de Hadley Wickham: gather: recibe múltiples columnas y las junta en pares de nombres y valores, convierte los datos anchos en largos. spread: recibe 2 columnas y las separa, haciendo los datos más anchos. Repasaremos los problemas más comunes que se encuentran en conjuntos de datos sucios y mostraremos cómo se puede manipular la tabla de datos (usando las funciones gather y spread) con el fin de estructurarla para que cumpla los principios de datos limpios. 1. Los encabezados de las columnas son valores Analicemos los datos que provienen de una encuesta de Pew Research que investiga la relación entre ingreso y afiliación religiosa. ¿Cuáles son las variables en estos datos? pew &lt;- read_csv(&quot;datos/pew.csv&quot;) knitr::kable(pew) %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;, font_size = 4) religion &lt;$10k $10-20k $20-30k $30-40k $40-50k $50-75k $75-100k $100-150k &gt;150k Don’t know/refused Agnostic 27 34 60 81 76 137 122 109 84 96 Atheist 12 27 37 52 35 70 73 59 74 76 Buddhist 27 21 30 34 33 58 62 39 53 54 Catholic 418 617 732 670 638 1116 949 792 633 1489 Don’t know/refused 15 14 15 11 10 35 21 17 18 116 Evangelical Prot 575 869 1064 982 881 1486 949 723 414 1529 Hindu 1 9 7 9 11 34 47 48 54 37 Historically Black Prot 228 244 236 238 197 223 131 81 78 339 Jehovah’s Witness 20 27 24 24 21 30 15 11 6 37 Jewish 19 19 25 25 30 95 69 87 151 162 Mainline Prot 289 495 619 655 651 1107 939 753 634 1328 Mormon 29 40 48 51 56 112 85 49 42 69 Muslim 6 7 9 10 9 23 16 8 6 22 Orthodox 13 17 23 32 32 47 38 42 46 73 Other Christian 9 7 11 13 13 14 18 14 12 18 Other Faiths 20 33 40 46 49 63 46 40 41 71 Other World Religions 5 2 3 4 2 7 3 4 4 8 Unaffiliated 217 299 374 365 341 528 407 321 258 597 Para limpiarla es necesario apilar las columnas, es decir, pasar los datos a forma larga. Esto lo realizaremos con la función gather(): pew_tidy &lt;- pew %&gt;% gather(income, frequency, -religion) # vemos las primeras líneas de nuestros datos alargados pew_tidy %&gt;% head() %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;, font_size = 4) #&gt; Warning in kableExtra::kable_styling(., bootstrap_options = &quot;striped&quot;, #&gt; font_size = 4): Please specify format in kable. kableExtra can customize #&gt; either HTML or LaTeX outputs. See https://haozhu233.github.io/kableExtra/ #&gt; for details. religion income frequency Agnostic &lt;$10k 27 Atheist &lt;$10k 12 Buddhist &lt;$10k 27 Catholic &lt;$10k 418 Don’t know/refused &lt;$10k 15 Evangelical Prot &lt;$10k 575 La nueva estructura de la base de datos nos permite, por ejemplo, hacer fácilmente una gráfica donde podemos comparar las diferencias en las frecuencias. library(dplyr) by_religion &lt;- group_by(pew_tidy, religion) pew_tidy_2 &lt;- pew_tidy %&gt;% filter(income != &quot;Don&#39;t know/refused&quot;) %&gt;% group_by(religion) %&gt;% mutate(percent = frequency / sum(frequency)) %&gt;% filter(sum(frequency) &gt; 1000) ggplot(pew_tidy_2, aes(x = income, y = percent, group = religion)) + facet_wrap(~ religion) + geom_bar(stat = &quot;identity&quot;, fill = &quot;darkgray&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) En el código de arriba utilizamos las funciones group_by, filter y mutate que estudiaremos más adelante. Otro ejemplo, billboard &lt;- tbl_df(read.csv(&quot;datos/billboard.csv&quot;, stringsAsFactors = FALSE)) billboard %&gt;% sample_n(5) %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;, font_size = 4) year artist track time date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 wk9 wk10 wk11 wk12 wk13 wk14 wk15 wk16 wk17 wk18 wk19 wk20 wk21 wk22 wk23 wk24 wk25 wk26 wk27 wk28 wk29 wk30 wk31 wk32 wk33 wk34 wk35 wk36 wk37 wk38 wk39 wk40 wk41 wk42 wk43 wk44 wk45 wk46 wk47 wk48 wk49 wk50 wk51 wk52 wk53 wk54 wk55 wk56 wk57 wk58 wk59 wk60 wk61 wk62 wk63 wk64 wk65 wk66 wk67 wk68 wk69 wk70 wk71 wk72 wk73 wk74 wk75 wk76 2000 Backstreet Boys, The The One 3:46 2000-05-27 58 50 43 37 31 30 39 47 55 61 76 90 93 93 100 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 Sisqo Incomplete 3:52 2000-06-24 77 66 61 61 61 55 2 1 1 2 2 4 5 5 7 8 10 10 9 14 17 20 25 31 32 46 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 Madison Avenue Don’t Call Me Baby 3:44 2000-07-08 98 96 93 93 93 92 92 92 90 92 88 88 88 95 93 98 93 92 90 97 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 Cagle, Chris My Love Goes On And … 3:02 2000-10-21 99 94 94 87 84 83 76 76 79 83 91 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2000 3 Doors Down Kryptonite 3:53 2000-04-08 81 70 68 67 66 57 54 53 51 51 51 51 47 44 38 28 22 18 18 14 12 7 6 6 6 5 5 4 4 4 4 3 3 3 4 5 5 9 9 15 14 13 14 16 17 21 22 24 28 33 42 42 49 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Queremos apilar las semanas de manera que sea una sola columna (nuevamente alargamos los datos): library(tidyr) billboard_long &lt;- gather(billboard, week, rank, wk1:wk76, na.rm=TRUE) billboard_long %&gt;% sample_n(10) %&gt;% knitr::kable() year artist track time date.entered week rank 2000 Lil’ Zane Callin’ Me 3:43 2000-07-29 wk9 45 2000 DMX Party Up (Up In Here… 3:45 2000-02-26 wk10 27 2000 Aguilera, Christina Come On Over Baby (A… 3:38 2000-08-05 wk6 18 2000 Thomas, Carl Emotional 4:31 2000-11-25 wk15 63 2000 Amber Sexual 4:38 1999-07-17 wk17 98 2000 Goo Goo Dolls Broadway 3:54 2000-04-22 wk20 87 2000 Westlife Swear It Again 4:07 2000-04-01 wk3 66 2000 Lonestar What About Now 3:30 2000-06-10 wk1 78 2000 Madonna Music 3:45 2000-08-12 wk6 1 2000 DMX Party Up (Up In Here… 3:45 2000-02-26 wk8 32 La instrucción na.rm = TRUE se utiliza para eliminar los valores faltantes en las columnas wk1 a wk76. Realizamos una limpieza adicional creando mejores variables de fecha. billboard_tidy &lt;- billboard_long %&gt;% mutate( week = extract_numeric(week), date = as.Date(date.entered) + 7 * (week - 1)) %&gt;% select(-date.entered) billboard_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() year artist track time week rank date 2000 Ginuwine The Best Man I Can B… 4:06 4 94 2000-01-29 2000 Dion, Celine That’s The Way It Is 4:03 8 30 2000-01-01 2000 Avant Separated 4:13 2 32 2000-05-06 2000 Vassar, Phil Carlene 4:07 7 47 2000-04-15 2000 Creed With Arms Wide Open 3:52 35 5 2001-01-06 2000 Aguilera, Christina Come On Over Baby (A… 3:38 6 18 2000-09-09 2000 Jagged Edge Let’s Get Married 4:23 14 14 2000-08-05 2000 Walker, Clay The Chain Of Love 5:03 15 71 2000-07-22 2000 Creed Higher 5:16 65 49 2000-12-02 2000 Lonestar Amazed 4:25 37 18 2000-02-12 Nuevamente, podemos hacer gráficas fácilmente. tracks &lt;- billboard_tidy %&gt;% filter(track %in% c(&quot;Come On Over Baby (A...&quot;, &quot;What A Girl Wants&quot;, &quot;Say My Name&quot;, &quot;Jumpin&#39; Jumpin&#39;&quot;, &quot;Bye Bye Bye&quot;)) ggplot(tracks, aes(x = date, y = rank)) + geom_line() + facet_wrap(~track, nrow = 1) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) 2. Una columna asociada a más de una variable La siguiente base de datos proviene de la Organización Mundial de la Salud y contiene el número de casos confirmados de tuberculosis por país y año, la información esta por grupo demográfico de acuerdo a sexo (m, f), y edad (0-4, 5-14, etc). library(countrycode) tb &lt;- read_csv(&quot;datos/tb.csv&quot;) tb$country_name &lt;- countrycode(tb$iso2, &#39;iso2c&#39;, &#39;country.name&#39;) tb %&gt;% sample_n(5) %&gt;% knitr::kable() iso2 year new_sp_m04 new_sp_m514 new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 new_sp_m4554 new_sp_m5564 new_sp_m65 new_sp_mu new_sp_f04 new_sp_f514 new_sp_f014 new_sp_f1524 new_sp_f2534 new_sp_f3544 new_sp_f4554 new_sp_f5564 new_sp_f65 new_sp_fu country_name PT 1980 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Portugal AT 2002 NA NA 1 8 14 32 43 20 25 NA NA NA 0 8 13 7 5 7 21 NA Austria LU 1984 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Luxembourg OM 1999 NA NA 2 10 11 23 15 7 10 NA NA NA 3 16 4 6 1 4 8 NA Oman NZ 1985 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA New Zealand De manera similar, utilizando la función gather() se busca apilar las columnas correspondientes a sexo-edad. ¿Cómo podemos separar la “variable” sexo-edad en dos columnas? tb_long &lt;- tb %&gt;% gather(demog, casos, new_sp_m04:new_sp_fu, na.rm=TRUE) tb_long %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name demog casos BZ 2001 Belize new_sp_m014 0 AN 2004 Netherlands Antilles new_sp_m3544 4 CA 1991 Canada new_sp_m4554 37 NP 2002 Nepal new_sp_f1524 1203 RO 2006 Romania new_sp_m65 580 BH 1995 Bahrain new_sp_m65 3 PH 1998 Philippines new_sp_f2534 109 GN 2004 Guinea new_sp_f65 63 GU 2001 Guam new_sp_m2534 4 TM 1999 Turkmenistan new_sp_m2534 225 Las variables sexo y edad se obtienen separando la columna demog, para esto se usa la función separate()con los siguientes argumentos: tidyr::separate(data, col = name_variabletoseparate, into = c(vector with names using &quot;&quot;), sep) tb_tidy &lt;- tb_long %&gt;% separate(col = demog, into = c(&quot;sex&quot;, &quot;age&quot;), sep = 8) tb_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name sex age casos GN 2004 Guinea new_sp_f 2534 521 NG 2008 Nigeria new_sp_m u 0 PY 2006 Paraguay new_sp_f 1524 130 AT 2007 Austria new_sp_f 2534 14 BO 1997 Bolivia (Plurinational State of) new_sp_m 1524 1214 MR 1999 Mauritania new_sp_f 3544 110 NI 2007 Nicaragua new_sp_f 3544 100 VU 2005 Vanuatu new_sp_f 65 2 NG 2005 Nigeria new_sp_f 65 415 JO 2006 Jordan new_sp_m 5564 4 Ahora para hacer mejor variable sex y age usaremos la función mutate() que permite crear nuevas variables sin modificar la dimensión del dataframe. library(stringr) tb_tidy &lt;- tb_long %&gt;% separate(col = demog, into = c(&quot;sex&quot;, &quot;age&quot;), sep = 8) %&gt;% mutate(sex = str_sub(sex, 8, 8), age = factor(age, levels = c(&quot;014&quot;, &quot;04&quot;, &quot;1524&quot;, &quot;2534&quot;, &quot;3544&quot;, &quot;4554&quot;, &quot;514&quot;, &quot;5564&quot;, &quot;65&quot;,&quot;u&quot;), labels = c(&quot;0-14&quot;, &quot;0-4&quot;, &quot;15-24&quot;, &quot;25-34&quot;, &quot;35-44&quot;, &quot;45-54&quot;, &quot;5-14&quot;, &quot;55-64&quot;, &quot;65+&quot;,&quot;unknown&quot;) ) ) tb_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name sex age casos LB 2004 Lebanon m 65+ 6 GT 2007 Guatemala m 45-54 203 EG 2007 Egypt m 25-34 853 BS 2003 Bahamas f 0-14 2 ZW 2007 Zimbabwe m 65+ 153 SG 1999 Singapore f 35-44 18 PA 1999 Panama m 25-34 209 SL 1998 Sierra Leone f 25-34 294 MX 2000 Mexico m 0-14 214 UA 1999 Ukraine m 45-54 1825 Se puede separar la columna demog en dos variables, sexo y edad, utilizando la función separate. Se debe indicar la posición de donde deseamos “cortar”: tb_tidy &lt;- tidyr::separate(tb_long, demog, c(&quot;sex&quot;, &quot;age&quot;), 8) tb_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() iso2 year country_name sex age casos KZ 1998 Kazakhstan new_sp_f 4554 204 PL 2001 Poland new_sp_m 3544 603 NR 2003 Nauru new_sp_f 014 0 NC 1996 New Caledonia new_sp_m 4554 5 BH 2008 Bahrain new_sp_f 1524 12 LK 1995 Sri Lanka new_sp_m 2534 361 GH 2000 Ghana new_sp_f 65 176 ES 2002 Spain new_sp_f 014 17 GA 1998 Gabon new_sp_f 014 15 SE 1999 Sweden new_sp_m 3544 12 3. Variables almacenadas en filas y columnas El problema más difícil es cuando las variables están tanto en filas como encolumnas, veamos una base de datos de clima en Cuernavaca. ¿Cuáles son las variables en estos datos? clima &lt;- tbl_df(read.delim(&quot;datos/clima.txt&quot;, stringsAsFactors=FALSE)) clima %&gt;% sample_n(10) %&gt;% knitr::kable() id year month element d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 d11 d12 d13 d14 d15 d16 d17 d18 d19 d20 d21 d22 d23 d24 d25 d26 d27 d28 d29 d30 d31 MX000017004 2010 5 TMIN NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 182 NA NA NA NA MX000017004 2010 4 TMIN NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 167 NA NA NA NA MX000017004 2010 1 TMAX NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 278 NA MX000017004 2010 5 TMAX NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 332 NA NA NA NA MX000017004 2010 12 TMAX 299 NA NA NA NA 278 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA MX000017004 2010 11 TMIN NA 163 NA 120 79 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 121 142 NA NA NA NA MX000017004 2010 4 TMAX NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 363 NA NA NA NA MX000017004 2010 11 TMAX NA 313 NA 272 263 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 281 277 NA NA NA NA MX000017004 2010 6 TMIN NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 175 NA NA NA NA NA NA NA NA NA NA NA 180 NA NA MX000017004 2010 3 TMIN NA NA NA NA 142 NA NA NA NA 168 NA NA NA NA NA 176 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Estos datos tienen variables en columnas individuales (id, año, mes), en múltiples columnas (día, d1-d31) y en filas (tmin, tmax). Comencemos por apilar las columnas. clima_long &lt;- clima %&gt;% gather(day, value, d1:d31, na.rm = TRUE) head(clima_long) %&gt;% knitr::kable() id year month element day value MX000017004 2010 12 TMAX d1 299 MX000017004 2010 12 TMIN d1 138 MX000017004 2010 2 TMAX d2 273 MX000017004 2010 2 TMIN d2 144 MX000017004 2010 11 TMAX d2 313 MX000017004 2010 11 TMIN d2 163 Podemos crear algunas variables adicionales. clima_vars &lt;- clima_long %&gt;% mutate(day = extract_numeric(day), value = value / 10) %&gt;% select(id, year, month, day, element, value) %&gt;% arrange(id, year, month, day) #&gt; extract_numeric() is deprecated: please use readr::parse_number() instead head(clima_vars) %&gt;% knitr::kable() id year month day element value MX000017004 2010 1 30 TMAX 27.8 MX000017004 2010 1 30 TMIN 14.5 MX000017004 2010 2 2 TMAX 27.3 MX000017004 2010 2 2 TMIN 14.4 MX000017004 2010 2 3 TMAX 24.1 MX000017004 2010 2 3 TMIN 14.4 Finalmente, la columna element no es una variable, sino que almacena el nombre de dos variables, la operación que debemos aplicar (spread) es el inverso de apilar (gather): clima_tidy &lt;- clima_vars %&gt;% spread(element, value) clima_tidy %&gt;% sample_n(10) %&gt;% knitr::kable() id year month day TMAX TMIN MX000017004 2010 8 23 26.4 15.0 MX000017004 2010 8 29 28.0 15.3 MX000017004 2010 4 27 36.3 16.7 MX000017004 2010 11 27 27.7 14.2 MX000017004 2010 7 14 29.9 16.5 MX000017004 2010 10 28 31.2 15.0 MX000017004 2010 6 17 28.0 17.5 MX000017004 2010 3 5 32.1 14.2 MX000017004 2010 8 13 29.8 16.5 MX000017004 2010 2 2 27.3 14.4 Ahora es inmediato no solo hacer gráficas sino también ajustar un modelo. # ajustamos un modelo lineal donde la variable respuesta es temperatura # máxima, y la variable explicativa es el mes clima_lm &lt;- lm(TMAX ~ factor(month), data = clima_tidy) summary(clima_lm) #&gt; #&gt; Call: #&gt; lm(formula = TMAX ~ factor(month), data = clima_tidy) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.65 -0.92 -0.02 1.05 3.18 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 27.800 1.861 14.94 5.3e-13 *** #&gt; factor(month)2 -0.050 2.081 -0.02 0.9810 #&gt; factor(month)3 4.767 2.149 2.22 0.0372 * #&gt; factor(month)4 8.500 2.632 3.23 0.0039 ** #&gt; factor(month)5 5.400 2.632 2.05 0.0523 . #&gt; factor(month)6 1.250 2.279 0.55 0.5889 #&gt; factor(month)7 1.450 2.279 0.64 0.5312 #&gt; factor(month)8 0.471 1.990 0.24 0.8149 #&gt; factor(month)10 1.100 2.039 0.54 0.5949 #&gt; factor(month)11 0.320 2.039 0.16 0.8767 #&gt; factor(month)12 1.050 2.279 0.46 0.6496 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.86 on 22 degrees of freedom #&gt; Multiple R-squared: 0.618, Adjusted R-squared: 0.445 #&gt; F-statistic: 3.56 on 10 and 22 DF, p-value: 0.0062 4. Mas de un tipo de observación en una misma tabla En ocasiones las bases de datos involucran valores en diferentes niveles, endiferentes tipos de unidad observacional. En la limpieza de datos, cada unidad observacional debe estar almacenada en su propia tabla (esto esta ligado a normalización de una base de datos), es importante para evitar inconsistencias en los datos. ¿Cuáles son las unidades observacionales de los datos de billboard? billboard_tidy %&gt;% arrange(artist, track, year, time) %&gt;% head(20) #&gt; # A tibble: 20 x 7 #&gt; year artist track time week rank date #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; #&gt; 1 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 1.00 87 2000-02-26 #&gt; 2 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 2.00 82 2000-03-04 #&gt; 3 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 3.00 72 2000-03-11 #&gt; 4 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 4.00 77 2000-03-18 #&gt; 5 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 5.00 87 2000-03-25 #&gt; 6 2000 2 Pac Baby Don&#39;t Cry (Keep... 4:22 6.00 94 2000-04-01 #&gt; # ... with 14 more rows Separemos esta base de datos en dos: la tabla canción que almacena artista, nombre de la canción y duración; la tabla rank que almacena el ranking de la canción en cada semana. song &lt;- billboard_tidy %&gt;% select(artist, track, year, time) %&gt;% unique() %&gt;% arrange(artist) %&gt;% mutate(song_id = row_number(artist)) song %&gt;% sample_n(10) %&gt;% knitr::kable() artist track year time song_id Zombie Nation Kernkraft 400 2000 3:30 316 Brooks, Garth Do What You Gotta Do 2000 2:56 48 Larrieux, Amel Get Up 2000 4:02 167 Sisqo Thong Song 2000 4:05 265 Next Wifey 2000 4:03 224 Diffie, Joe The Quittin’ Kind 2000 3:23 79 Lil Wayne Tha Block Is Hot 2000 4:13 172 SheDaisy This Woman Needs 2000 3:20 258 Aaliyah Try Again 2000 4:03 9 Jay-Z Big Pimpin’ 2000 3:55 145 rank &lt;- billboard_tidy %&gt;% left_join(song, c(&quot;artist&quot;, &quot;track&quot;, &quot;year&quot;, &quot;time&quot;)) %&gt;% select(song_id, date, week, rank) %&gt;% arrange(song_id, date) %&gt;% tbl_df rank %&gt;% sample_n(10) %&gt;% knitr::kable() song_id date week rank 253 2000-07-15 23 26 255 2000-03-11 21 4 126 2000-04-08 8 77 65 2000-09-23 1 97 131 1999-10-30 4 43 214 2000-10-28 8 10 156 2000-09-16 7 36 211 2000-12-02 13 45 157 2000-08-19 3 75 264 2000-07-01 2 66 5. Una misma unidad observacional está almacenada en múltiples tablas También es común que los valores sobre una misma unidad observacional estén separados en muchas tablas o archivos, es común que estas tablas esten divididas de acuerdo a una variable, de tal manera que cada archivo representa a una persona, año o ubicación. Para juntar los archivos hacemos lo siguiente: Leemos los archivos en una lista de tablas. Para cada tabla agregamos una columna que registra el nombre del archivo original. Combinamos las tablas en un solo data frame. Veamos un ejemplo, la carpeta specdata contiene 332 archivos csv que almacenan información de monitoreo de contaminación en 332 ubicaciones de EUA. Cada archivo contiene información de una unidad de monitoreo y el número de identificación del monitor es el nombre del archivo. Los pasos en R (usando el paquete plyr), primero creamos un vector con los nombres de los archivos en un directorio, aligiendo aquellos que contengan las letras “.csv”. paths &lt;- dir(&quot;datos/specdata&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) Después le asignamos el nombre del csv al nombre de cada elemento del vector. Este paso se realiza para preservar los nmobres de los archivos ya que estos los asignaremos a una variable mas adelante. names(paths) &lt;- basename(paths) La función map_df del paquete purrr itera sobre cada dirección, lee el csv en dicha dirección y los combina en un data frame. specdata_US &lt;- map_df(paths, read.csv, stringsAsFactors = FALSE) specdata &lt;- specdata_US %&gt;% mutate(monitor = extract_numeric(ID), date = as.Date(Date)) %&gt;% select(id = ID, monitor, date, sulfate, nitrate) glimpse(specdata) #&gt; Observations: 772,087 #&gt; Variables: 5 #&gt; $ id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... #&gt; $ monitor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... #&gt; $ date &lt;date&gt; 2003-01-01, 2003-01-02, 2003-01-03, 2003-01-04, 2003-... #&gt; $ sulfate &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... #&gt; $ nitrate &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... 6. Otras consideraciones En las buenas prácticas es importante tomar en cuenta los siguientes puntos: Incluir un encabezado con el nombre de las variables. Los nombres de las variables deben ser entendibles (e.g. age_at_diagnosis es mejor que AgeDx). En general los datos se deben guardar en un archivo por tabla. Escribir un script con las modificaciones que se hicieron a los datos crudos (reproducibilidad). Otros aspectos importantes en la limpieza de datos son: selección del tipo de variables (por ejemplo fechas), datos faltantes, typos y detección de valores atípicos. 3.3 Separa-aplica-combina Muchos problemas de análisis de datos involucran la aplicación de la estrategia split-apply-combine de Hadley Whickam, 2011. Esto se traduce en realizar filtros, cálculos y agregación de datos. Split-apply-combine Separa la base de datos original. Aplica funciones a cada subconjunto. Combina los resultados en una nueva base de datos. Consiste en romper un problema en pedazos (de acuerdo a una variable de interés), operar sobre cada subconjunto de manera independiente (calcular la media de cada grupo) y después unir los pedazos nuevamente. Cuando pensamos como implementar la estrategia divide-aplica-combina es natural pensar en iteraciones para recorrer cada grupo de interés y aplicar las funciones. Para esto usaremos la librería dplyr que contiene funciones que facilitan la implementación de la estrategia. Son importantes las siguientes funciones de la librería dplyr: filter: obtiene un subconjunto de las filas de acuerdo a una condición. select: selecciona columnas de acuerdo al nombre. arrange: re ordena las filas. mutate: agrega nuevas variables. summarise: reduce variables a valores (crear nuevas bases de datos). Para mostrar las funciones se usará el siguiente dataframe. df_ej &lt;- data.frame(genero = c(&quot;mujer&quot;, &quot;hombre&quot;, &quot;mujer&quot;, &quot;mujer&quot;, &quot;hombre&quot;), estatura = c(1.65, 1.80, 1.70, 1.60, 1.67)) df_ej %&gt;% knitr::kable() genero estatura mujer 1.65 hombre 1.80 mujer 1.70 mujer 1.60 hombre 1.67 Filtrar Filtrar una base de datos dependiendo de una condición requiere la función filter() que tiene los siguientes argumentos dplyr::filter(data, condition). df_ej %&gt;% filter(genero == &quot;mujer&quot;) #&gt; genero estatura #&gt; 1 mujer 1.65 #&gt; 2 mujer 1.70 #&gt; 3 mujer 1.60 Seleccionar Elegir columnas de un conjunto de datos se puede hacer con la función select() que tiene los siguientes argumentos dplyr::select(data, seq_variables). df_ej %&gt;% select(genero) #&gt; genero #&gt; 1 mujer #&gt; 2 hombre #&gt; 3 mujer #&gt; 4 mujer #&gt; 5 hombre También, existen funciones que se usan exclusivamente en select(): starts_with(x, ignore.case = TRUE): los nombres empiezan con x. ends_with(x, ignore.case = TRUE): los nombres terminan con x. contains(x, ignore.case = TRUE): selecciona las variable que contengan x. matches(x, ignore.case = TRUE): selecciona las variable que igualen la expresión regular x. num_range(&quot;x&quot;, 1:5, width = 2): selecciona las variables (numéricamente) de x01 a x05. one_of(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;): selecciona las variables que estén en un vector de caracteres. everything(): selecciona todas las variables. Por ejemplo: df_ej %&gt;% select(starts_with(&quot;g&quot;)) #&gt; genero #&gt; 1 mujer #&gt; 2 hombre #&gt; 3 mujer #&gt; 4 mujer #&gt; 5 hombre Arreglar Arreglar u ordenar de acuerdo al valor de una o más variables es posible con la función arrange() que tiene los siguientes argumentos dplyr::arrange(data, variables_por_las_que_ordenar). La función desc() permite que se ordene de forma descendiente. df_ej %&gt;% arrange(desc(estatura)) #&gt; genero estatura #&gt; 1 hombre 1.80 #&gt; 2 mujer 1.70 #&gt; 3 hombre 1.67 #&gt; 4 mujer 1.65 #&gt; 5 mujer 1.60 Mutar Mutar consiste en crear nuevas variables con la función mutate() que tiene los siguientes argumentos dplyr::mutate(data, nuevas_variables = operaciones): df_ej %&gt;% mutate(estatura_cm = estatura * 100) #&gt; genero estatura estatura_cm #&gt; 1 mujer 1.65 165 #&gt; 2 hombre 1.80 180 #&gt; 3 mujer 1.70 170 #&gt; 4 mujer 1.60 160 #&gt; 5 hombre 1.67 167 Resumir Los resúmenes permiten crear nuevas bases de datos que son agregaciones de los datos originales. La función summarise() permite realizar este resumendplyr::summarise(data, nuevas_variables = operaciones): df_ej %&gt;% dplyr::summarise(promedio = mean(estatura)) #&gt; promedio #&gt; 1 1.68 También es posible hacer resúmenes agrupando por variables determinadas de la base de datos. Pero, primero es necesario crear una base agrupada con la función group_by() con argumentos dplyr::group_by(data, add = variables_por_agrupar): df_ej %&gt;% group_by(genero) #&gt; # A tibble: 5 x 2 #&gt; # Groups: genero [2] #&gt; genero estatura #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.80 #&gt; 3 mujer 1.70 #&gt; 4 mujer 1.60 #&gt; 5 hombre 1.67 Después se opera sobre cada grupo, creando un resumen a nivel grupo y uniendo los subconjuntos en una base nueva: df_ej %&gt;% group_by(genero) %&gt;% dplyr::summarise(promedio = mean(estatura)) #&gt; # A tibble: 2 x 2 #&gt; genero promedio #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 hombre 1.74 #&gt; 2 mujer 1.65 3.4 Muertes por armas de fuego en EUA Los datos que vamos a utilizar provienen principalmente de la base de datos de causas múltiples de la muerte de los Centros para el Control y Prevención de la Enfermedad (CDCs) de Estados Unidos, de certificados de defunción de los 50 estados. Se considera que esta fuente de información es la base de datos más completa de muertes por armas de fuego. Para más información puedes leer el artículo: https://fivethirtyeight.com/features/gun-deaths/ Comencemos leyendo los datos para los años 2012, 2013 y 2014: guns_12 &lt;- read_csv(&quot;datos/guns_12.csv&quot;, na = &quot;&quot;) guns_13 &lt;- read_csv(&quot;datos/guns_13.csv&quot;, na = &quot;&quot;) guns_14 &lt;- read_csv(&quot;datos/guns_14.csv&quot;, na = &quot;&quot;) Las tres tablas tienen las mismas variables en el mismo orden. Examinemos la tabla para el año 2012: glimpse(guns_12) #&gt; Observations: 33,096 #&gt; Variables: 40 #&gt; $ res_status &lt;int&gt; 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ... #&gt; $ education_89 &lt;chr&gt; &quot;16&quot;, &quot;13&quot;, &quot;16&quot;, &quot;17&quot;, &quot;12&quot;, &quot;10&quot;, &quot;12&quot;, &quot;12... #&gt; $ education_03 &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ education_flag &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... #&gt; $ month &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;03... #&gt; $ sex &lt;chr&gt; &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, ... #&gt; $ detail_age &lt;int&gt; 1034, 1021, 1060, 1064, 1031, 1017, 1048, 104... #&gt; $ age_flag &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ age_recode &lt;int&gt; 32, 30, 38, 38, 32, 29, 35, 34, 36, 32, 30, 3... #&gt; $ age_recode2 &lt;chr&gt; &quot;12&quot;, &quot;10&quot;, &quot;18&quot;, &quot;18&quot;, &quot;12&quot;, &quot;09&quot;, &quot;15&quot;, &quot;14... #&gt; $ age_group &lt;chr&gt; &quot;05&quot;, &quot;04&quot;, &quot;08&quot;, &quot;08&quot;, &quot;05&quot;, &quot;04&quot;, &quot;07&quot;, &quot;06... #&gt; $ age_infant &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ death_place &lt;int&gt; 4, 7, 7, 4, 7, 4, 1, 7, 7, 4, 4, 4, 2, 4, 4, ... #&gt; $ marital &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;M&quot;, &quot;S&quot;, &quot;U&quot;, &quot;S&quot;, &quot;W&quot;, &quot;M&quot;, &quot;M&quot;, ... #&gt; $ day_of_week &lt;int&gt; 6, 4, 7, 7, 1, 7, 5, 1, 7, 5, 1, 5, 4, 4, 5, ... #&gt; $ data_year &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 201... #&gt; $ at_work &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, ... #&gt; $ death_manner &lt;chr&gt; &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;5&quot;, &quot;2&quot;, &quot;1&quot;, ... #&gt; $ burial &lt;chr&gt; &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, ... #&gt; $ autopsy &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, ... #&gt; $ activity &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ... #&gt; $ injury_place &lt;int&gt; 0, 4, 8, 0, 8, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, ... #&gt; $ underlying_cause &lt;chr&gt; &quot;X74&quot;, &quot;X74&quot;, &quot;X72&quot;, &quot;X74&quot;, &quot;X72&quot;, &quot;X73&quot;, &quot;Y2... #&gt; $ cause_recode358 &lt;int&gt; 429, 429, 429, 429, 429, 429, 446, 429, 407, ... #&gt; $ cause_recode113 &lt;int&gt; 125, 125, 125, 125, 125, 125, 132, 125, 119, ... #&gt; $ cause_recode130 &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ cause_recode39 &lt;int&gt; 40, 40, 40, 40, 40, 40, 42, 40, 39, 40, 40, 4... #&gt; $ race &lt;chr&gt; &quot;68&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;03&quot;, &quot;01&quot;, &quot;03... #&gt; $ race_bridged &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ race_flag &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA... #&gt; $ race_recode &lt;int&gt; 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, ... #&gt; $ race_recode2 &lt;int&gt; 4, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 3, ... #&gt; $ hispanic &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, ... #&gt; $ hispanic_recode &lt;int&gt; 8, 6, 6, 6, 6, 8, 6, 8, 6, 6, 8, 6, 8, 6, 8, ... #&gt; $ intent &lt;chr&gt; &quot;Suicide&quot;, &quot;Suicide&quot;, &quot;Suicide&quot;, &quot;Suicide&quot;, &quot;... #&gt; $ police &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... #&gt; $ weapon &lt;chr&gt; &quot;Other/unknown&quot;, &quot;Other/unknown&quot;, &quot;Handgun&quot;, ... #&gt; $ year &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 201... #&gt; $ age &lt;int&gt; 34, 21, 60, 64, 31, 17, 48, 41, 50, 30, 21, 4... #&gt; $ place &lt;chr&gt; &quot;Home&quot;, &quot;Street&quot;, &quot;Other specified&quot;, &quot;Home&quot;, ... Para pegar las tablas para los 3 años vamos a utilizar la función bind_rows() del paquete dplyr: guns &lt;- guns_12 %&gt;% bind_rows(guns_13) %&gt;% bind_rows(guns_14) Veamos otro ejemplo de cómo recodificar variables categóricas, en este caso para la variable de nivel educativo: guns &lt;- guns %&gt;% mutate(education = ifelse(education_flag == 1, cut(as.numeric(education_03), breaks = c(0, 2, 3, 5, 8, 9, labels = c(&quot;Less than HS&quot;, &quot;HS/GED&quot;, &quot;Some college&quot;, &quot;BA+&quot;, NA))), cut(as.numeric(education_89), breaks = c(0, 11, 12, 15, 17, 99), labels = c(&quot;Less than HS&quot;, &quot;HS/GED&quot;, &quot;Some college&quot;, &quot;BA+&quot;, NA)))) Otro ejemplo, para la variable de raza: guns &lt;- guns %&gt;% mutate(race = as.integer(race), race = ifelse(hispanic &gt; 199 &amp; hispanic &lt;996, &quot;Hispanic&quot;, ifelse(race == &quot;01&quot;, &quot;White&quot;, ifelse(race == &quot;02&quot;, &quot;Black&quot;, ifelse(as.numeric(race) &gt;= 4 &amp; as.numeric(race) &lt;= 78, &quot;Asian/Pacific Islander&quot;,&quot;Native American/Native Alaskan&quot;)))), race = ifelse(is.na(race), &quot;Unknown&quot;, race)) Para quedarnos con las variables con las que vamos a trabajar utilizamos la función select(): guns &lt;- guns %&gt;% select(year, month, intent, police, sex, age, race, hispanic, place, education) Veamos de nuevo cómo es la estructura de la tabla: str(guns) #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 99396 obs. of 10 variables: #&gt; $ year : int 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ... #&gt; $ month : chr &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;02&quot; ... #&gt; $ intent : chr &quot;Suicide&quot; &quot;Suicide&quot; &quot;Suicide&quot; &quot;Suicide&quot; ... #&gt; $ police : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ sex : chr &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; ... #&gt; $ age : int 34 21 60 64 31 17 48 41 50 30 ... #&gt; $ race : chr &quot;Asian/Pacific Islander&quot; &quot;Native American/Native Alaskan&quot; &quot;Native American/Native Alaskan&quot; &quot;Native American/Native Alaskan&quot; ... #&gt; $ hispanic : int 100 100 100 100 100 100 100 100 100 100 ... #&gt; $ place : chr &quot;Home&quot; &quot;Street&quot; &quot;Other specified&quot; &quot;Home&quot; ... #&gt; $ education: int 4 3 4 4 2 1 2 2 3 3 ... Supongamos que nos interesa analizar el número de suicidios por arma de fuego para cada uno de los tres años. Esto quiere decir que es necesario agrupar y usar una función de resumen: guns %&gt;% filter(intent == &quot;Suicide&quot;) %&gt;% group_by(year) %&gt;% summarize(suicides = n()) #&gt; # A tibble: 3 x 2 #&gt; year suicides #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2012 20663 #&gt; 2 2013 21172 #&gt; 3 2014 21333 Supongamos que deseamos filtar (quitar las observaciones) de homicidios para los cuales se tiene la categoría de “Other”, para ello utilizamos la función filter(): guns_sin_especificar &lt;- guns %&gt;% filter(place != &quot;Other unspecified&quot; &amp; place != &quot;Other specified&quot;) Podemos analizar la siguiente gráfica de mosaico: ggplot(guns_sin_especificar, aes(x=as.factor(place), fill=as.factor(intent))) + geom_bar(position=&#39;fill&#39;) + coord_flip() + theme(aspect.ratio = 1,legend.position=&quot;bottom&quot;, axis.text.y=element_text(color=&#39;black&#39;,size=10), axis.text.x=element_text(color=&#39;black&#39;,size=10), axis.title.x=element_text(size=10), axis.title.y=element_text(size=10), legend.text=element_text(size=10)) + scale_fill_discrete(&quot;&quot;) + ylab(&#39;Proporción&#39;) + xlab(&quot;Lugar&quot;) + ggtitle(&quot;Lugar de homicidios por intención&quot;) Se podría concluir, por ejemplo, que si un homicidio ocurrió en una granja, entonces lo más probable es que haya sido un suicidio. 3.5 El Cuarteto de Anscombe “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey En 1971 un estadístico llamado Frank Anscombe (fundador del departamento de Estadística de la Universidad de Yale) encontró cuatro conjuntos de datos (I, II, III y IV). Cada uno consiste de 11 observaciones y tienen las mismas propiedades estadísticas. anscombe \\(x_1\\) \\(y_1\\) \\(x_2\\) \\(y_2\\) \\(x_3\\) \\(y_3\\) \\(x_4\\) \\(y_4\\) 10.0 8.04 10.0 9.14 10.0 7.46 8.0 6.58 8.0 6.95 8.0 8.14 8.0 6.77 8.0 5.76 13.0 7.58 13.0 8.74 13.0 12.74 8.0 7.71 9.0 8.81 9.0 8.77 9.0 7.11 8.0 8.84 11.0 8.33 11.0 9.26 11.0 7.81 8.0 8.47 14.0 9.96 14.0 8.10 14.0 8.84 8.0 7.04 6.0 7.24 6.0 6.13 6.0 6.08 8.0 5.25 4.0 4.26 4.0 3.10 4.0 5.39 19.0 12.50 12.0 10.84 12.0 9.13 12.0 8.15 8.0 5.56 7.0 4.82 7.0 7.26 7.0 6.42 8.0 7.91 5.0 5.68 5.0 4.74 5.0 5.73 8.0 6.89 Por ejemplo, todos los conjuntos de datos I, II, III, y IV, tienen exactamente misma media de \\(x\\), \\(\\bar{x}_i = \\bar{x}_j\\), y misma media de \\(y\\), \\(\\bar{y}_i = \\bar{y}_j\\) para toda \\(i,j=1,2,3,4\\). Además, se puede ver que todos tienen misma varianza muestral de \\(x\\) y de \\(y\\). En cada conjunto de datos la correlación entre \\(x\\) y \\(y\\) es la misma, y por consiguiente, los coeficientes de la regresión lineal \\(\\beta_0\\) y \\(\\beta_1\\) también son iguales. Propiedad Valor Media de \\(x\\) 9 Varianza muestral de \\(x\\) 11 Media de \\(y\\) 7.50 Varianza muestral de \\(y\\) 4.12 Correlación entre \\(x\\) y \\(y\\) 0.816 Línea de regresión lineal \\(y = 3.00 + 0.500x\\) ¿En qué son diferentes estos conjuntos de datos? ¿Es posible con la información anterior concluir que los cuatro conjuntos de datos deben ser similares? ¿Que tengan estadísticas similares asegura que provienen de un mismo modelo? Cuando analizamos los datos de manera gráfica en un histograma encontramos rápidamente que los conjuntos de datos son muy distintos. “Una imagen dice más que mil palabras.” En la gráfica del primer conjunto de datos, se ven datos como los que se tendrían en una relación lineal simple con un modelo que cumple los supuestos de normalidad. La segunda gráfica (la de arriba a la derecha) muestra unos datos que tienen una asociación pero definitivamente no es lineal y el coeficiente de correlación no es relevante en este caso. En la tercera gráfica (abajo a la izquierda) están puntos alineados perfectamente en una línea recta, excepto por uno de ellos. En la última gráfica podemos ver un ejemplo en el cual basta tener una observación atípica para que se produzca un coeficiente de correlación alto aún cuando en realidad no existe una asociación lineal entre las dos variables. Edward Tufte usó el cuarteto en la primera página del primer capítulo de su libro The Visual Display of Quantitative Information, para enfatizar la importancia de mirar los datos antes de analizarlos. (Tufte and Graves-Morris 2014) 3.6 The Grammar of Graphics de Leland Wilkinson Una ventaje de ggplot es que implementa una gramática de gráficas de forma organizada y con sentido orientada a esta forma de asociar variables con geometrías (Wilkinson 2005). En lugar de tener una lista enorme y conceptualmente plana de opciones para hacer gráficas, ggplot parte en varios pasos el procedimiento para realizar una gráfica: primero, se debe proporcionar información a la función sobre qué datos y qué variables se van a utilizar. segundo, se debe vincular las variables que se van a utilizar en la gráfica con las características específicas que se requiere tener en la gráfica. tercero, se debe elegir una función geom_ para indicar qué tipo de gráfica se dibujará, un diagrama de dispersión, una gráfica de barras o un diagrama de caja. En general, según Leland Wilkinson, hay dos principios generales que se deben seguir: La geometría utilizada debe coincidir con los datos que se están visualizando. La geometría utilizada debe ser fácil de interpretar. 3.7 ggplot Vamos a ver cómo visualizar los datos usando ggplot2. R tiene varios sistemas para hacer gráficas, pero ggplot2 es uno de los más elegantes y versátiles. ggplot2 implementa la gramática de gráficas, un sistema consistente para describir y construir gráficas. Con ggplot2, pueden hacerse cosas más rápido, aprendiendo un único sistema consistente, y aplicándolo de muchas formas. Para mayor información sobre los fundamentos teóricos de ggplot2 se recomienda leer el artículo titulado “The Layered Grammar of Graphics”, visitando la siguiente liga: http://vita.had.co.nz/papers/layered-grammar.pdf. Lo más importante para entender ggplot es comprender la estructura y la lógica para hacer una gráfica. El código debe decir cuáles son las conexiones entre las variables en los datos y los elementos de la gráfica tal como los vamos a ver en la pantalla, los puntos, los colores y las formas. En ggplot, estas conexiones lógicas entre los datos y los elementos de la gráfica se denominan asignaciones estéticas o simplemente estéticas. Se comienza una gráfica indicando a ggplot cuáles son los datos, qué variables en los datos se van a usar y luego cómo las variables en estos datos se mapean lógicamente en la estética de la gráfica. Luego, toma el resultado y se indica qué tipo de gráfica se desea, por ejemplo, un diagrama de dispersión, una gráfica de barras, o una gráfica de línea. En ggplot este tipo general de gráficas se llama geom. Cada geom tiene una función que lo crea. Por ejemplo, geom_point() hace diagramas de dispersión, geom_bar() hace gráficas de barras, geom_line() hace gráficas de línea, y así sucesivamente. Para combinar estas dos piezas, el objeto ggplot() y el geom se suman literalmente en una expresión, utilizando el símbolo “+”. ¿Qué geometrías son más adecuadas para cada tipo de variable? Usaremos los datos de gapminder para hacer nuestras primeras gráficas. Vamos a asegurarnos de que la biblioteca que contiene los datos esté cargada: library(gapminder) Esto hace que una tabla de datos esté disponible para su uso. Para ver un pedazo de la tabla utilizamos la función glimpse(): library(tidyverse) glimpse(gapminder) #&gt; Observations: 1,704 #&gt; Variables: 6 #&gt; $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, ... #&gt; $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia... #&gt; $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992... #&gt; $ lifeExp &lt;dbl&gt; 28.8, 30.3, 32.0, 34.0, 36.1, 38.4, 39.9, 40.8, 41.7... #&gt; $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 1488... #&gt; $ gdpPercap &lt;dbl&gt; 779, 821, 853, 836, 740, 786, 978, 852, 649, 635, 72... Supongamos que queremos graficar la esperanza de vida vs el PIB per cápita para todos los años y países en los datos. Haremos esto creando un objeto que contenga parte de la información necesario y a partir de ahí vamos a construir nuestra gráfica. Primero debemos indicarle a la función ggplot() qué datos estamos utilizando: p &lt;- ggplot(data = gapminder) p En este punto, ggplot sabe cuáles son nuestros datos, pero no cuál es el mapeo, es decir, qué variables de los datos deben correlacionarse con qué elementos visuales de la trama. Tampoco sabe qué tipo de trama queremos. En ggplot, las asignaciones se especifican utilizando la función aes(). Me gusta esta: Hasta este punto ggplot conoce qué datos se van a utilizar para hacer la gráfico, pero no el mapeo o asociación de qué variables se van a relacionar con los elementos visuales de la gráfica. Tampoco se sabe qué tipo de gráfica se va a hacer. En ggplot, las asignaciones se especifican utilizando la función aes(): p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) El argumento mapping = aes(...) vincula variables a cosas que se van a ver en la gráfica. Los valores de \\(x\\) y \\(y\\) son los más obvios. Otras asignaciones estéticas pueden incluir, por ejemplo, el color, la forma, el tamaño y el tipo de línea (si una línea es sólida o discontinua, o algún otro patrón). Un mapeo no dice directamente qué formas o colores van a aparecer en la gráfica. Más bien, dicen qué variables en los datos serán representadas por los elementos visuales como color, forma o un punto. ¿Qué sucede si simplemente escribimos p en la consola y ejecutamos? p El objeto p ha sido creado por la función ggplot(), y ya tiene información sobre las asignaciones que queremos, junto con mucha otra información añadida por defecto. (Si quiere ver cuánta información hay en el objeto p, intente solicitar str(p)). Sin embargo, no le hemos dado ninguna instrucción acerca de qué tipo de diagrama dibujar. Necesitamos agregar una capa a la trama. Esto significa elegir una función geom_*. Usaremos geom_point(). Sabe cómo tomar valores xey y trazarlos en un diagrama de dispersión. Se ha creado el objeto p utilizando la función ggplot() y este objeto ya tiene información de las asignacionesque queremos. Sin embargo, no se le ha dado ninguna instrucción sobre qué tipo de gráfica se quiere dibujar. Necesitamos agregar una capa a la gráfica. Esto se hace mediante el símbolo +. Esto significa elegir una función geom_. Utilizaremos geom_point() para hacer un diagrama de dispersión. p + geom_point() El mapeo de las propiedades estéticas se denomina escalamiento y depende del tipo de variable, las variables discretas (por ejemplo, genero, escolaridad, país) se mapean a distintas escalas que las variables continuas (variables numéricas como edad, estatura, etc.), los defaults para algunos atributos son (estos se pueden modificar): aes Discreta Continua Color (color) Arcoiris de colores Gradiente de colores Tamaño (size) Escala discreta de tamaños Mapeo lineal entre el área y el valor Forma (shape) Distintas formas No aplica Transparencia (alpha) No aplica Mapeo lineal a la transparencia Los geoms controlan el tipo de gráfica: p + geom_smooth() Podemos ver de inmediato que algunos de estos geoms hacen mucho más que simplemente poner puntos en una cuadrícula. Aquí geom_smooth() ha calculado una línea suavizada y la región sombreada representa el error estándar de la línea suavizada. Si queremos ver los puntos de datos y la línea juntos, simplemente agregamos geom_point() de nuevo como una capa adicional utilizando +: p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp)) p + geom_point() + geom_smooth() El mensaje de la consola de R nos dice que la función geom_smooth() está utilizando un método llamado gam, que en este caso significa que se ajusta a un modelo aditivo generalizado. Esto sugiere que tal vez haya otros métodos en geom_smooth(). Podemos intentar agregar method = &quot;lm&quot; (para “modelo lineal”) como un argumento para geom_smooth(): p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp)) p + geom_point() + geom_smooth(method=&quot;lm&quot;) Se puede agregar al mapeo del color de la línea el continente y del relleno de los puntos (fill) también el continente para obtener una gráfica que nos dé una idea más general de como se tiene esta relación por continente. p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent)) p + geom_point(size = 0.5) + geom_smooth(method=&#39;loess&#39;) + scale_x_log10() 3.8 Un histograma de las muertes en Iraq Iraq Body Count (IBC) mantiene la base de datos pública más grande sobre muertes violentas de civiles desde la invasión en Iraq del 2003. Los datos de IBC provienen de informes de medios cruzados, de hospitales, morgue, ONG y cifras o registros oficiales. Para mayor información puedes visitar https://www.iraqbodycount.org/. Los datos los leemos con la función read_csv() de la librería readr: ibc &lt;- read_csv(&quot;datos/ibc-incidents-2016-8-8.csv&quot;) ibc %&gt;% sample_n(10) %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;, font_size = 4) IBC_code Start_Date End_Date Time Location Target Weapons Deaths_recorded Sources m2046 2-Sep-13 2-Sep-13 PM 13 Street, Al-Bayaa, southwest Baghdad employee at the Ministry of Education in front of his home gunfire 1 AIN 2 Sep, Sotaliraq 2 Sep d2329 9-Feb-07 9-Feb-07 NA near Al-Shimal Garage, Mosul NA explosive device 1 MO 09 Feb, WP 04 Apr (MoH) k7919 29-Oct-07 29-Oct-07 8:00-9:00 AM Ishbilliyah Square, central Baquba police recruits awaiting training suicide bomber 28 AFP 31 Oct, NINA 29 Oct, REU 29 Oct, DPA 29 Oct s1254 22-Apr-15 22-Apr-15 NA Al-Mansour, west Baghdad civilian in Al-Mansour drive-by shooting 1 NINA 22 Apr h0379 25-Feb-14 25-Feb-14 NA Saba’ Abkar, north Baghdad Mohamed Taha Mohamed, Iraqi Sports Channel Director car in hit-and-run 0 AIN 25 Feb, INN 25 Feb k18626 13-Mar-12 14-Mar-12 NA Hoswa, Karma, east of Falluja civilian car roadside bomb 1 AKnews 14 Mar, Al-Shorfa 13 Mar k1708 1-Aug-05 1-Aug-05 PM west Baghdad Shaikh Akil al-Ma’adhidi, a cleric from al-Muhajirin mosque, brother also killed gunfire 2 Al-Jaz 02 Aug, AFP 02 Aug k10200 15-Apr-08 15-Apr-08 NA al-Zahraa, east Mosul ‘operator for a private electricity generator’ gunfire 1 VOI 15 Apr m3013 2-Dec-13 2-Dec-13 AM Refaq, east Mosul civilian in his car magnetic bomb 1 AIN 2 Dec, NINA 2 Dec k3289e 18-Jun-06 18-Jun-06 NA Al-Sha’b, Baghdad bodies found shot, tortured gunfire, executed, tortured 1 Al-Shar 18 Jun, DPA 18 Jun Primero filtramos los incidentes en los que hubo al menos cinco fatalidades: ibc_fatalidades &lt;- ibc %&gt;% filter(Deaths_recorded &gt;= 5) Una forma fácil de dibujar un histograma es utilizando la geometría geom_histogram(): ggplot(ibc_fatalidades, aes(x=Deaths_recorded)) + geom_histogram() + scale_x_log10() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.9 Inglehart–Welzel: un mapa cultural del mundo Los teóricos de la modernización de Karl Marx a Daniel Bell han sostenido que el desarrollo económico trae cambios culturales penetrantes. Pero otros, desde Max Weber hasta Samuel Huntington, han afirmado que los valores culturales son una influencia duradera y autónoma sobre la sociedad. En un artículo de la ciencia política, los autores Inglehart y Welzel de la Universidad de Michigan, afirman que el desarrollo económico está vinculado a cambios sistemáticos en los valores culturales. Utilizando los datos de la encuesta de valores mundiales WVS (World Values Survey), crearon dos índices: uno que pone énfasis en valores tradicionales y otro que pone énfasis en valores de supervivencia. Características de valores tradicionales en una sociedad: fuerte sentimiento de orgullo nacional le da más importancia a que un niño aprenda obediencia y fé religiosa en lugar de independencia y determinación el aborta nunca es justificada fuerte sentido de orgullo nacional favorece más el respeto por la autoridad. Los valores seculares o racionales enfatizan lo opuesto. Características de valores de supervivencia en una sociedad: le da prioridad a la economía sobre la calidad de vida se describe como no muy feliz aún no ha firmado o jamás firmaría una petición la homosexualidad nuna es justificada se debe ser muy cuidadoso al confiar en las personas. Los valores de autoexpresión enfatizan lo opuesto. Ronald Inglehart en su artículo de 1971 The silent revolution in Europe. Intergenerational change in post-industrial societies. publicado en el American Political Science Review, propone una medida de los valores postmaterialistas de una sociedad. Esta medida se conoce como índice post-materialista de Inglehart (4-item) . La siguiente pregunta de la encuesta es el punto de partida para medir el materialismo o el post-materialismo: “Si tuvieras que elegir entre las siguientes cosas, ¿cuáles son las dos que te parecen más deseables?” Mantener el orden en la nación. Dando a la gente más voz en importantes decisiones políticas. La lucha contra el aumento de los precios. Proteger la libertad de expresión. La medida se basa entonces en la observación de que dos de las cuatro opciones, la primera y la tercera, se consideran como “preferencia hacia el valor adquisitivo en relación con la protección y adquisición de bienes”. Si se eligen las dos opciones postmaterialistas, entonces la puntuación es 3. Si se elige sólo una opción post-materialista, entonces la puntuación es 2, y de lo contrario es 1. Como todas las opciones podrían ser deseables, la medida se relaciona con la “prioridad relativa” de las elecciones materialistas sobre la segunda y cuarta y aborda las concesiones que típicamente conllevan las decisiones políticas. La conceptualización del postmaterialismo a lo largo de un continuo unidimensional está cerca del concepto de la “jerarquía de necesidades” propuesta por Maslow. library(tidyverse) factores_inglehart &lt;- read_csv(file = &quot;datos/factores_inglehart.csv&quot;) glimpse(factores_inglehart) #&gt; Observations: 60 #&gt; Variables: 6 #&gt; $ country_code &lt;int&gt; 112, 12, 152, 156, 158, 170, 196, 218,... #&gt; $ country &lt;chr&gt; &quot;Belarus&quot;, &quot;Algeria&quot;, &quot;Chile&quot;, &quot;China&quot;... #&gt; $ region &lt;chr&gt; &quot;Eastern Europe&quot;, &quot;Northern Africa&quot;, &quot;... #&gt; $ reg &lt;chr&gt; &quot;Europe &amp; Eurasia&quot;, &quot;Middle East &amp; Nor... #&gt; $ traditional_secular &lt;dbl&gt; 0.91766, -0.68003, 0.14525, 1.45307, 1... #&gt; $ survival_selfexpression &lt;dbl&gt; -0.3187, -0.3300, 1.5769, -0.5487, 0.9... 3.9.1 Creando un ggplot Para graficar factores_inglehart, ejecuta este código para poner survival_selfexpression en el eje x (eje horizontal) y traditional_secular en el eje y (eje vertical): ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular)) 3.9.2 Mapeos: Aesthetics “The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey En la gráfica de abajo, un grupo de puntos (en rojo) parece estar fuera de la tendencia lineal. Estos países tienen menores valores de supervivencia de lo que esperaríamos de acuerdo a sus mayores valores de tradicionalismo. Podemos formular la hipótesis de que se trata de países latinoamericanos. Una forma de probar esta hipótesis es con la variable reg. La variable reg del conjunto de datos factores_inglehart clasifica a los países de acuerdo a su región geográfica. Podemos agregar una tercera variable, como reg, a un diagrama de dispersión bidimensional asignándolo a un aesthetic o mapeo. Un mapeo es una propiedad visual de los objetos en la gráfica. Un mapeo incluye cosas como el tamaño, la forma o el color de los puntos. Puede mostrar un punto (como el que se muestra a continuación) de diferentes maneras cambiando los valores de sus propiedades de mapeos. Aquí cambiamos los niveles de tamaño, forma y color de un punto para hacer que el punto sea pequeño, triangular o azul: Podemos transmitir información sobre los datos mapeando los aesthetics en la gráfica a las variables del data frame. Por ejemplo, podemos asignar los colores de los puntos a la variable reg para revelar la región de cada país. ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular, color=reg)) Para asignar una característica a una variable, asociamos el nombre del mapeo al nombre de la variable dentro de aes(). ggplot2 asignará automáticamente un nivel único de dicha característica (o mapeo) a cada valor único de la variable, un proceso conocido como escalamiento. ggplot2 también agregará una leyenda que explique qué niveles corresponden a qué valores. También podríamos agregar etiquetas: 3.9.2.1 Objetos geométricos ¿En qué se parecen las siguiente dos gráficas? Ambas gráficas contienen la misma variable x, la misma variable y, y ambas describen los mismos datos. Pero las gráficas no son idénticas. Cada una utiliza un objeto visual diferente para representar los datos. En la sintaxis de ggplot2, decimos que usan diferentes geoms. Un geom es un objeto geométrico que una gráfica utiliza para representar a los datos. La gente a menudo describe las gráficas por el tipo de geometría que usa la gráfica. Por ejemplo, las gráficas de barras usan geometrías de barras, los gráficos de línea utilizan geoms de línea, los boxplots usan geoms de boxplot, y así sucesivamente. Los diagramas de dispersión rompen la tendencia; Utilizan la geometría de punto. La gráfica de la izquierda utiliza el punto geom, y la gráfica de la derecha utiliza el geom de smooth, una línea ajustada a los datos. Para hacer las gráficas mostradas arriba se puede utilizar el siguiente código. #izquierda ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular)) #derecha ggplot(data = factores_inglehart) + geom_smooth(mapping = aes(x = survival_selfexpression, y = traditional_secular), method = &quot;loess&quot;) Cada función geom en ggplot2 toma un argumento mapping. Sin embargo, no todas las propiedades de aesthetics funciona con cada geom. Podríamos cambiar la forma de un punto, pero no la “forma” de una línea. Por otro lado, podríamos establecer el tipo de línea de una línea. geom_smooth() dibujará una línea diferente, con un tipo de línea diferente, para cada valor único de la variable que se asigna al tipo de línea. ggplot(data = factores_inglehart) + geom_smooth(mapping = aes(x = survival_selfexpression, y = traditional_secular, linetype = reg), method = &quot;loess&quot;, se = F, span = 1) Aquí geom_smooth() separa los países en líneas basándose en su valor de reg (región geográfica). Podemos superponer las líneas encima de los datos sin procesar y luego coloreándolo todo de acuerdo a reg. Para mostrar varios geoms en la misma gráfica, agregamos varias funciones geom a ggplot(): ggplot(data = factores_inglehart) + geom_point(mapping = aes(x = survival_selfexpression, y = traditional_secular)) + geom_smooth(mapping = aes(x = survival_selfexpression, y = traditional_secular), method = &quot;loess&quot;) Este código genera la misma gráfica que el código anterior: ggplot(data = factores_inglehart, mapping = aes(x = survival_selfexpression, y = traditional_secular)) + geom_point() + geom_smooth(method = &quot;loess&quot;) Si colocan asignaciones en una función geom, ggplot2 las tratará como asignaciones locales para cada capa, de tal forma que usará estas asignaciones para extender o sobrescribir las asignaciones globales para esa capa solamente. Esto hace posible visualizar elementos diferentes en diferentes capas. ggplot(data = factores_inglehart, mapping = aes(x = survival_selfexpression, y = traditional_secular)) + geom_point(mapping = aes(color = reg)) + geom_smooth(method = &quot;loess&quot;) 3.10 Poniendo todo junto El Billboard Hot 100 es un ranking semanal publicado en Estados Unidos y es utilizado en la industria de la música como una medida del rendimiento de las canciones en ventas y en streaming en el país. Con el siguiente código podemos descargar los datos del Billboard semana a semana y obtener un conjunto de datos como el que se presenta a continuación: suppressPackageStartupMessages({ library(tidyverse) library(rvest) library(lubridate) }) extract_song_info &lt;- function(html_row) { node_primary &lt;- html_row %&gt;% html_node(css = &#39;.chart-row__primary&#39;) node_secondary &lt;- html_row %&gt;% html_node(css = &#39;.chart-row__secondary&#39;) song_features &lt;- c( &#39;.chart-row__history--rising&#39;, &#39;.chart-row__bullet&#39;, &#39;.chart-row__history--falling&#39;, &#39;.chart-row__award-indicator&#39;, &#39;.chart-row__new-indicator&#39;, &#39;.chart-row__history--steady&#39;) feat_search &lt;- map(.x = song_features, .f = function(y) { node_primary %&gt;% html_nodes(css = y) }) song_node &lt;- node_primary %&gt;% html_node(css = &#39;.chart-row__main-display&#39;) info_node &lt;- song_node %&gt;% html_node(css = &#39;.chart-row__container&#39;) song_name_node &lt;- info_node %&gt;% html_node(css = &#39;.chart-row__song&#39;) song_artist_node &lt;- info_node %&gt;% html_node(css = &#39;.chart-row__artist&#39;) song_stats &lt;- node_secondary %&gt;% html_node(css = &#39;.chart-row__stats&#39;) last_week &lt;- song_stats %&gt;% html_node(css = &#39;.chart-row__last-week .chart-row__value&#39;) %&gt;% html_text() peak_position &lt;- song_stats %&gt;% html_node(css = &#39;.chart-row__top-spot .chart-row__value&#39;) %&gt;% html_text() wks_on_chart &lt;- song_stats %&gt;% html_node(css = &#39;.chart-row__weeks-on-chart .chart-row__value&#39;) %&gt;% html_text() current_week_rank &lt;- node_primary %&gt;% html_node(css = &#39;.chart-row__current-week&#39;) %&gt;% html_text() artist &lt;- song_artist_node %&gt;% html_text() %&gt;% str_replace_all(&#39;\\n&#39;,&#39;&#39;) name &lt;- song_name_node %&gt;% html_text() %&gt;% str_replace_all(&#39;\\n&#39;,&#39;&#39;) song &lt;- tibble(current_week_rank = as.character(current_week_rank), name = str_trim(as.character(name)), artist = str_trim(as.character(artist)), rising = length(feat_search[[1]]) &gt; 0, steady = length(feat_search[[6]]) &gt; 0, falling = length(feat_search[[3]]) &gt; 0, gains_performance = length(feat_search[[2]]) &gt; 0, award = length(feat_search[[4]]) &gt; 0, hot_debut = length(feat_search[[5]]) &gt; 0, last_week = as.character(last_week), peak_position = as.character(peak_position), wks_on_chart = as.character(wks_on_chart) ) cat(sprintf(&#39;%-3s\\t %-40s\\t%s\\n&#39;, song$current_week_rank, song$artist, song$name)) song } billboard_weekchart &lt;- function(fecha){ cat(sprintf(&#39;\\n\\n Fecha: \\t%s\\n\\n&#39;, toString(fecha))) # url base del Billboard Hot 100 base_url &lt;- &quot;http://www.billboard.com/charts/hot-100/&quot; current_url &lt;- paste0(base_url, fecha) webpage &lt;- tryCatch( { Sys.sleep(5) read_html(current_url); }, error=function(cond) { message(&quot;Error: Webpage did not respond succesfully.&quot;) message(cond) cat(&#39;\\n\\n&#39;) return(NA) }, finally={ cat(&#39;\\n&#39;) } ) if(length(webpage) &gt; 1){ chart &lt;- html_nodes(webpage, css=&#39;.chart-data&#39;) rows &lt;-html_nodes(chart, css=&#39;.chart-row&#39;) week_songs &lt;- map_df(.x = rows, .f = extract_song_info) fecha_df &lt;- data.frame(fecha=rep(fecha,nrow(week_songs))) week_songs &lt;- cbind(fecha_df, week_songs) }else{ write(toString(fecha), file=&quot;/home/andreu/scripts/billboard/missing.txt&quot;, append = T) Sys.sleep(60) week_songs &lt;- NULL } if(length(week_songs) &gt; 0){ write_csv(x = week_songs, path = paste0(&quot;/home/andreu/scripts/billboard/data/&quot;,fecha,&quot;.csv&quot;)) val = T }else{ val = F } val } billboard_alltime &lt;- function(start_date = ymd(&#39;1958-08-04&#39;), current_date = ymd(&#39;2018-02-03&#39;)){ file.create(&quot;/home/andreu/scripts/billboard/missing.txt&quot;) fechas &lt;- seq(start_date, current_date, by = &#39;1 week&#39;) alltime_songs &lt;- map_lgl(.x = fechas, .f = billboard_weekchart) alltime_songs } alltime_songs &lt;- billboard_alltime(start_date = ymd(&#39;2018-02-03&#39;)) paths &lt;- dir(&quot;/home/andreu/scripts/billboard/data/&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) billboard &lt;- map_df(paths, read_csv, col_types=paste(rep(&#39;c&#39;, 13), collapse=&#39;&#39;)) No es necesario entender a profundidad el código utilizado para parsear el html de cada página de Billboard, sin embargo, es interesante ver el uso de las funciones map, map_lgl, y map_df en el fragmento de código anteior. La función map se utiliza en la llamada a la primera función extract_song_info. Primero se definen tags del html que se necesitan parsear y posteriormente se utiliza map para buscar cada uno dentro del texto plano en html. La función map_df se utiliza para aplicar cada elemento (rengón) del html en una fecha dada la función extract_song_info y el resultado de aplicar esta función a cada renglón del html da como resultado un data frame con los datos de la información de cada canción para cada fecha. Posteriormente, este conjunto de datos se guarda en formato csv. La función map_lgl aplica la función billboard_weekchart a cada elemento de la lista de fecha y regresa un vector lógico que indica si el parseo del html tuvo éxito para cada una de las fechas. La función map_df al final del fragmento de código lee todos los archivos correspondientes a las fechas utilizadas y como resultado junta todos los data frames en uno solo. Una muestra de los datos obtenidos se puede ver en la siguiente tabla: billboard %&gt;% sample_n(10) %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(bootstrap_options = &quot;striped&quot;, font_size = 4) fecha current_week_rank name artist rising steady falling gains_performance award hot_debut last_week peak_position wks_on_chart 2010-06-21 23 Undo It Carrie Underwood TRUE FALSE FALSE TRUE FALSE FALSE 25 23 7 1996-11-18 59 Knocks Me Off My Feet Donell Jones TRUE FALSE FALSE TRUE FALSE FALSE 67 59 4 2007-04-23 19 Go Getta Young Jeezy Featuring R. Kelly FALSE FALSE TRUE FALSE FALSE FALSE 18 18 13 1996-07-01 13 Twisted Keith Sweat TRUE FALSE FALSE TRUE FALSE FALSE 21 13 3 1968-04-22 41 She’s Lookin’ Good Wilson Pickett TRUE FALSE FALSE FALSE FALSE FALSE 45 41 3 1980-04-21 18 Hurt So Bad Linda Ronstadt TRUE FALSE FALSE FALSE FALSE FALSE 23 18 3 1965-12-27 3 I Got You (I Feel Good) James Brown And The Famous Flames FALSE TRUE FALSE FALSE FALSE FALSE 3 3 8 2006-01-16 45 Heard ’Em Say Kanye West Featuring Adam Levine FALSE FALSE TRUE FALSE FALSE FALSE 40 26 13 1967-04-24 8 The Happening The Supremes TRUE FALSE FALSE FALSE FALSE FALSE 11 8 4 1980-04-14 75 Steal Away Robbie Dupree TRUE FALSE FALSE FALSE FALSE FALSE 85 75 2 Veamos de qué tipos son cada una de las columnas en los datos. Podemos usar nuevamente la función glimpse: glimpse(billboard) #&gt; Observations: 310,500 #&gt; Variables: 13 #&gt; $ fecha &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08... #&gt; $ current_week_rank &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... #&gt; $ name &lt;chr&gt; &quot;Poor Little Fool&quot;, &quot;Patricia&quot;, &quot;Splish Spla... #&gt; $ artist &lt;chr&gt; &quot;Ricky Nelson&quot;, &quot;Perez Prado And His Orchest... #&gt; $ rising &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ steady &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ falling &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ gains_performance &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ award &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ hot_debut &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA... #&gt; $ last_week &lt;chr&gt; &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;--&quot;, &quot;-... #&gt; $ peak_position &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... #&gt; $ wks_on_chart &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... Supongamos que deseamos hacer una gráfica de barras del número de semanas en el top 10 para los 20 artistas que más semanas han permanecido en el top 10. artistas_top10 &lt;- billboard %&gt;% filter(current_week_rank &gt;= 10) %&gt;% group_by(artist) %&gt;% summarise(num_semanas_top_10 = n()) %&gt;% arrange(desc(num_semanas_top_10)) %&gt;% top_n(20, wt = num_semanas_top_10) Para hacer la gráfica con ggplot debemos primero ordenar los artistas de manera descendente por el número de semanas en el top 10. Para esto utilizamos la función de fct_reorder del paquete forcats. artistas_top10$artist &lt;- forcats::fct_reorder(f = artistas_top10$artist, x = artistas_top10$num_semanas_top_10, .desc = T) Por último, hacemos la gráfica indicando a geom_bar que la transformación estadística que debe usar es la de identity, es decir, la longitud de la barra corresponde al valor absoluto de la variable num_semanas_top_10: ggplot(artistas_top10, aes(x = artist, y = num_semanas_top_10)) + geom_bar(stat = &#39;identity&#39;) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) ??fct_reorder 3.11 Tarea Utiliza los datos del Billboard Hot 100 para contestar las siguientes preguntas: ¿Cuáles son los 10 artistas que han tenido más #1’s en la historia de Billboard? ¿Cuáles son los 10 artistas que han tenido más #1’s en los últimos 10 años? Realiza una gráfica de barras para responder a la pregunta. ¿Cómo se ha comportado el número promedio de semanas que una canción permanece en el #1 a través de la historia? ¿Existe alguna relación entre el número de presentaciones en vivo (gains_performance) y el número de semanas que permance una canción en #1? Para algunos últimos singles que han permanecido más semanas en el Hot 100 en los últimos meses realiza una gráfica de su posición en el tiempo semana a semana. Referencias "],
["referencias.html", "Referencias", " Referencias "]
]
