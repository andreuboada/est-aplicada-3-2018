<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Estadística Aplicada III</title>
  <meta name="description" content="Notas y material para el curso de Estadística Aplicada III, 2018 (ITAM)">
  <meta name="generator" content="bookdown 0.7.10 and GitBook 2.6.7">

  <meta property="og:title" content="Estadística Aplicada III" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas y material para el curso de Estadística Aplicada III, 2018 (ITAM)" />
  <meta name="github-repo" content="andreuboada/est-aplicada-3-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Estadística Aplicada III" />
  
  <meta name="twitter:description" content="Notas y material para el curso de Estadística Aplicada III, 2018 (ITAM)" />
  

<meta name="author" content="Andreu Boada de Atela">


<meta name="date" content="2018-05-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="analisis-de-discriminante-lineal-1.html">
<link rel="next" href="componentes-principales-1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Estadística Aplicada III</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario y referencias</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#evaluacion"><i class="fa fa-check"></i>Evaluación</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-r-y-rstudio"><i class="fa fa-check"></i>Software: R y Rstudio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias-principales"><i class="fa fa-check"></i>Referencias principales</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#otras-referencias"><i class="fa fa-check"></i>Otras referencias</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#tareas"><i class="fa fa-check"></i>Tareas</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#por-que-un-analisis-multivariado"><i class="fa fa-check"></i><b>1.1</b> ¿Por qué un análisis multivariado?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#la-paradoja-de-simpson"><i class="fa fa-check"></i><b>1.2</b> La paradoja de Simpson</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#modelos-log-lineales"><i class="fa fa-check"></i><b>1.3</b> Modelos log-lineales</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#interpretacion-de-parametros"><i class="fa fa-check"></i><b>1.4</b> Interpretación de parámetros</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#ejemplo-dos-monedas"><i class="fa fa-check"></i><b>1.4.1</b> Ejemplo: dos monedas</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#otros-ejemplos"><i class="fa fa-check"></i><b>1.5</b> Otros ejemplos</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#discriminacion-de-residentes-hispanos-con-discapacidades"><i class="fa fa-check"></i><b>1.5.1</b> Discriminación de residentes hispanos con discapacidades</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#consumo-de-chocolate-y-premios-nobel"><i class="fa fa-check"></i><b>1.5.2</b> Consumo de chocolate y premios Nobel</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#tarea"><i class="fa fa-check"></i><b>1.6</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Rintro.html"><a href="Rintro.html"><i class="fa fa-check"></i><b>2</b> Temas selectos de R</a><ul>
<li class="chapter" data-level="2.1" data-path="Rintro.html"><a href="Rintro.html#que-ventajas-tiene-r"><i class="fa fa-check"></i><b>2.1</b> ¿Qué ventajas tiene R?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="Rintro.html"><a href="Rintro.html#r-es-gratuito-y-de-codigo-abierto"><i class="fa fa-check"></i><b>2.1.1</b> R es gratuito y de código abierto</a></li>
<li class="chapter" data-level="2.1.2" data-path="Rintro.html"><a href="Rintro.html#r-tiene-una-comunidad-comprometida"><i class="fa fa-check"></i><b>2.1.2</b> R tiene una comunidad comprometida</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="Rintro.html"><a href="Rintro.html#flujo-basico-de-trabajo-para-el-analisis-de-datos-en-r."><i class="fa fa-check"></i><b>2.2</b> Flujo básico de trabajo para el análisis de datos en R.</a></li>
<li class="chapter" data-level="2.3" data-path="Rintro.html"><a href="Rintro.html#introduccion-a-r-como-lenguaje-de-programacion-y-la-plataforma-interactiva-de-rstudio."><i class="fa fa-check"></i><b>2.3</b> Introducción a R como lenguaje de programación, y la plataforma interactiva de RStudio.</a><ul>
<li class="chapter" data-level="2.3.1" data-path="Rintro.html"><a href="Rintro.html#como-entender-r"><i class="fa fa-check"></i><b>2.3.1</b> ¿Cómo entender R?</a></li>
<li class="chapter" data-level="2.3.2" data-path="Rintro.html"><a href="Rintro.html#por-que-r"><i class="fa fa-check"></i><b>2.3.2</b> ¿Por qué R?</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="Rintro.html"><a href="Rintro.html#estructuras-de-datos"><i class="fa fa-check"></i><b>2.4</b> Estructuras de datos</a><ul>
<li class="chapter" data-level="2.4.1" data-path="Rintro.html"><a href="Rintro.html#vectores"><i class="fa fa-check"></i><b>2.4.1</b> Vectores</a></li>
<li class="chapter" data-level="2.4.2" data-path="Rintro.html"><a href="Rintro.html#data-frames"><i class="fa fa-check"></i><b>2.4.2</b> Data Frames</a></li>
<li class="chapter" data-level="2.4.3" data-path="Rintro.html"><a href="Rintro.html#listas"><i class="fa fa-check"></i><b>2.4.3</b> Listas</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="Rintro.html"><a href="Rintro.html#r-markdown"><i class="fa fa-check"></i><b>2.5</b> R Markdown</a><ul>
<li class="chapter" data-level="2.5.1" data-path="Rintro.html"><a href="Rintro.html#que-es-r-markdown"><i class="fa fa-check"></i><b>2.5.1</b> ¿Qué es R Markdown?</a></li>
<li class="chapter" data-level="2.5.2" data-path="Rintro.html"><a href="Rintro.html#estructura-basica-de-r-markdown"><i class="fa fa-check"></i><b>2.5.2</b> Estructura básica de R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="Rintro.html"><a href="Rintro.html#proyectos-de-rstudio"><i class="fa fa-check"></i><b>2.6</b> Proyectos de RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="Rintro.html"><a href="Rintro.html#otros-aspectos-importantes-de-r"><i class="fa fa-check"></i><b>2.7</b> Otros aspectos importantes de R</a><ul>
<li class="chapter" data-level="2.7.1" data-path="Rintro.html"><a href="Rintro.html#valores-faltantes"><i class="fa fa-check"></i><b>2.7.1</b> Valores faltantes</a></li>
<li class="chapter" data-level="2.7.2" data-path="Rintro.html"><a href="Rintro.html#funciones"><i class="fa fa-check"></i><b>2.7.2</b> Funciones</a></li>
<li class="chapter" data-level="2.7.3" data-path="Rintro.html"><a href="Rintro.html#funcionales"><i class="fa fa-check"></i><b>2.7.3</b> Funcionales</a></li>
<li class="chapter" data-level="2.7.4" data-path="Rintro.html"><a href="Rintro.html#rendimiento-en-r"><i class="fa fa-check"></i><b>2.7.4</b> Rendimiento en R</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="Rintro.html"><a href="Rintro.html#tarea-1"><i class="fa fa-check"></i><b>2.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html"><i class="fa fa-check"></i><b>3</b> Manipulación y visualización de datos</a><ul>
<li class="chapter" data-level="3.1" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#el-principio-de-datos-limpios"><i class="fa fa-check"></i><b>3.1</b> El principio de datos limpios</a></li>
<li class="chapter" data-level="3.2" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#limpieza-de-datos"><i class="fa fa-check"></i><b>3.2</b> Limpieza de datos</a></li>
<li class="chapter" data-level="3.3" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#separa-aplica-combina"><i class="fa fa-check"></i><b>3.3</b> <em>Separa-aplica-combina</em></a></li>
<li class="chapter" data-level="3.4" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#muertes-por-armas-de-fuego-en-eua"><i class="fa fa-check"></i><b>3.4</b> Muertes por armas de fuego en EUA</a></li>
<li class="chapter" data-level="3.5" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#el-cuarteto-de-anscombe"><i class="fa fa-check"></i><b>3.5</b> El Cuarteto de Anscombe</a></li>
<li class="chapter" data-level="3.6" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#the-grammar-of-graphics-de-leland-wilkinson"><i class="fa fa-check"></i><b>3.6</b> The <em>Grammar of Graphics</em> de Leland Wilkinson</a></li>
<li class="chapter" data-level="3.7" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#ggplot"><i class="fa fa-check"></i><b>3.7</b> ggplot</a></li>
<li class="chapter" data-level="3.8" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#un-histograma-de-las-muertes-en-iraq"><i class="fa fa-check"></i><b>3.8</b> Un histograma de las muertes en Iraq</a></li>
<li class="chapter" data-level="3.9" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#inglehartwelzel-un-mapa-cultural-del-mundo"><i class="fa fa-check"></i><b>3.9</b> Inglehart–Welzel: un mapa cultural del mundo</a><ul>
<li class="chapter" data-level="3.9.1" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#creando-un-ggplot"><i class="fa fa-check"></i><b>3.9.1</b> Creando un ggplot</a></li>
<li class="chapter" data-level="3.9.2" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#mapeos-aesthetics"><i class="fa fa-check"></i><b>3.9.2</b> Mapeos: Aesthetics</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#poniendo-todo-junto"><i class="fa fa-check"></i><b>3.10</b> Poniendo todo junto</a></li>
<li class="chapter" data-level="3.11" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#tarea-2"><i class="fa fa-check"></i><b>3.11</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html"><i class="fa fa-check"></i><b>4</b> Teorema del Límite Central</a><ul>
<li class="chapter" data-level="4.1" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#la-distribucion-de-la-media"><i class="fa fa-check"></i><b>4.1</b> La distribución de la media</a></li>
<li class="chapter" data-level="4.2" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#de-donde-proviene-la-distribucion-normal"><i class="fa fa-check"></i><b>4.2</b> ¿De dónde proviene la distribución normal?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#que-signo-tiene-c"><i class="fa fa-check"></i><b>4.2.1</b> ¿Qué signo tiene c?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#otras-observaciones"><i class="fa fa-check"></i><b>4.3</b> Otras observaciones</a></li>
<li class="chapter" data-level="4.4" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#diagramas-de-caja-y-brazos"><i class="fa fa-check"></i><b>4.4</b> Diagramas de caja y brazos</a><ul>
<li class="chapter" data-level="" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#graficas-de-cuantiles-teoricos"><i class="fa fa-check"></i><b>4.5</b> Gráficas de cuantiles teóricos</a><ul>
<li class="chapter" data-level="" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo-normal"><i class="fa fa-check"></i>Ejemplo: normal</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#graficas-de-cuantiles-para-un-conjunto-de-datos"><i class="fa fa-check"></i><b>4.6</b> Gráficas de cuantiles para un conjunto de datos</a><ul>
<li class="chapter" data-level="4.6.1" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#que-buscar-en-una-grafica-de-cuantiles"><i class="fa fa-check"></i><b>4.6.1</b> ¿Qué buscar en una gráfica de cuantiles?</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#graficas-qq-normales"><i class="fa fa-check"></i><b>4.7</b> Gráficas qq-normales</a><ul>
<li class="chapter" data-level="" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo-cantantes"><i class="fa fa-check"></i>Ejemplo: cantantes</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#el-tlc-y-errores-estandar"><i class="fa fa-check"></i><b>4.8</b> El TLC y errores estándar</a></li>
<li class="chapter" data-level="4.9" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo-1"><i class="fa fa-check"></i><b>4.9</b> Ejemplo</a></li>
<li class="chapter" data-level="4.10" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#tarea-3"><i class="fa fa-check"></i><b>4.10</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html"><i class="fa fa-check"></i><b>5</b> Análisis de datos categóricos</a><ul>
<li class="chapter" data-level="5.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#repaso-y-algunos-conceptos"><i class="fa fa-check"></i><b>5.1</b> Repaso y algunos conceptos</a><ul>
<li class="chapter" data-level="5.1.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#caso-binomial"><i class="fa fa-check"></i><b>5.1.1</b> Caso binomial</a></li>
<li class="chapter" data-level="" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#estimacion-de-parametros-multinomiales"><i class="fa fa-check"></i>Estimación de parámetros multinomiales</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#la-chi2-de-pearson-de-una-multinomial"><i class="fa fa-check"></i><b>5.2</b> La <span class="math inline">\(\chi^2\)</span> de Pearson de una multinomial</a><ul>
<li class="chapter" data-level="5.2.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#cociente-de-verosimilitud-de-una-multinomial"><i class="fa fa-check"></i><b>5.2.1</b> Cociente de verosimilitud de una multinomial</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#definiciones"><i class="fa fa-check"></i><b>5.3</b> Definiciones</a><ul>
<li class="chapter" data-level="5.3.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#notacion"><i class="fa fa-check"></i><b>5.3.1</b> Notación</a></li>
<li class="chapter" data-level="5.3.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#razon-de-momios"><i class="fa fa-check"></i><b>5.3.2</b> Razón de momios</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#asociacion-en-tablas-de-tamano-itimes-j"><i class="fa fa-check"></i><b>5.4</b> Asociación en tablas de tamaño <span class="math inline">\(I\times J\)</span></a><ul>
<li class="chapter" data-level="5.4.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#razones-de-momios-en-tablas-itimes-j"><i class="fa fa-check"></i><b>5.4.1</b> Razones de momios en tablas <span class="math inline">\(I\times J\)</span></a></li>
<li class="chapter" data-level="5.4.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-mushrooms"><i class="fa fa-check"></i><b>5.4.2</b> Ejemplo: mushrooms</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#intervalos-de-confianza-para-los-parametros-de-asociacion"><i class="fa fa-check"></i><b>5.5</b> Intervalos de confianza para los parámetros de asociación</a><ul>
<li class="chapter" data-level="5.5.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#error-estandar-de-la-razon-de-momios"><i class="fa fa-check"></i><b>5.5.1</b> Error estándar de la razón de momios</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#prueba-de-independencia"><i class="fa fa-check"></i><b>5.6</b> Prueba de independencia</a><ul>
<li class="chapter" data-level="5.6.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#la-prueba-chi2-de-pearson"><i class="fa fa-check"></i><b>5.6.1</b> La prueba <span class="math inline">\(\chi^2\)</span> de Pearson</a></li>
<li class="chapter" data-level="5.6.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-brecha-de-genero"><i class="fa fa-check"></i><b>5.6.2</b> Ejemplo: brecha de género</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#general-social-survey-1972---2016"><i class="fa fa-check"></i><b>5.7</b> General Social Survey 1972 - 2016</a></li>
<li class="chapter" data-level="5.8" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#la-catadora-de-te"><i class="fa fa-check"></i><b>5.8</b> La catadora de té</a></li>
<li class="chapter" data-level="5.9" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#modelos-multinomiales-para-conteos"><i class="fa fa-check"></i><b>5.9</b> Modelos multinomiales para conteos</a></li>
<li class="chapter" data-level="5.10" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#modelos-log-lineales-con-tres-variables-categoricas"><i class="fa fa-check"></i><b>5.10</b> Modelos log lineales con tres variables categóricas</a><ul>
<li class="chapter" data-level="5.10.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#tipos-de-independencia"><i class="fa fa-check"></i><b>5.10.1</b> Tipos de independencia</a></li>
<li class="chapter" data-level="5.10.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#asociacion-homogenea-e-interacciones-de-3-factores"><i class="fa fa-check"></i><b>5.10.2</b> Asociación homogénea e interacciones de 3 factores</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-sensitividad-y-especificidad"><i class="fa fa-check"></i><b>5.11</b> Ejemplo: sensitividad y especificidad</a></li>
<li class="chapter" data-level="5.12" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-horoscopos"><i class="fa fa-check"></i><b>5.12</b> Ejemplo: horóscopos</a></li>
<li class="chapter" data-level="5.13" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#tarea-opcional"><i class="fa fa-check"></i><b>5.13</b> Tarea (opcional)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html"><i class="fa fa-check"></i><b>6</b> Regresión logística 1</a><ul>
<li class="chapter" data-level="6.1" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#regresion-logistica-con-un-solo-predictor"><i class="fa fa-check"></i><b>6.1</b> Regresión logística con un solo predictor</a></li>
<li class="chapter" data-level="6.2" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#el-modelo-de-regresion-logistica"><i class="fa fa-check"></i><b>6.2</b> El modelo de regresión logística</a><ul>
<li class="chapter" data-level="6.2.1" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#funcion-logistica"><i class="fa fa-check"></i><b>6.2.1</b> Función logística</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#ejemplo-2"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#tarea-4"><i class="fa fa-check"></i><b>6.3</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html"><i class="fa fa-check"></i><b>7</b> Regresión logística 2</a><ul>
<li class="chapter" data-level="7.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#incertidumbre-en-la-estimacion"><i class="fa fa-check"></i><b>7.1</b> Incertidumbre en la estimación</a></li>
<li class="chapter" data-level="7.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#funcion-logistica-1"><i class="fa fa-check"></i><b>7.2</b> Función logística</a></li>
<li class="chapter" data-level="7.3" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#interpretacion-de-los-coeficientes"><i class="fa fa-check"></i><b>7.3</b> Interpretación de los coeficientes</a><ul>
<li class="chapter" data-level="7.3.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#evaluar-en-o-alrededor-de-la-media"><i class="fa fa-check"></i><b>7.3.1</b> Evaluar en (o alrededor de) la media</a></li>
<li class="chapter" data-level="7.3.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#la-regla-de-dividir-entre-4"><i class="fa fa-check"></i><b>7.3.2</b> La regla de “dividir entre 4”</a></li>
<li class="chapter" data-level="7.3.3" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#interpretacion-de-los-coeficientes-como-cocientes-de-momios"><i class="fa fa-check"></i><b>7.3.3</b> Interpretación de los coeficientes como cocientes de momios</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ejemplo-pozos-en-bangladesh"><i class="fa fa-check"></i><b>7.4</b> Ejemplo: pozos en Bangladesh</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#descripcion-del-problema"><i class="fa fa-check"></i><b>7.4.1</b> Descripción del problema</a></li>
<li class="chapter" data-level="7.4.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#antecedentes-del-problema"><i class="fa fa-check"></i><b>7.4.2</b> Antecedentes del problema</a></li>
<li class="chapter" data-level="7.4.3" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#metodologia-para-abordar-el-problema"><i class="fa fa-check"></i><b>7.4.3</b> Metodología para abordar el problema</a></li>
<li class="chapter" data-level="7.4.4" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ajuste-y-resultados-del-modelo"><i class="fa fa-check"></i><b>7.4.4</b> Ajuste y resultados del modelo</a></li>
<li class="chapter" data-level="7.4.5" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#interpretacion-de-los-coeficientes-1"><i class="fa fa-check"></i><b>7.4.5</b> Interpretación de los coeficientes</a></li>
<li class="chapter" data-level="7.4.6" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#agregamos-una-segunda-variable-de-entrada"><i class="fa fa-check"></i><b>7.4.6</b> Agregamos una segunda variable de entrada</a></li>
<li class="chapter" data-level="7.4.7" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#comparacion-de-coeficientes-cuando-anades-un-predictor"><i class="fa fa-check"></i><b>7.4.7</b> Comparación de coeficientes cuando añades un predictor</a></li>
<li class="chapter" data-level="7.4.8" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#graficar-el-modelo-ajustado-con-dos-predictores"><i class="fa fa-check"></i><b>7.4.8</b> Graficar el modelo ajustado con dos predictores</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ajuste-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>7.5</b> Ajuste de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="7.6" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>7.6</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="7.6.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>7.6.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="7.6.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>7.6.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ejemplo-diabetes"><i class="fa fa-check"></i><b>7.7</b> Ejemplo: diabetes</a></li>
<li class="chapter" data-level="7.8" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#observaciones-adicionales"><i class="fa fa-check"></i><b>7.8</b> Observaciones adicionales</a></li>
<li class="chapter" data-level="7.9" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>7.9</b> Regresión logística para problemas de más de 2 clases</a></li>
<li class="chapter" data-level="7.10" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>7.10</b> Regresión logística multinomial</a></li>
<li class="chapter" data-level="7.11" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#identificabilidad-y-separacion"><i class="fa fa-check"></i><b>7.11</b> Identificabilidad y separación</a></li>
<li class="chapter" data-level="7.12" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#tarea-5"><i class="fa fa-check"></i><b>7.12</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html"><i class="fa fa-check"></i><b>8</b> Regresión logística 3</a><ul>
<li class="chapter" data-level="8.1" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#ejemplo-oscares"><i class="fa fa-check"></i><b>8.1</b> Ejemplo óscares</a></li>
<li class="chapter" data-level="8.2" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#repaso-de-regresion-logistica"><i class="fa fa-check"></i><b>8.2</b> Repaso de regresión logística</a></li>
<li class="chapter" data-level="8.3" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#regresion-logistica-con-interacciones"><i class="fa fa-check"></i><b>8.3</b> Regresión logística con interacciones</a></li>
<li class="chapter" data-level="8.4" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#graficas-del-modelo-con-interacciones"><i class="fa fa-check"></i><b>8.4</b> Gráficas del modelo con interacciones</a></li>
<li class="chapter" data-level="8.5" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#agregar-mas-predictores"><i class="fa fa-check"></i><b>8.5</b> Agregar más predictores</a></li>
<li class="chapter" data-level="8.6" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#evaluacion-de-modelos-de-regresion-logistica"><i class="fa fa-check"></i><b>8.6</b> Evaluación de modelos de regresión logística</a><ul>
<li class="chapter" data-level="8.6.1" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#graficas-de-residuales-agrupados-vs-predictores"><i class="fa fa-check"></i><b>8.6.1</b> Gráficas de residuales agrupados vs predictores</a></li>
<li class="chapter" data-level="8.6.2" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#transformaciones"><i class="fa fa-check"></i><b>8.6.2</b> Transformaciones</a></li>
<li class="chapter" data-level="8.6.3" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#tasa-de-error-y-comparacion-contra-el-modelo-nulo"><i class="fa fa-check"></i><b>8.6.3</b> Tasa de error y comparación contra el modelo nulo</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#diferencias-predictivas-promedio-en-la-escala-de-probabilidad"><i class="fa fa-check"></i><b>8.7</b> Diferencias predictivas promedio en la escala de probabilidad</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#diferencias-predictivas-promedio-en-presencia-de-interacciones"><i class="fa fa-check"></i>Diferencias predictivas promedio en presencia de interacciones</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#notacion-general-para-diferencias-predictivas"><i class="fa fa-check"></i>Notación general para diferencias predictivas</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#tarea-6"><i class="fa fa-check"></i><b>8.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regularizacion.html"><a href="regularizacion.html"><i class="fa fa-check"></i><b>9</b> Regularización</a><ul>
<li class="chapter" data-level="9.1" data-path="regularizacion.html"><a href="regularizacion.html#repaso"><i class="fa fa-check"></i><b>9.1</b> Repaso</a></li>
<li class="chapter" data-level="9.2" data-path="regularizacion.html"><a href="regularizacion.html#otras-medidas-de-clasificacion"><i class="fa fa-check"></i><b>9.2</b> Otras medidas de clasificación</a></li>
<li class="chapter" data-level="9.3" data-path="regularizacion.html"><a href="regularizacion.html#analisis-de-error-en-clasificacion-binaria"><i class="fa fa-check"></i><b>9.3</b> Análisis de error en clasificación binaria</a><ul>
<li class="chapter" data-level="9.3.1" data-path="regularizacion.html"><a href="regularizacion.html#punto-de-corte-para-un-clasificador-binario"><i class="fa fa-check"></i><b>9.3.1</b> Punto de corte para un clasificador binario</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="regularizacion.html"><a href="regularizacion.html#curvas-roc"><i class="fa fa-check"></i><b>9.4</b> Curvas ROC</a><ul>
<li class="chapter" data-level="9.4.1" data-path="regularizacion.html"><a href="regularizacion.html#espacio-roc"><i class="fa fa-check"></i><b>9.4.1</b> Espacio ROC</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-1"><i class="fa fa-check"></i><b>9.5</b> Regularización</a><ul>
<li class="chapter" data-level="9.5.1" data-path="regularizacion.html"><a href="regularizacion.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>9.5.1</b> Reduciendo varianza de los coeficientes</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-ridge"><i class="fa fa-check"></i><b>9.6</b> Regularización Ridge</a><ul>
<li class="chapter" data-level="9.6.1" data-path="regularizacion.html"><a href="regularizacion.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>9.6.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-lasso"><i class="fa fa-check"></i><b>9.7</b> Regularización Lasso</a></li>
<li class="chapter" data-level="9.8" data-path="regularizacion.html"><a href="regularizacion.html#tarea-7"><i class="fa fa-check"></i><b>9.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>10</b> Modelos lineales generalizados</a><ul>
<li class="chapter" data-level="10.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresion-lineal-y-logistica"><i class="fa fa-check"></i><b>10.1</b> Regresión lineal y logística</a></li>
<li class="chapter" data-level="10.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#otros-modelos"><i class="fa fa-check"></i><b>10.2</b> Otros modelos</a></li>
<li class="chapter" data-level="10.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-accidentes-de-trafico"><i class="fa fa-check"></i><b>10.3</b> Ejemplo: accidentes de tráfico</a></li>
<li class="chapter" data-level="10.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#interpretacion-de-coeficientes-poisson"><i class="fa fa-check"></i><b>10.4</b> Interpretación de coeficientes Poisson</a></li>
<li class="chapter" data-level="10.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#diferencias-entre-el-modelo-binomial-y-poisson"><i class="fa fa-check"></i><b>10.5</b> Diferencias entre el modelo binomial y Poisson</a></li>
<li class="chapter" data-level="10.6" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-fertilidad-en-fiji"><i class="fa fa-check"></i><b>10.6</b> Ejemplo: fertilidad en Fiji</a></li>
<li class="chapter" data-level="10.7" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#variable-de-expuestos-offset"><i class="fa fa-check"></i><b>10.7</b> Variable de expuestos (offset)</a></li>
<li class="chapter" data-level="10.8" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplos-seguros"><i class="fa fa-check"></i><b>10.8</b> Ejemplos: seguros</a><ul>
<li class="chapter" data-level="" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#numero-de-expuestos-interpretacion"><i class="fa fa-check"></i>Número de expuestos (interpretación)</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-arboles"><i class="fa fa-check"></i><b>10.9</b> Ejemplo: árboles</a></li>
<li class="chapter" data-level="10.10" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#sobredispersion"><i class="fa fa-check"></i><b>10.10</b> Sobredispersión</a></li>
<li class="chapter" data-level="10.11" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-numero-de-publicaciones"><i class="fa fa-check"></i><b>10.11</b> Ejemplo: número de publicaciones</a></li>
<li class="chapter" data-level="10.12" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#tarea-8"><i class="fa fa-check"></i><b>10.12</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html"><i class="fa fa-check"></i><b>11</b> Análisis de Discriminante Lineal 1</a><ul>
<li class="chapter" data-level="11.1" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#problemas-de-clasificacion"><i class="fa fa-check"></i><b>11.1</b> Problemas de clasificación</a></li>
<li class="chapter" data-level="11.2" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#funciones-de-discriminante"><i class="fa fa-check"></i><b>11.2</b> Funciones de discriminante</a></li>
<li class="chapter" data-level="11.3" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#regresion-lineal-en-una-matriz-indicadora"><i class="fa fa-check"></i><b>11.3</b> Regresión lineal en una matriz indicadora</a></li>
<li class="chapter" data-level="11.4" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#discriminante-lineal-de-fisher"><i class="fa fa-check"></i><b>11.4</b> Discriminante lineal de Fisher</a><ul>
<li class="chapter" data-level="11.4.1" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#ejemplo-separacion-entre-clases"><i class="fa fa-check"></i><b>11.4.1</b> Ejemplo: separación entre clases</a></li>
<li class="chapter" data-level="11.4.2" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#ejemplo-iris-de-fisher"><i class="fa fa-check"></i><b>11.4.2</b> Ejemplo: iris de Fisher</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#tarea-9"><i class="fa fa-check"></i><b>11.5</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html"><i class="fa fa-check"></i><b>12</b> Análisis de Discriminante Lineal 2</a><ul>
<li class="chapter" data-level="12.1" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#aplicaciones"><i class="fa fa-check"></i><b>12.1</b> Aplicaciones</a></li>
<li class="chapter" data-level="12.2" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#ejemplo-vinos"><i class="fa fa-check"></i><b>12.2</b> Ejemplo: vinos</a></li>
<li class="chapter" data-level="12.3" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#ejemplo-admisiones-al-mba"><i class="fa fa-check"></i><b>12.3</b> Ejemplo: admisiones al MBA</a></li>
<li class="chapter" data-level="12.4" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#repaso-1"><i class="fa fa-check"></i><b>12.4</b> Repaso</a><ul>
<li><a href="analisis-de-discriminante-lineal-2.html#caso-k2">Caso <span class="math inline">\(k=2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#supuestos-probabilisticos"><i class="fa fa-check"></i><b>12.5</b> Supuestos probabilísticos</a></li>
<li class="chapter" data-level="12.6" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#relacion-con-minimos-cuadrados"><i class="fa fa-check"></i><b>12.6</b> Relación con mínimos cuadrados</a></li>
<li class="chapter" data-level="12.7" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#tarea-10"><i class="fa fa-check"></i><b>12.7</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html"><i class="fa fa-check"></i><b>13</b> Componentes Principales 1</a><ul>
<li class="chapter" data-level="13.1" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#motivacion"><i class="fa fa-check"></i><b>13.1</b> Motivación</a></li>
<li class="chapter" data-level="13.2" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#formulacion-de-maxima-varianza"><i class="fa fa-check"></i><b>13.2</b> Formulación de máxima varianza</a></li>
<li class="chapter" data-level="13.3" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#formulacion-de-error-minimo"><i class="fa fa-check"></i><b>13.3</b> Formulación de error mínimo</a></li>
<li class="chapter" data-level="13.4" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#aplicaciones-de-pca"><i class="fa fa-check"></i><b>13.4</b> Aplicaciones de PCA</a><ul>
<li class="chapter" data-level="13.4.1" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#compresion-de-datos"><i class="fa fa-check"></i><b>13.4.1</b> Compresión de datos</a></li>
<li class="chapter" data-level="13.4.2" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#ejemplo-compresion-de-una-imagen"><i class="fa fa-check"></i><b>13.4.2</b> Ejemplo: compresión de una imagen</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#tarea-11"><i class="fa fa-check"></i><b>13.5</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html"><i class="fa fa-check"></i><b>14</b> Componentes Principales 2</a><ul>
<li class="chapter" data-level="14.1" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#pca-probabilistico-y-analisis-de-factores"><i class="fa fa-check"></i><b>14.1</b> PCA probabilístico y Análisis de Factores</a><ul>
<li class="chapter" data-level="14.1.1" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#analisis-de-factores"><i class="fa fa-check"></i><b>14.1.1</b> Análisis de factores</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#analisis-de-factores-descripcion-tradicional"><i class="fa fa-check"></i><b>14.2</b> Análisis de factores (descripción tradicional)</a><ul>
<li class="chapter" data-level="14.2.1" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#el-modelo"><i class="fa fa-check"></i><b>14.2.1</b> El modelo</a></li>
<li class="chapter" data-level="14.2.2" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#estimacion-del-modelo"><i class="fa fa-check"></i><b>14.2.2</b> Estimación del modelo</a></li>
<li class="chapter" data-level="14.2.3" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#analisis-de-factores-de-maxima-verosimilitud"><i class="fa fa-check"></i><b>14.2.3</b> Análisis de factores de máxima verosimilitud</a></li>
<li class="chapter" data-level="14.2.4" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#evaluacion-del-modelo"><i class="fa fa-check"></i><b>14.2.4</b> Evaluación del modelo</a></li>
<li class="chapter" data-level="14.2.5" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#visualizacion"><i class="fa fa-check"></i><b>14.2.5</b> Visualización</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html#tarea-12"><i class="fa fa-check"></i><b>14.3</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="correlacion-canonica-cca.html"><a href="correlacion-canonica-cca.html"><i class="fa fa-check"></i><b>15</b> Correlación Canónica (CCA)</a><ul>
<li class="chapter" data-level="15.1" data-path="correlacion-canonica-cca.html"><a href="correlacion-canonica-cca.html#cca-vs-pca"><i class="fa fa-check"></i><b>15.1</b> CCA vs PCA</a></li>
<li class="chapter" data-level="15.2" data-path="correlacion-canonica-cca.html"><a href="correlacion-canonica-cca.html#variables-y-correlaciones-canonicas"><i class="fa fa-check"></i><b>15.2</b> Variables y correlaciones canónicas</a><ul>
<li class="chapter" data-level="15.2.1" data-path="correlacion-canonica-cca.html"><a href="correlacion-canonica-cca.html#combinaciones-lineaes-de-factores"><i class="fa fa-check"></i><b>15.2.1</b> Combinaciones lineaes de factores</a></li>
<li class="chapter" data-level="15.2.2" data-path="correlacion-canonica-cca.html"><a href="correlacion-canonica-cca.html#ejemplo-simple"><i class="fa fa-check"></i><b>15.2.2</b> Ejemplo simple</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="correlacion-canonica-cca.html"><a href="correlacion-canonica-cca.html#ejemplo-test-psicologico"><i class="fa fa-check"></i><b>15.3</b> Ejemplo: test psicológico</a></li>
<li class="chapter" data-level="15.4" data-path="correlacion-canonica-cca.html"><a href="correlacion-canonica-cca.html#tarea-13"><i class="fa fa-check"></i><b>15.4</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html"><i class="fa fa-check"></i><b>16</b> Conglomerados (clustering) 1</a><ul>
<li class="chapter" data-level="16.1" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#introduccion"><i class="fa fa-check"></i><b>16.1</b> Introducción</a><ul>
<li class="chapter" data-level="" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#ejemplo-7"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#enfoques-combinatorio-y-basado-en-modelos."><i class="fa fa-check"></i><b>16.2</b> Enfoques: combinatorio y basado en modelos.</a></li>
<li class="chapter" data-level="16.3" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#k-medias"><i class="fa fa-check"></i><b>16.3</b> K-medias</a><ul>
<li class="chapter" data-level="" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#algoritmo-de-k-medias"><i class="fa fa-check"></i>Algoritmo de k-medias</a></li>
<li class="chapter" data-level="" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#ejemplo-8"><i class="fa fa-check"></i>Ejemplo</a></li>
<li><a href="conglomerados-clustering-1.html#usando-la-funcion-kmeans">Usando la funcion <code>kmeans</code></a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#seleccion-de-numero-de-clusters."><i class="fa fa-check"></i><b>16.4</b> Selección de número de clusters.</a><ul>
<li class="chapter" data-level="" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#variacion-dentro-de-clusters-para-distintas-soluciones"><i class="fa fa-check"></i>Variación dentro de clusters para distintas soluciones</a></li>
<li class="chapter" data-level="16.4.1" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#criterios-especificos"><i class="fa fa-check"></i><b>16.4.1</b> Criterios específicos</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#dificultades-en-segmentacionclustering."><i class="fa fa-check"></i><b>16.5</b> Dificultades en segmentación/clustering.</a><ul>
<li class="chapter" data-level="16.5.1" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#estructuras-no-compactas"><i class="fa fa-check"></i><b>16.5.1</b> Estructuras no compactas</a></li>
<li class="chapter" data-level="16.5.2" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#existencia-o-no-de-grupos-naturales"><i class="fa fa-check"></i><b>16.5.2</b> Existencia o no de grupos “naturales”</a></li>
<li class="chapter" data-level="" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#grupos-en-dimension-alta"><i class="fa fa-check"></i>Grupos en dimensión alta</a></li>
<li class="chapter" data-level="" data-path="conglomerados-clustering-1.html"><a href="conglomerados-clustering-1.html#dificultades-en-la-seleccion-de-metrica"><i class="fa fa-check"></i>Dificultades en la selección de métrica</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="conglomerados-clustering-2.html"><a href="conglomerados-clustering-2.html"><i class="fa fa-check"></i><b>17</b> Conglomerados (clustering) 2</a><ul>
<li class="chapter" data-level="17.1" data-path="conglomerados-clustering-2.html"><a href="conglomerados-clustering-2.html#clustering-jerarquico"><i class="fa fa-check"></i><b>17.1</b> Clustering jerárquico</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Estadística Aplicada III</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analisis-de-discriminante-lineal-2" class="section level1">
<h1><span class="header-section-number">Clase 12</span> Análisis de Discriminante Lineal 2</h1>
<style>
  .espacio {
    margin-bottom: 1cm;
  }
</style>
<style>
  .espacio3 {
    margin-bottom: 3cm;
  }
</style>
<p class="espacio">
</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)</code></pre></div>
<div id="aplicaciones" class="section level2">
<h2><span class="header-section-number">12.1</span> Aplicaciones</h2>
<p>El análisis discriminante lineal (LDA) de Fisher es un método utilizado en estadística, reconocimiento de patrones y aprendizaje estadístico para encontrar una combinación lineal de variables que caracteriza o separa dos o más clases de objetos o eventos.</p>
<p>La combinación lineal resultante se puede utilizar como un clasificador lineal o, más comúnmente, para la reducción de dimensionalidad antes de usar otro método de clasificación.</p>
<p>El análisis de discriminante lineal tiene entre sus posibles aplicaciones:</p>
<ol style="list-style-type: decimal">
<li><p>Predicción de bancarrota: la predicción de bancarrota se basa en datos de en indicadores contables y otras variables financieras. El análisis discriminante lineal fue el primer método estadístico aplicado para explicar sistemáticamente qué empresas entraron en bancarrota vs. cuáles sobrevivieron.</p></li>
<li><p>Marketing: el análisis discriminante solía utilizarse para determinar los factores que distinguen diferentes tipos de clientes y/o productos utilizando datos provenientes de encuestas u otras fuentes de datos.</p></li>
<li><p>Estudios biomédicos: la principal aplicación del análisis discriminante en medicina es la evaluación del estado de gravedad de un paciente y el pronóstico del desenlace de la enfermedad. Por ejemplo, en un análisis retrospectivo, los pacientes se dividen en grupos según la gravedad de la enfermedad: leve, moderada y grave. Luego, se estudian los resultados de los análisis clínicos y de laboratorio para ver qué variables son toman valores diferentes entre los grupos estudiados. Usando estas variables, se construyen funciones discriminantes que ayudan a clasificar objetivamente la enfermedad en un futuro paciente en leve, moderada o severa.</p></li>
</ol>
<p>El análisis de discriminante lineal también se conoce como “análisis discriminante canónico”, o simplemente “análisis discriminante”.</p>
</div>
<div id="ejemplo-vinos" class="section level2">
<h2><span class="header-section-number">12.2</span> Ejemplo: vinos</h2>
<p>Tenemos 13 concentraciones químicas que describen muestras de vino de tres cultivares.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;datos/wine.csv&quot;</span>)
wine <span class="op">%&gt;%</span><span class="st"> </span>head <span class="op">%&gt;%</span><span class="st"> </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Type</th>
<th align="right">Alcohol</th>
<th align="right">Malic</th>
<th align="right">Ash</th>
<th align="right">Alcalinity</th>
<th align="right">Magnesium</th>
<th align="right">Phenols</th>
<th align="right">Flavanoids</th>
<th align="right">Nonflavanoids</th>
<th align="right">Proanthocyanins</th>
<th align="right">Color</th>
<th align="right">Hue</th>
<th align="right">Dilution</th>
<th align="right">Proline</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">14.2</td>
<td align="right">1.71</td>
<td align="right">2.43</td>
<td align="right">15.6</td>
<td align="right">127</td>
<td align="right">2.80</td>
<td align="right">3.06</td>
<td align="right">0.28</td>
<td align="right">2.29</td>
<td align="right">5.64</td>
<td align="right">1.04</td>
<td align="right">3.92</td>
<td align="right">1065</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">13.2</td>
<td align="right">1.78</td>
<td align="right">2.14</td>
<td align="right">11.2</td>
<td align="right">100</td>
<td align="right">2.65</td>
<td align="right">2.76</td>
<td align="right">0.26</td>
<td align="right">1.28</td>
<td align="right">4.38</td>
<td align="right">1.05</td>
<td align="right">3.40</td>
<td align="right">1050</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">13.2</td>
<td align="right">2.36</td>
<td align="right">2.67</td>
<td align="right">18.6</td>
<td align="right">101</td>
<td align="right">2.80</td>
<td align="right">3.24</td>
<td align="right">0.30</td>
<td align="right">2.81</td>
<td align="right">5.68</td>
<td align="right">1.03</td>
<td align="right">3.17</td>
<td align="right">1185</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">14.4</td>
<td align="right">1.95</td>
<td align="right">2.50</td>
<td align="right">16.8</td>
<td align="right">113</td>
<td align="right">3.85</td>
<td align="right">3.49</td>
<td align="right">0.24</td>
<td align="right">2.18</td>
<td align="right">7.80</td>
<td align="right">0.86</td>
<td align="right">3.45</td>
<td align="right">1480</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">13.2</td>
<td align="right">2.59</td>
<td align="right">2.87</td>
<td align="right">21.0</td>
<td align="right">118</td>
<td align="right">2.80</td>
<td align="right">2.69</td>
<td align="right">0.39</td>
<td align="right">1.82</td>
<td align="right">4.32</td>
<td align="right">1.04</td>
<td align="right">2.93</td>
<td align="right">735</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">14.2</td>
<td align="right">1.76</td>
<td align="right">2.45</td>
<td align="right">15.2</td>
<td align="right">112</td>
<td align="right">3.27</td>
<td align="right">3.39</td>
<td align="right">0.34</td>
<td align="right">1.97</td>
<td align="right">6.75</td>
<td align="right">1.05</td>
<td align="right">2.85</td>
<td align="right">1450</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GGally)
<span class="kw">ggpairs</span>(wine[<span class="dv">2</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">columns =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">title =</span> <span class="st">&quot;&quot;</span>,  
  <span class="dt">axisLabels =</span> <span class="st">&quot;show&quot;</span>, <span class="dt">columnLabels =</span> <span class="kw">colnames</span>(wine[<span class="dv">2</span><span class="op">:</span><span class="dv">6</span>]))</code></pre></div>
<p><img src="12-lda-2_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>El propósito del análisis de discriminante lineal (LDA) en este ejemplo es encontrar las combinaciones lineales de las variables originales (las 13 concentraciones químicas) que proporcionen <em>la mejor separación</em> posible entre los grupos (variedades de vino) en nuestro conjunto de datos.</p>
<p>Supongamos entonces que queremos separar los vinos por cultivar. Los vinos provienen de tres cultivares diferentes, por lo que el número de clases es <span class="math inline">\(K = 3\)</span> y el número de variables predictoras es 13 (concentraciones de 13 componentes químicos, <span class="math inline">\(p = 13\)</span>).</p>
<p>El número máximo de funciones discriminantes útiles que pueden separar los vinos por cultivar es el mínimo entre <span class="math inline">\(K-1\)</span> y <span class="math inline">\(p\)</span>, por lo que en este caso es el número de superficies de decisión es 2.</p>
<p>Por lo tanto, podemos encontrar como máximo 2 funciones discriminantes útiles para separar los vinos por cultivar, utilizando las 13 variables de concentraciones químicas.</p>
<p>Ajustamos el modelo de discriminante lineal:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
wine_lda &lt;-<span class="st"> </span><span class="kw">lda</span>(Type <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> wine)</code></pre></div>
<p>Para obtener los valores de los pesos de las funciones discriminantes, podemos escribir:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine_lda
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lda(Type ~ ., data = wine)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Prior probabilities of groups:</span>
<span class="co">#&gt;     1     2     3 </span>
<span class="co">#&gt; 0.331 0.399 0.270 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group means:</span>
<span class="co">#&gt;   Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids</span>
<span class="co">#&gt; 1    13.7  2.01 2.46       17.0     106.3    2.84      2.982         0.290</span>
<span class="co">#&gt; 2    12.3  1.93 2.24       20.2      94.5    2.26      2.081         0.364</span>
<span class="co">#&gt; 3    13.2  3.33 2.44       21.4      99.3    1.68      0.781         0.448</span>
<span class="co">#&gt;   Proanthocyanins Color   Hue Dilution Proline</span>
<span class="co">#&gt; 1            1.90  5.53 1.062     3.16    1116</span>
<span class="co">#&gt; 2            1.63  3.09 1.056     2.79     520</span>
<span class="co">#&gt; 3            1.15  7.40 0.683     1.68     630</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients of linear discriminants:</span>
<span class="co">#&gt;                      LD1       LD2</span>
<span class="co">#&gt; Alcohol         -0.40340  0.871793</span>
<span class="co">#&gt; Malic            0.16525  0.305380</span>
<span class="co">#&gt; Ash             -0.36908  2.345850</span>
<span class="co">#&gt; Alcalinity       0.15480 -0.146381</span>
<span class="co">#&gt; Magnesium       -0.00216 -0.000463</span>
<span class="co">#&gt; Phenols          0.61805 -0.032213</span>
<span class="co">#&gt; Flavanoids      -1.66119 -0.491998</span>
<span class="co">#&gt; Nonflavanoids   -1.49582 -1.630954</span>
<span class="co">#&gt; Proanthocyanins  0.13409 -0.307088</span>
<span class="co">#&gt; Color            0.35506  0.253231</span>
<span class="co">#&gt; Hue             -0.81804 -1.515634</span>
<span class="co">#&gt; Dilution        -1.15756  0.051184</span>
<span class="co">#&gt; Proline         -0.00269  0.002853</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Proportion of trace:</span>
<span class="co">#&gt;   LD1   LD2 </span>
<span class="co">#&gt; 0.688 0.312</span></code></pre></div>
<p>Esto significa que la primera función discriminante es una combinación lineal de las variables: <span class="math display">\[
-0.403 \cdot \mbox{Alcohol} + 0.165 \cdot \mbox{Malic} \;+\cdots+\; -0.003 \cdot \mbox{Proline}. 
\]</span></p>
<p>Por conveniencia, el valor de cada función discriminante (por ejemplo, la primera función discriminante) se escala de modo que su media sea cero y su varianza sea uno.</p>
<p>La “proporción de traza” que se imprime al final es una medida de la separación porcentual lograda por cada función discriminante. Por ejemplo, para estos datos de vinos obtenemos los mismos valores que acabamos de calcular (68.75% y 31.25%).</p>
<p>Una buena forma de mostrar los resultados de un análisis de discriminante lineal es hacer un histograma apilado de los valores de la función discriminante para las diferentes clases.</p>
<p>Podemos hacer esto usando la función <code>ldahist()</code>. Por ejemplo, para hacer un histograma apilado de los valores de la primera función discriminante:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(wine_lda)
<span class="kw">ldahist</span>(<span class="dt">data =</span> wine_pred<span class="op">$</span>x[,<span class="dv">1</span>], <span class="dt">g=</span>wine<span class="op">$</span>Type)</code></pre></div>
<p><img src="12-lda-2_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Por lo tanto, investigamos si la segunda función discriminante separa esos cultivares, al hacer un histograma apilado de los valores de la segunda función discriminante:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ldahist</span>(<span class="dt">data =</span> wine_pred<span class="op">$</span>x[,<span class="dv">2</span>], <span class="dt">g=</span>wine<span class="op">$</span>Type)</code></pre></div>
<p><img src="12-lda-2_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Concluimos que la segunda función discriminante distingue los vinos del Tipo 2 de los vinos de Tipo 1 y 3.</p>
<p>Podemos obtener un diagrama de dispersión de las funciones discriminantes, etiquetando los puntos por clase:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine<span class="op">$</span>LD1 &lt;-<span class="st"> </span>wine_pred<span class="op">$</span>x[,<span class="dv">1</span>]
wine<span class="op">$</span>LD2 &lt;-<span class="st"> </span>wine_pred<span class="op">$</span>x[,<span class="dv">2</span>]
wine<span class="op">$</span>Type &lt;-<span class="st"> </span><span class="kw">as.factor</span>(wine<span class="op">$</span>Type)
<span class="kw">ggplot</span>(wine, <span class="kw">aes</span>(<span class="dt">x =</span> LD1, <span class="dt">y =</span> LD2, <span class="dt">color =</span> Type, <span class="dt">pch =</span> Type)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="12-lda-2_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p class="espacio">
</p>
<p>Notas:</p>
<ul>
<li><p>En el diagrama de dispersión de las dos primeras funciones discriminantes, podemos ver que los vinos de los tres cultivares están bien separados.</p></li>
<li><p>La primera función discriminante (eje x) separa muy bien los cultivares 1 y 3, pero no separa perfectamente los cultivares 1 y 3, ni los cultivares 2 y 3.</p></li>
<li><p>La segunda función discriminante (eje y) logra una separación bastante buena de los cultivares 1 y 3, y los cultivares 2 y 3, aunque no es totalmente perfecto.</p></li>
<li><p>Para lograr una muy buena separación de los tres cultivares, sería mejor usar juntas la primera y la segunda función discriminante, ya que la primera función discriminante puede separar los cultivares 1 y 3 muy bien, y la segunda función discriminante puede separar los cultivares 1 y 2, y los cultivares 2 y 3, razonablemente bien.</p></li>
</ul>
</div>
<div id="ejemplo-admisiones-al-mba" class="section level2">
<h2><span class="header-section-number">12.3</span> Ejemplo: admisiones al MBA</h2>
<p>Se tienen datos de admisión para los solicitantes a las escuelas de posgrado en administración de negocios. El objetivo es usar los puntajes de GPA y GMAT para predecir la probabilidad de admisión (admitido, no admitido y en el límite).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">url &lt;-<span class="st"> &#39;http://www.biz.uiowa.edu/faculty/jledolter/DataMining/admission.csv&#39;</span>
mba &lt;-<span class="st"> </span><span class="kw">read_csv</span>(url)
<span class="co">#&gt; Parsed with column specification:</span>
<span class="co">#&gt; cols(</span>
<span class="co">#&gt;   GPA = col_double(),</span>
<span class="co">#&gt;   GMAT = col_integer(),</span>
<span class="co">#&gt;   De = col_character()</span>
<span class="co">#&gt; )</span>
mba <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">GPA</th>
<th align="right">GMAT</th>
<th align="left">De</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3.26</td>
<td align="right">664</td>
<td align="left">admit</td>
</tr>
<tr class="even">
<td align="right">2.35</td>
<td align="right">321</td>
<td align="left">notadmit</td>
</tr>
<tr class="odd">
<td align="right">2.36</td>
<td align="right">399</td>
<td align="left">notadmit</td>
</tr>
<tr class="even">
<td align="right">2.57</td>
<td align="right">542</td>
<td align="left">notadmit</td>
</tr>
<tr class="odd">
<td align="right">2.85</td>
<td align="right">381</td>
<td align="left">notadmit</td>
</tr>
<tr class="even">
<td align="right">3.12</td>
<td align="right">463</td>
<td align="left">border</td>
</tr>
<tr class="odd">
<td align="right">3.44</td>
<td align="right">692</td>
<td align="left">admit</td>
</tr>
<tr class="even">
<td align="right">3.47</td>
<td align="right">552</td>
<td align="left">admit</td>
</tr>
<tr class="odd">
<td align="right">2.19</td>
<td align="right">411</td>
<td align="left">notadmit</td>
</tr>
<tr class="even">
<td align="right">2.51</td>
<td align="right">412</td>
<td align="left">notadmit</td>
</tr>
</tbody>
</table>
<p>Primero veamos un diagrama de dispersión de los datos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(mba, <span class="kw">aes</span>(<span class="dt">x=</span>GPA, <span class="dt">y =</span> GMAT, <span class="dt">color =</span> De)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="12-lda-2_files/figure-html/unnamed-chunk-11-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Comencemos haciendo el análisis de discriminante lineal, observemos que en este caso tenemos 3 clases:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lda</span>(De <span class="op">~</span><span class="st"> </span>., mba)
m1
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lda(De ~ ., data = mba)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Prior probabilities of groups:</span>
<span class="co">#&gt;    admit   border notadmit </span>
<span class="co">#&gt;    0.365    0.306    0.329 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group means:</span>
<span class="co">#&gt;           GPA GMAT</span>
<span class="co">#&gt; admit    3.40  561</span>
<span class="co">#&gt; border   2.99  446</span>
<span class="co">#&gt; notadmit 2.48  447</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients of linear discriminants:</span>
<span class="co">#&gt;          LD1     LD2</span>
<span class="co">#&gt; GPA  5.00877  1.8767</span>
<span class="co">#&gt; GMAT 0.00857 -0.0145</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Proportion of trace:</span>
<span class="co">#&gt;    LD1    LD2 </span>
<span class="co">#&gt; 0.9673 0.0327</span></code></pre></div>
<p>Para analizar podemos hacer una predicción para una nueva observación en específico:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(m1, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">GPA =</span> <span class="fl">3.21</span>, <span class="dt">GMAT =</span> <span class="dv">497</span>))
<span class="co">#&gt; $class</span>
<span class="co">#&gt; [1] admit</span>
<span class="co">#&gt; Levels: admit border notadmit</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $posterior</span>
<span class="co">#&gt;   admit border notadmit</span>
<span class="co">#&gt; 1 0.518  0.482 0.000356</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $x</span>
<span class="co">#&gt;    LD1   LD2</span>
<span class="co">#&gt; 1 1.25 0.318</span></code></pre></div>
<p>La predicción es muy ambigua. La razón es que con una sola función de discriminante no es posible separar los datos. La razón es que no se cumple el supuesto</p>
<p><span class="math display">\[
\Sigma_k = \Sigma\quad \mbox{para toda } k.
\]</span></p>
<p>Usamos análisis discriminante cuadrático:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">qda</span>(De <span class="op">~</span><span class="st"> </span>., mba)
m2
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; qda(De ~ ., data = mba)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Prior probabilities of groups:</span>
<span class="co">#&gt;    admit   border notadmit </span>
<span class="co">#&gt;    0.365    0.306    0.329 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group means:</span>
<span class="co">#&gt;           GPA GMAT</span>
<span class="co">#&gt; admit    3.40  561</span>
<span class="co">#&gt; border   2.99  446</span>
<span class="co">#&gt; notadmit 2.48  447</span></code></pre></div>
<p>Repetimos la predicción para el mismo punto:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(m2, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">GPA =</span> <span class="fl">3.21</span>, <span class="dt">GMAT =</span> <span class="dv">497</span>))
<span class="co">#&gt; $class</span>
<span class="co">#&gt; [1] admit</span>
<span class="co">#&gt; Levels: admit border notadmit</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $posterior</span>
<span class="co">#&gt;   admit border notadmit</span>
<span class="co">#&gt; 1 0.923 0.0769 0.000454</span></code></pre></div>
<p>¿Qué modelo es el mejor? Para responder a esta pregunta, evaluamos el análisis de discriminante lineal seleccionando aleatoriamente 60 de 85 estudiantes, estimando los parámetros en los datos y clasificando a los 25 estudiantes restantes de la muestra retenida.</p>
<p>Repetimos esto 100 veces primero con LDA:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">85</span>
nt &lt;-<span class="st"> </span><span class="dv">60</span>
neval &lt;-<span class="st"> </span>n <span class="op">-</span><span class="st"> </span>nt
rep &lt;-<span class="st"> </span><span class="dv">100</span>

<span class="kw">set.seed</span>(<span class="dv">123456</span>)
calcula_error &lt;-<span class="st"> </span><span class="cf">function</span>(i){
  muestra &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n, nt)
  m1 &lt;-<span class="st"> </span><span class="kw">lda</span>(De <span class="op">~</span><span class="st"> </span>., mba[muestra,])
  tablin &lt;-<span class="st"> </span><span class="kw">table</span>(mba<span class="op">$</span>De[<span class="op">-</span>muestra],<span class="kw">predict</span>(m1,mba[<span class="op">-</span>muestra,])<span class="op">$</span>class)
  <span class="kw">return</span>((neval<span class="op">-</span><span class="kw">sum</span>(<span class="kw">diag</span>(tablin)))<span class="op">/</span>neval)
}

merrlin &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(<span class="dt">.x =</span> <span class="dv">1</span><span class="op">:</span>rep, <span class="dt">.f =</span> calcula_error)
<span class="kw">mean</span>(merrlin)
<span class="co">#&gt; [1] 0.0964</span></code></pre></div>
<p>Ahora con QDA:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">calcula_error_Q &lt;-<span class="st"> </span><span class="cf">function</span>(i){
  muestra &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n, nt)
  m1 &lt;-<span class="st"> </span><span class="kw">qda</span>(De <span class="op">~</span><span class="st"> </span>., mba[muestra,])
  tablin &lt;-<span class="st"> </span><span class="kw">table</span>(mba<span class="op">$</span>De[<span class="op">-</span>muestra],<span class="kw">predict</span>(m1,mba[<span class="op">-</span>muestra,])<span class="op">$</span>class)
  <span class="kw">return</span>((neval<span class="op">-</span><span class="kw">sum</span>(<span class="kw">diag</span>(tablin)))<span class="op">/</span>neval)
}

qerrlin &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(<span class="dt">.x =</span> <span class="dv">1</span><span class="op">:</span>rep, <span class="dt">.f =</span> calcula_error_Q)
<span class="kw">mean</span>(qerrlin)
<span class="co">#&gt; [1] 0.0588</span></code></pre></div>
<p>Logramos una tasa de clasificación errónea del 9% en LDA y aproximadamente del 6% en QDA. En este caso ambos métodos funcionan bien porque las dimensiones del problema son más menores: <span class="math inline">\(p\)</span>, <span class="math inline">\(n\)</span>, y <span class="math inline">\(K\)</span> chicas.</p>
<p>R también nos da algunas herramientas de visualización. Por ejemplo el paquete <code>klaR</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(klaR)
mba<span class="op">$</span>De &lt;-<span class="st"> </span><span class="kw">as.factor</span>(mba<span class="op">$</span>De)
<span class="kw">partimat</span>(<span class="dt">formula =</span> De <span class="op">~</span><span class="st"> </span>GMAT <span class="op">+</span><span class="st"> </span>GPA, <span class="dt">data =</span> mba, <span class="dt">method =</span> <span class="st">&quot;lda&quot;</span>) </code></pre></div>
<p><img src="12-lda-2_files/figure-html/unnamed-chunk-18-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">partimat</span>(<span class="dt">formula =</span> De <span class="op">~</span><span class="st"> </span>GMAT <span class="op">+</span><span class="st"> </span>GPA, <span class="dt">data =</span> mba, <span class="dt">method =</span> <span class="st">&quot;qda&quot;</span>) </code></pre></div>
<p><img src="12-lda-2_files/figure-html/unnamed-chunk-19-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="repaso-1" class="section level2">
<h2><span class="header-section-number">12.4</span> Repaso</h2>
<p>Se tienen datos <span class="math inline">\(x_1, x_2, \ldots, x_n \in \mathbb{R}^p\)</span> y <span class="math inline">\(y_1,\ldots,y_n\in \{1,2,\ldots,K\}\)</span> donde</p>
<p><span class="math display">\[
\begin{eqnarray*}
n &amp;=&amp; \mbox{# observaciones} \\
p &amp;=&amp; \mbox{# de covariables o predictores} \\
k &amp;=&amp; \mbox{# clases que dividen a todos los datos.}
\end{eqnarray*}
\]</span></p>
<p>Definimos</p>
<p><span class="math display">\[
m_k = \dfrac{1}{N_k}\sum_{i\in C_k}x_i
\]</span> donde $N_k=kC_kk</p>
<p>Una medida de la variabilidad dentro de cada clase es</p>
<p><span class="math display">\[
s_k^2 = \sum_{i\in C_k}{(y_i - w^T x_i)^2}
\]</span> porque el modelo está determinado por la ecuación <span class="math inline">\(y(x)=w^Tx\)</span>.</p>
<div id="caso-k2" class="section level3 unnumbered">
<h3>Caso <span class="math inline">\(k=2\)</span></h3>
<p>El vector <span class="math inline">\(w\)</span> lo interpretamos como el vector de pesos que determinan el discriminante lineal.</p>
<p>La clasificación subsecuente se hace de tal forma que si <span class="math inline">\(y(x)&gt;y_0\)</span> se clasifica a la observación como perteneciente a la clase 1 y en otro caso a la clase 2.</p>
<p>Definimos el criterio de Fisher como</p>
<p><span class="math display">\[
J(w)  = \dfrac{w^T(m_2 - m_1)}{s_1^2 + s_2^2}.
\]</span></p>
<p>Buscamos <span class="math display">\[
\begin{eqnarray*}
\mbox{max} &amp;&amp; J(w) \\
\mbox{s.a.} &amp;\;&amp; \| w \| = 1.
\end{eqnarray*}
\]</span></p>
<p><strong>Idea:</strong> En general, necesitamos encontrar la dirección de <span class="math inline">\(w\)</span> y pensar en que luego es posible normalizar.</p>
<p>Definimos las matrices</p>
<p><span class="math display">\[
S_W = \sum_{i\in C_k}(x_i - m_1)(x_i-m_1)^T + \sum_{i\in C_k}(x_i - m_2)(x_i - m_2)^T
\]</span> y</p>
<p><span class="math display">\[
S_B = (m_2 - m_1)(m_2 - m_1)^T
\]</span></p>
<p>Se puede ver que</p>
<p><span class="math display">\[
J(w) = \dfrac{w^T S_B w}{w^T S_W w}.
\]</span></p>
<p class="espacio">
</p>

<div class="information">
Si <span class="math inline">\(x\)</span> es un vector y <span class="math inline">\(A\)</span> es una matriz, entonces el gradiente de una forma cuadrática <span class="math display">\[
\dfrac{1}{2}x^T A x + b^T x + c
\]</span> es <span class="math display">\[
Ax + b.
\]</span>
</div>

<p><br></p>
<p><br></p>
<p>Por lo tanto,</p>
<p><span class="math display">\[
\nabla J(w) = w^T S_B w \cdot \nabla_w(w^T S_W w) - w^T S_W w \cdot \nabla_w(w^T S_B w),
\]</span> por lo que <span class="math inline">\(J(w)\)</span> se maximiza cuando</p>
<p><span class="math display">\[
\begin{eqnarray*}
(w^T S_B w) S_W w &amp;=&amp; (w^T S_W w)S_B w \\
S_W w&amp;=&amp; \left(\dfrac{w^T S_W w}{w^T S_B w}\right) S_B w \\
&amp;=&amp; \dfrac{w^T S_W w}{w^T S_B w} (m_2-m_1)(m_2 - m_1)^T w\\
&amp;=&amp; \left\{\left(\dfrac{w^T S_W w}{w^T S_B w}\right)(m_2 - m_1)^T w\right\}(m_2-m_1).
\end{eqnarray*}
\]</span></p>
<p>Por lo tanto, <span class="math inline">\(w\)</span> es tal que</p>
<p><span class="math display">\[
w = c \cdot S_W ^{-1}(m_2 - m_1)
\]</span> donde <span class="math inline">\(c\)</span> es una constante tal que <span class="math inline">\(\|w\|_2^2=1\)</span>.</p>
</div>
</div>
<div id="supuestos-probabilisticos" class="section level2">
<h2><span class="header-section-number">12.5</span> Supuestos probabilísticos</h2>
<p>Definimos como <span class="math inline">\(\pi_k\)</span> la probabilidad inicial de la pertenencia a la clase <span class="math inline">\(k\)</span>, es decir,</p>
<p><span class="math display">\[
\pi_k = P(y_i = k).
\]</span></p>
<p>En la literatura las <span class="math inline">\(\pi_i\)</span>’s se conocen como <em>probabilidades a priori</em>. Además podemos definir una dsitribución de probabilidad para <span class="math inline">\(x\)</span> para cada clase <span class="math inline">\(k\)</span></p>
<p><span class="math display">\[
f_k(x) = P(x|k).
\]</span></p>
<p>Por el teorema de Bayes</p>
<p><span class="math display">\[
p(k|x) = \dfrac{f_k(x)\pi_k}{\sum_{l=1}^K f_l(x)\pi_l}.
\]</span></p>
<p>A las probabilidades finales de cada clase <span class="math inline">\(k\)</span> dada una observación <span class="math inline">\(x\)</span> se les conoce comúnmente como <em>probabilidades posteriores</em>.</p>
<p>En el análisis de discriminante lineal suponemos que <span class="math inline">\(p(x|k)\)</span> es la densidad normal</p>
<p><span class="math display">\[
f_k(x) = \dfrac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}\exp\left(-\dfrac{1}{2}(x-\mu_k)^T\Sigma_k ^{-1} (x-\mu_k)\right).
\]</span></p>
<p>En LDA suponemos que <span class="math inline">\(\Sigma_k = \Sigma\)</span> para toda <span class="math inline">\(k\)</span>. A este supuesto le llamamos <em>supuesto de homogeneidad</em>.</p>
<p>Para comparar la pertenencia de clases podemos ver el logaritmo del cociente entre las clases <span class="math inline">\(k\)</span> y <span class="math inline">\(l\)</span>:</p>
<p><span class="math display">\[
\begin{eqnarray*}
\log\dfrac{p(k|x)}{p(j|x)} &amp;=&amp; \log\dfrac{f_k(x)}{f_l(x)} + \log\left(\dfrac{\pi_k}{\pi_l}\right)\\
&amp;=&amp; \log\left(\dfrac{\pi_k}{\pi_l}\right) -\dfrac{1}{2}(\mu_k - \mu_l)^T\Sigma^{-1}(\mu_k - \mu_l) + x^T\Sigma^{-1}(\mu_k - \mu_l).
\end{eqnarray*}
\]</span></p>
<p>Ésta es una ecuación lineal en <span class="math inline">\(x\)</span>. Esto implica que la superficie de decisión (que separa a las clases) es un hiperplano de dimensión <span class="math inline">\(p\)</span>.</p>
<p>Si dividimos <span class="math inline">\(\mathbb{R}^p\)</span> entre regiones que separan las clases <span class="math inline">\(1,2,\ldots,K\)</span>, estas regiones están determinadas por hiperplanos.</p>
<p><img src="figuras/lda_3.png" width="70%" style="display: block; margin: auto;" /></p>
<p><br> <br></p>
<p>Definimos las funciones de discriminante lineal como</p>
<p><span class="math display">\[
f_k(x) = x^T \Sigma ^{-1} \mu_k -\dfrac{1}{2}\mu_k^T \Sigma^{-1}\mu_k + \log(\pi_k).
\]</span></p>
<p>Estos los estimamos por máxima verosimilitud y es equivalente a la vista antes</p>
<p><span class="math display">\[
\hat{\pi}_k= \dfrac{N_k{}}N
\]</span></p>
<p><span class="math display">\[
\hat{\mu}_k = \dfrac{1}{N_k}\sum_{i\in C_k}x_i
\]</span></p>
<p><span class="math display">\[
\hat{\Sigma} = \dfrac{1}{N-K} \sum_{k=1}^{K}\sum_{i\in C_k}(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^T
\]</span></p>
<p>Si <span class="math inline">\(\Sigma_k\)</span> no es igual a <span class="math inline">\(\Sigma\)</span> para todas las clases entonces definimos las <em>funciones de discriminante cuadráticas</em> (QDA) como</p>
<p><span class="math display">\[
f_k(x) = -\dfrac{1}{2}\log|\Sigma_k| - \dfrac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log(\pi_k).
\]</span></p>
<p>Las superficies de decisión son ecuaciones cuadráticas en <span class="math inline">\(\mathbb{R}^p\)</span>.</p>

<div class="nota">
<p><strong>Nota</strong></p>
<p class="espacio3">
</p>
<ol style="list-style-type: decimal">
<li><p>Las estimaciones se hacen de manera similar que para LDA excepto que en este caso se deben estimar todas las <span class="math inline">\(\Sigma_k\)</span>`s por separado.</p></li>
<li>El número de parámetros puede aumentar sustancialmente.</li>
</ol>
</div>

</div>
<div id="relacion-con-minimos-cuadrados" class="section level2">
<h2><span class="header-section-number">12.6</span> Relación con mínimos cuadrados</h2>
<p>Vimos cómo utilizar regresión lineal para encontrar regiones de discriminante lineal. Codificamos <span class="math inline">\(Y\)</span> de la siguiente forma:</p>
<p><span class="math display">\[
y_i=(0,\ldots,0,N/N_1,0,\ldots,0)
\]</span></p>
<p>donde la posición distinta de <span class="math inline">\(0\)</span> es la <span class="math inline">\(i\)</span>-ésima posición correspondiente a la observación cuando está en la clase 1, y cuando está en la clase 2 es</p>
<p><span class="math display">\[
y_i=(0,\ldots,0,-N/N_2,0,\ldots,0).
\]</span></p>
<p>El error como suma de cuadrados es:</p>
<p><span class="math display">\[
\dfrac{1}{2}\sum_{i=1}^n{(w^Tx_i+w_0-y_i)^2}.
\]</span></p>
<p>Derivando con respecto a <span class="math inline">\(w_0\)</span> y <span class="math inline">\(w\)</span> obtenemos:</p>
<p><span class="math display">\[
\begin{eqnarray*}
\sum_{i=1}^n{(w^Tx_i+w_0-y_i)} &amp;=&amp; 0\\
\sum_{i=1}^n{(w^Tx_i+w_0-y_i)\cdot x_i} &amp;=&amp; 0
\end{eqnarray*}
\]</span></p>
<p>Sustituyendo las <span class="math inline">\(y_i\)</span>`s obtenemos que</p>
<p><span class="math display">\[
w_0 = -w^Tm
\]</span></p>
<p>donde</p>
<p><span class="math display">\[
\sum_{i=1}^n{y_i} = N_1\dfrac{N}{N_1} - N_2\dfrac{N}{N_2},
\]</span></p>
<p>y <span class="math inline">\(m\)</span> es la media del conjunto de datos y está dado por</p>
<p><span class="math display">\[
m = \dfrac{1}{N}\sum_{i=1}^N x_i = \dfrac{1}{N}(N_1 m_1 + N_2 m_2).
\]</span></p>
<p>Se puede demostrar (tarea) que:</p>
<p><span class="math display">\[
\left(S_W + \dfrac{N_1N_2}{N}S_B\right)w=N(m_1 - m_2).
\]</span></p>
<p>Notemos que <span class="math inline">\(S_B w\)</span> está en la dirección de <span class="math inline">\(m_2-m_1\)</span> porque</p>
<p><span class="math display">\[
S_B = (m_2 - m_1)(m_2 - m_1)^T,
\]</span></p>
<p>y entonces <span class="math inline">\(w\)</span> es proporcional a <span class="math inline">\(S_W^{-1}(m_2 - m_1)\)</span>.</p>
<p>Por lo tanto, el resultado obtenido por mínimos cuadrados es equivalente a la dirección de discriminante lineal, ignorando factores escalares que no son relevantes. Por lo tanto, el vector de pesos coincide con el encontrado mediante el criterio de Fisher.</p>
<p>El umbral <span class="math inline">\(w_0\)</span> es tal que si <span class="math inline">\(y(x) = w^T(x-m)&gt; 0\)</span>, entonces se clasifica en la clase 1, y en otro caso, se clasifica en la clase 2.</p>

<div class="information">
<strong>Nota:</strong> La extensión para múltiples clases es análoga a lo que ya hemos visto.
</div>

<p><br></p>
</div>
<div id="tarea-10" class="section level2">
<h2><span class="header-section-number">12.7</span> Tarea</h2>
<p>Utilizando las definiciones de las matrices de covarianza <em>intraclase</em> <span class="math inline">\(S_W\)</span> e <em>interclase</em> <span class="math inline">\(S_B\)</span> y que</p>
<p><span class="math display">\[
w_0 = -w^tm,
\]</span> donde <span class="math display">\[
m = \dfrac{1}{N}\sum_{i=1}^N x_i = \dfrac{1}{N}(N_1 m_1 + N_2 m_2),
\]</span> demuestra que</p>
<p><span class="math display">\[
\sum_{i=1}^n{(w^Tx_i+w_0-y_i)\cdot x_i}= 0
\]</span></p>
<p>se puede escribir de la forma</p>
<p><span class="math display">\[
\left(S_W + \dfrac{N_1N_2}{N}S_B\right)w=N(m_1 - m_2).
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="analisis-de-discriminante-lineal-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="componentes-principales-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
