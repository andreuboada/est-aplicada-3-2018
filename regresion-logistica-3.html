<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Estadística Aplicada III</title>
  <meta name="description" content="Notas y material para el curso de Estadística Aplicada III, 2018 (ITAM)">
  <meta name="generator" content="bookdown 0.7.8 and GitBook 2.6.7">

  <meta property="og:title" content="Estadística Aplicada III" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas y material para el curso de Estadística Aplicada III, 2018 (ITAM)" />
  <meta name="github-repo" content="andreuboada/est-aplicada-3-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Estadística Aplicada III" />
  
  <meta name="twitter:description" content="Notas y material para el curso de Estadística Aplicada III, 2018 (ITAM)" />
  

<meta name="author" content="Andreu Boada de Atela">


<meta name="date" content="2018-04-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regresion-logistica-2.html">
<link rel="next" href="regularizacion.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Estadística Aplicada III</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario y referencias</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#evaluacion"><i class="fa fa-check"></i>Evaluación</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-r-y-rstudio"><i class="fa fa-check"></i>Software: R y Rstudio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias-principales"><i class="fa fa-check"></i>Referencias principales</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#otras-referencias"><i class="fa fa-check"></i>Otras referencias</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#tareas"><i class="fa fa-check"></i>Tareas</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#por-que-un-analisis-multivariado"><i class="fa fa-check"></i><b>1.1</b> ¿Por qué un análisis multivariado?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#la-paradoja-de-simpson"><i class="fa fa-check"></i><b>1.2</b> La paradoja de Simpson</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#modelos-log-lineales"><i class="fa fa-check"></i><b>1.3</b> Modelos log-lineales</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#interpretacion-de-parametros"><i class="fa fa-check"></i><b>1.4</b> Interpretación de parámetros</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#ejemplo-dos-monedas"><i class="fa fa-check"></i><b>1.4.1</b> Ejemplo: dos monedas</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#otros-ejemplos"><i class="fa fa-check"></i><b>1.5</b> Otros ejemplos</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#discriminacion-de-residentes-hispanos-con-discapacidades"><i class="fa fa-check"></i><b>1.5.1</b> Discriminación de residentes hispanos con discapacidades</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#consumo-de-chocolate-y-premios-nobel"><i class="fa fa-check"></i><b>1.5.2</b> Consumo de chocolate y premios Nobel</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#tarea"><i class="fa fa-check"></i><b>1.6</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Rintro.html"><a href="Rintro.html"><i class="fa fa-check"></i><b>2</b> Temas selectos de R</a><ul>
<li class="chapter" data-level="2.1" data-path="Rintro.html"><a href="Rintro.html#que-ventajas-tiene-r"><i class="fa fa-check"></i><b>2.1</b> ¿Qué ventajas tiene R?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="Rintro.html"><a href="Rintro.html#r-es-gratuito-y-de-codigo-abierto"><i class="fa fa-check"></i><b>2.1.1</b> R es gratuito y de código abierto</a></li>
<li class="chapter" data-level="2.1.2" data-path="Rintro.html"><a href="Rintro.html#r-tiene-una-comunidad-comprometida"><i class="fa fa-check"></i><b>2.1.2</b> R tiene una comunidad comprometida</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="Rintro.html"><a href="Rintro.html#flujo-basico-de-trabajo-para-el-analisis-de-datos-en-r."><i class="fa fa-check"></i><b>2.2</b> Flujo básico de trabajo para el análisis de datos en R.</a></li>
<li class="chapter" data-level="2.3" data-path="Rintro.html"><a href="Rintro.html#introduccion-a-r-como-lenguaje-de-programacion-y-la-plataforma-interactiva-de-rstudio."><i class="fa fa-check"></i><b>2.3</b> Introducción a R como lenguaje de programación, y la plataforma interactiva de RStudio.</a><ul>
<li class="chapter" data-level="2.3.1" data-path="Rintro.html"><a href="Rintro.html#como-entender-r"><i class="fa fa-check"></i><b>2.3.1</b> ¿Cómo entender R?</a></li>
<li class="chapter" data-level="2.3.2" data-path="Rintro.html"><a href="Rintro.html#por-que-r"><i class="fa fa-check"></i><b>2.3.2</b> ¿Por qué R?</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="Rintro.html"><a href="Rintro.html#estructuras-de-datos"><i class="fa fa-check"></i><b>2.4</b> Estructuras de datos</a><ul>
<li class="chapter" data-level="2.4.1" data-path="Rintro.html"><a href="Rintro.html#vectores"><i class="fa fa-check"></i><b>2.4.1</b> Vectores</a></li>
<li class="chapter" data-level="2.4.2" data-path="Rintro.html"><a href="Rintro.html#data-frames"><i class="fa fa-check"></i><b>2.4.2</b> Data Frames</a></li>
<li class="chapter" data-level="2.4.3" data-path="Rintro.html"><a href="Rintro.html#listas"><i class="fa fa-check"></i><b>2.4.3</b> Listas</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="Rintro.html"><a href="Rintro.html#r-markdown"><i class="fa fa-check"></i><b>2.5</b> R Markdown</a><ul>
<li class="chapter" data-level="2.5.1" data-path="Rintro.html"><a href="Rintro.html#que-es-r-markdown"><i class="fa fa-check"></i><b>2.5.1</b> ¿Qué es R Markdown?</a></li>
<li class="chapter" data-level="2.5.2" data-path="Rintro.html"><a href="Rintro.html#estructura-basica-de-r-markdown"><i class="fa fa-check"></i><b>2.5.2</b> Estructura básica de R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="Rintro.html"><a href="Rintro.html#proyectos-de-rstudio"><i class="fa fa-check"></i><b>2.6</b> Proyectos de RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="Rintro.html"><a href="Rintro.html#otros-aspectos-importantes-de-r"><i class="fa fa-check"></i><b>2.7</b> Otros aspectos importantes de R</a><ul>
<li class="chapter" data-level="2.7.1" data-path="Rintro.html"><a href="Rintro.html#valores-faltantes"><i class="fa fa-check"></i><b>2.7.1</b> Valores faltantes</a></li>
<li class="chapter" data-level="2.7.2" data-path="Rintro.html"><a href="Rintro.html#funciones"><i class="fa fa-check"></i><b>2.7.2</b> Funciones</a></li>
<li class="chapter" data-level="2.7.3" data-path="Rintro.html"><a href="Rintro.html#funcionales"><i class="fa fa-check"></i><b>2.7.3</b> Funcionales</a></li>
<li class="chapter" data-level="2.7.4" data-path="Rintro.html"><a href="Rintro.html#rendimiento-en-r"><i class="fa fa-check"></i><b>2.7.4</b> Rendimiento en R</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="Rintro.html"><a href="Rintro.html#tarea-1"><i class="fa fa-check"></i><b>2.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html"><i class="fa fa-check"></i><b>3</b> Manipulación y visualización de datos</a><ul>
<li class="chapter" data-level="3.1" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#el-principio-de-datos-limpios"><i class="fa fa-check"></i><b>3.1</b> El principio de datos limpios</a></li>
<li class="chapter" data-level="3.2" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#limpieza-de-datos"><i class="fa fa-check"></i><b>3.2</b> Limpieza de datos</a></li>
<li class="chapter" data-level="3.3" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#separa-aplica-combina"><i class="fa fa-check"></i><b>3.3</b> <em>Separa-aplica-combina</em></a></li>
<li class="chapter" data-level="3.4" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#muertes-por-armas-de-fuego-en-eua"><i class="fa fa-check"></i><b>3.4</b> Muertes por armas de fuego en EUA</a></li>
<li class="chapter" data-level="3.5" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#el-cuarteto-de-anscombe"><i class="fa fa-check"></i><b>3.5</b> El Cuarteto de Anscombe</a></li>
<li class="chapter" data-level="3.6" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#the-grammar-of-graphics-de-leland-wilkinson"><i class="fa fa-check"></i><b>3.6</b> The <em>Grammar of Graphics</em> de Leland Wilkinson</a></li>
<li class="chapter" data-level="3.7" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#ggplot"><i class="fa fa-check"></i><b>3.7</b> ggplot</a></li>
<li class="chapter" data-level="3.8" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#un-histograma-de-las-muertes-en-iraq"><i class="fa fa-check"></i><b>3.8</b> Un histograma de las muertes en Iraq</a></li>
<li class="chapter" data-level="3.9" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#inglehartwelzel-un-mapa-cultural-del-mundo"><i class="fa fa-check"></i><b>3.9</b> Inglehart–Welzel: un mapa cultural del mundo</a><ul>
<li class="chapter" data-level="3.9.1" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#creando-un-ggplot"><i class="fa fa-check"></i><b>3.9.1</b> Creando un ggplot</a></li>
<li class="chapter" data-level="3.9.2" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#mapeos-aesthetics"><i class="fa fa-check"></i><b>3.9.2</b> Mapeos: Aesthetics</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#poniendo-todo-junto"><i class="fa fa-check"></i><b>3.10</b> Poniendo todo junto</a></li>
<li class="chapter" data-level="3.11" data-path="manipulacion-y-visualizacion-de-datos.html"><a href="manipulacion-y-visualizacion-de-datos.html#tarea-2"><i class="fa fa-check"></i><b>3.11</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html"><i class="fa fa-check"></i><b>4</b> Teorema del Límite Central</a><ul>
<li class="chapter" data-level="4.1" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#la-distribucion-de-la-media"><i class="fa fa-check"></i><b>4.1</b> La distribución de la media</a></li>
<li class="chapter" data-level="4.2" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#de-donde-proviene-la-distribucion-normal"><i class="fa fa-check"></i><b>4.2</b> ¿De dónde proviene la distribución normal?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#que-signo-tiene-c"><i class="fa fa-check"></i><b>4.2.1</b> ¿Qué signo tiene c?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#otras-observaciones"><i class="fa fa-check"></i><b>4.3</b> Otras observaciones</a></li>
<li class="chapter" data-level="4.4" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#diagramas-de-caja-y-brazos"><i class="fa fa-check"></i><b>4.4</b> Diagramas de caja y brazos</a><ul>
<li class="chapter" data-level="" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#graficas-de-cuantiles-teoricos"><i class="fa fa-check"></i><b>4.5</b> Gráficas de cuantiles teóricos</a><ul>
<li class="chapter" data-level="" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo-normal"><i class="fa fa-check"></i>Ejemplo: normal</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#graficas-de-cuantiles-para-un-conjunto-de-datos"><i class="fa fa-check"></i><b>4.6</b> Gráficas de cuantiles para un conjunto de datos</a><ul>
<li class="chapter" data-level="4.6.1" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#que-buscar-en-una-grafica-de-cuantiles"><i class="fa fa-check"></i><b>4.6.1</b> ¿Qué buscar en una gráfica de cuantiles?</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#graficas-qq-normales"><i class="fa fa-check"></i><b>4.7</b> Gráficas qq-normales</a><ul>
<li class="chapter" data-level="" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo-cantantes"><i class="fa fa-check"></i>Ejemplo: cantantes</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#el-tlc-y-errores-estandar"><i class="fa fa-check"></i><b>4.8</b> El TLC y errores estándar</a></li>
<li class="chapter" data-level="4.9" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#ejemplo-1"><i class="fa fa-check"></i><b>4.9</b> Ejemplo</a></li>
<li class="chapter" data-level="4.10" data-path="teorema-del-limite-central.html"><a href="teorema-del-limite-central.html#tarea-3"><i class="fa fa-check"></i><b>4.10</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html"><i class="fa fa-check"></i><b>5</b> Análisis de datos categóricos</a><ul>
<li class="chapter" data-level="5.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#repaso-y-algunos-conceptos"><i class="fa fa-check"></i><b>5.1</b> Repaso y algunos conceptos</a><ul>
<li class="chapter" data-level="5.1.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#caso-binomial"><i class="fa fa-check"></i><b>5.1.1</b> Caso binomial</a></li>
<li class="chapter" data-level="" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#estimacion-de-parametros-multinomiales"><i class="fa fa-check"></i>Estimación de parámetros multinomiales</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#la-chi2-de-pearson-de-una-multinomial"><i class="fa fa-check"></i><b>5.2</b> La <span class="math inline">\(\chi^2\)</span> de Pearson de una multinomial</a><ul>
<li class="chapter" data-level="5.2.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#cociente-de-verosimilitud-de-una-multinomial"><i class="fa fa-check"></i><b>5.2.1</b> Cociente de verosimilitud de una multinomial</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#definiciones"><i class="fa fa-check"></i><b>5.3</b> Definiciones</a><ul>
<li class="chapter" data-level="5.3.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#notacion"><i class="fa fa-check"></i><b>5.3.1</b> Notación</a></li>
<li class="chapter" data-level="5.3.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#razon-de-momios"><i class="fa fa-check"></i><b>5.3.2</b> Razón de momios</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#asociacion-en-tablas-de-tamano-itimes-j"><i class="fa fa-check"></i><b>5.4</b> Asociación en tablas de tamaño <span class="math inline">\(I\times J\)</span></a><ul>
<li class="chapter" data-level="5.4.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#razones-de-momios-en-tablas-itimes-j"><i class="fa fa-check"></i><b>5.4.1</b> Razones de momios en tablas <span class="math inline">\(I\times J\)</span></a></li>
<li class="chapter" data-level="5.4.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-mushrooms"><i class="fa fa-check"></i><b>5.4.2</b> Ejemplo: mushrooms</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#intervalos-de-confianza-para-los-parametros-de-asociacion"><i class="fa fa-check"></i><b>5.5</b> Intervalos de confianza para los parámetros de asociación</a><ul>
<li class="chapter" data-level="5.5.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#error-estandar-de-la-razon-de-momios"><i class="fa fa-check"></i><b>5.5.1</b> Error estándar de la razón de momios</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#prueba-de-independencia"><i class="fa fa-check"></i><b>5.6</b> Prueba de independencia</a><ul>
<li class="chapter" data-level="5.6.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#la-prueba-chi2-de-pearson"><i class="fa fa-check"></i><b>5.6.1</b> La prueba <span class="math inline">\(\chi^2\)</span> de Pearson</a></li>
<li class="chapter" data-level="5.6.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-brecha-de-genero"><i class="fa fa-check"></i><b>5.6.2</b> Ejemplo: brecha de género</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#general-social-survey-1972---2016"><i class="fa fa-check"></i><b>5.7</b> General Social Survey 1972 - 2016</a></li>
<li class="chapter" data-level="5.8" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#la-catadora-de-te"><i class="fa fa-check"></i><b>5.8</b> La catadora de té</a></li>
<li class="chapter" data-level="5.9" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#modelos-multinomiales-para-conteos"><i class="fa fa-check"></i><b>5.9</b> Modelos multinomiales para conteos</a></li>
<li class="chapter" data-level="5.10" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#modelos-log-lineales-con-tres-variables-categoricas"><i class="fa fa-check"></i><b>5.10</b> Modelos log lineales con tres variables categóricas</a><ul>
<li class="chapter" data-level="5.10.1" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#tipos-de-independencia"><i class="fa fa-check"></i><b>5.10.1</b> Tipos de independencia</a></li>
<li class="chapter" data-level="5.10.2" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#asociacion-homogenea-e-interacciones-de-3-factores"><i class="fa fa-check"></i><b>5.10.2</b> Asociación homogénea e interacciones de 3 factores</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-sensitividad-y-especificidad"><i class="fa fa-check"></i><b>5.11</b> Ejemplo: sensitividad y especificidad</a></li>
<li class="chapter" data-level="5.12" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#ejemplo-horoscopos"><i class="fa fa-check"></i><b>5.12</b> Ejemplo: horóscopos</a></li>
<li class="chapter" data-level="5.13" data-path="analisis-de-datos-categoricos.html"><a href="analisis-de-datos-categoricos.html#tarea-opcional"><i class="fa fa-check"></i><b>5.13</b> Tarea (opcional)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html"><i class="fa fa-check"></i><b>6</b> Regresión logística 1</a><ul>
<li class="chapter" data-level="6.1" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#regresion-logistica-con-un-solo-predictor"><i class="fa fa-check"></i><b>6.1</b> Regresión logística con un solo predictor</a></li>
<li class="chapter" data-level="6.2" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#el-modelo-de-regresion-logistica"><i class="fa fa-check"></i><b>6.2</b> El modelo de regresión logística</a><ul>
<li class="chapter" data-level="6.2.1" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#funcion-logistica"><i class="fa fa-check"></i><b>6.2.1</b> Función logística</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#ejemplo-2"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regresion-logistica-1.html"><a href="regresion-logistica-1.html#tarea-4"><i class="fa fa-check"></i><b>6.3</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html"><i class="fa fa-check"></i><b>7</b> Regresión logística 2</a><ul>
<li class="chapter" data-level="7.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#incertidumbre-en-la-estimacion"><i class="fa fa-check"></i><b>7.1</b> Incertidumbre en la estimación</a></li>
<li class="chapter" data-level="7.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#funcion-logistica-1"><i class="fa fa-check"></i><b>7.2</b> Función logística</a></li>
<li class="chapter" data-level="7.3" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#interpretacion-de-los-coeficientes"><i class="fa fa-check"></i><b>7.3</b> Interpretación de los coeficientes</a><ul>
<li class="chapter" data-level="7.3.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#evaluar-en-o-alrededor-de-la-media"><i class="fa fa-check"></i><b>7.3.1</b> Evaluar en (o alrededor de) la media</a></li>
<li class="chapter" data-level="7.3.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#la-regla-de-dividir-entre-4"><i class="fa fa-check"></i><b>7.3.2</b> La regla de “dividir entre 4”</a></li>
<li class="chapter" data-level="7.3.3" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#interpretacion-de-los-coeficientes-como-cocientes-de-momios"><i class="fa fa-check"></i><b>7.3.3</b> Interpretación de los coeficientes como cocientes de momios</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ejemplo-pozos-en-bangladesh"><i class="fa fa-check"></i><b>7.4</b> Ejemplo: pozos en Bangladesh</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#descripcion-del-problema"><i class="fa fa-check"></i><b>7.4.1</b> Descripción del problema</a></li>
<li class="chapter" data-level="7.4.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#antecedentes-del-problema"><i class="fa fa-check"></i><b>7.4.2</b> Antecedentes del problema</a></li>
<li class="chapter" data-level="7.4.3" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#metodologia-para-abordar-el-problema"><i class="fa fa-check"></i><b>7.4.3</b> Metodología para abordar el problema</a></li>
<li class="chapter" data-level="7.4.4" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ajuste-y-resultados-del-modelo"><i class="fa fa-check"></i><b>7.4.4</b> Ajuste y resultados del modelo</a></li>
<li class="chapter" data-level="7.4.5" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#interpretacion-de-los-coeficientes-1"><i class="fa fa-check"></i><b>7.4.5</b> Interpretación de los coeficientes</a></li>
<li class="chapter" data-level="7.4.6" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#agregamos-una-segunda-variable-de-entrada"><i class="fa fa-check"></i><b>7.4.6</b> Agregamos una segunda variable de entrada</a></li>
<li class="chapter" data-level="7.4.7" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#comparacion-de-coeficientes-cuando-anades-un-predictor"><i class="fa fa-check"></i><b>7.4.7</b> Comparación de coeficientes cuando añades un predictor</a></li>
<li class="chapter" data-level="7.4.8" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#graficar-el-modelo-ajustado-con-dos-predictores"><i class="fa fa-check"></i><b>7.4.8</b> Graficar el modelo ajustado con dos predictores</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ajuste-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>7.5</b> Ajuste de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="7.6" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>7.6</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="7.6.1" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>7.6.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="7.6.2" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>7.6.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#ejemplo-diabetes"><i class="fa fa-check"></i><b>7.7</b> Ejemplo: diabetes</a></li>
<li class="chapter" data-level="7.8" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#observaciones-adicionales"><i class="fa fa-check"></i><b>7.8</b> Observaciones adicionales</a></li>
<li class="chapter" data-level="7.9" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>7.9</b> Regresión logística para problemas de más de 2 clases</a></li>
<li class="chapter" data-level="7.10" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>7.10</b> Regresión logística multinomial</a></li>
<li class="chapter" data-level="7.11" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#identificabilidad-y-separacion"><i class="fa fa-check"></i><b>7.11</b> Identificabilidad y separación</a></li>
<li class="chapter" data-level="7.12" data-path="regresion-logistica-2.html"><a href="regresion-logistica-2.html#tarea-5"><i class="fa fa-check"></i><b>7.12</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html"><i class="fa fa-check"></i><b>8</b> Regresión logística 3</a><ul>
<li class="chapter" data-level="8.1" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#ejemplo-oscares"><i class="fa fa-check"></i><b>8.1</b> Ejemplo óscares</a></li>
<li class="chapter" data-level="8.2" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#repaso-de-regresion-logistica"><i class="fa fa-check"></i><b>8.2</b> Repaso de regresión logística</a></li>
<li class="chapter" data-level="8.3" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#regresion-logistica-con-interacciones"><i class="fa fa-check"></i><b>8.3</b> Regresión logística con interacciones</a></li>
<li class="chapter" data-level="8.4" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#graficas-del-modelo-con-interacciones"><i class="fa fa-check"></i><b>8.4</b> Gráficas del modelo con interacciones</a></li>
<li class="chapter" data-level="8.5" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#agregar-mas-predictores"><i class="fa fa-check"></i><b>8.5</b> Agregar más predictores</a></li>
<li class="chapter" data-level="8.6" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#evaluacion-de-modelos-de-regresion-logistica"><i class="fa fa-check"></i><b>8.6</b> Evaluación de modelos de regresión logística</a><ul>
<li class="chapter" data-level="8.6.1" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#graficas-de-residuales-agrupados-vs-predictores"><i class="fa fa-check"></i><b>8.6.1</b> Gráficas de residuales agrupados vs predictores</a></li>
<li class="chapter" data-level="8.6.2" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#transformaciones"><i class="fa fa-check"></i><b>8.6.2</b> Transformaciones</a></li>
<li class="chapter" data-level="8.6.3" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#tasa-de-error-y-comparacion-contra-el-modelo-nulo"><i class="fa fa-check"></i><b>8.6.3</b> Tasa de error y comparación contra el modelo nulo</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#diferencias-predictivas-promedio-en-la-escala-de-probabilidad"><i class="fa fa-check"></i><b>8.7</b> Diferencias predictivas promedio en la escala de probabilidad</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#diferencias-predictivas-promedio-en-presencia-de-interacciones"><i class="fa fa-check"></i>Diferencias predictivas promedio en presencia de interacciones</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#notacion-general-para-diferencias-predictivas"><i class="fa fa-check"></i>Notación general para diferencias predictivas</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="regresion-logistica-3.html"><a href="regresion-logistica-3.html#tarea-6"><i class="fa fa-check"></i><b>8.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regularizacion.html"><a href="regularizacion.html"><i class="fa fa-check"></i><b>9</b> Regularización</a><ul>
<li class="chapter" data-level="9.1" data-path="regularizacion.html"><a href="regularizacion.html#repaso"><i class="fa fa-check"></i><b>9.1</b> Repaso</a></li>
<li class="chapter" data-level="9.2" data-path="regularizacion.html"><a href="regularizacion.html#otras-medidas-de-clasificacion"><i class="fa fa-check"></i><b>9.2</b> Otras medidas de clasificación</a></li>
<li class="chapter" data-level="9.3" data-path="regularizacion.html"><a href="regularizacion.html#analisis-de-error-en-clasificacion-binaria"><i class="fa fa-check"></i><b>9.3</b> Análisis de error en clasificación binaria</a><ul>
<li class="chapter" data-level="9.3.1" data-path="regularizacion.html"><a href="regularizacion.html#punto-de-corte-para-un-clasificador-binario"><i class="fa fa-check"></i><b>9.3.1</b> Punto de corte para un clasificador binario</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="regularizacion.html"><a href="regularizacion.html#curvas-roc"><i class="fa fa-check"></i><b>9.4</b> Curvas ROC</a><ul>
<li class="chapter" data-level="9.4.1" data-path="regularizacion.html"><a href="regularizacion.html#espacio-roc"><i class="fa fa-check"></i><b>9.4.1</b> Espacio ROC</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-1"><i class="fa fa-check"></i><b>9.5</b> Regularización</a><ul>
<li class="chapter" data-level="9.5.1" data-path="regularizacion.html"><a href="regularizacion.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>9.5.1</b> Reduciendo varianza de los coeficientes</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-ridge"><i class="fa fa-check"></i><b>9.6</b> Regularización Ridge</a><ul>
<li class="chapter" data-level="9.6.1" data-path="regularizacion.html"><a href="regularizacion.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>9.6.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-lasso"><i class="fa fa-check"></i><b>9.7</b> Regularización Lasso</a></li>
<li class="chapter" data-level="9.8" data-path="regularizacion.html"><a href="regularizacion.html#tarea-7"><i class="fa fa-check"></i><b>9.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>10</b> Modelos lineales generalizados</a><ul>
<li class="chapter" data-level="10.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresion-lineal-y-logistica"><i class="fa fa-check"></i><b>10.1</b> Regresión lineal y logística</a></li>
<li class="chapter" data-level="10.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#otros-modelos"><i class="fa fa-check"></i><b>10.2</b> Otros modelos</a></li>
<li class="chapter" data-level="10.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-accidentes-de-trafico"><i class="fa fa-check"></i><b>10.3</b> Ejemplo: accidentes de tráfico</a></li>
<li class="chapter" data-level="10.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#interpretacion-de-coeficientes-poisson"><i class="fa fa-check"></i><b>10.4</b> Interpretación de coeficientes Poisson</a></li>
<li class="chapter" data-level="10.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#diferencias-entre-el-modelo-binomial-y-poisson"><i class="fa fa-check"></i><b>10.5</b> Diferencias entre el modelo binomial y Poisson</a></li>
<li class="chapter" data-level="10.6" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-fertilidad-en-fiji"><i class="fa fa-check"></i><b>10.6</b> Ejemplo: fertilidad en Fiji</a></li>
<li class="chapter" data-level="10.7" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#variable-de-expuestos-offset"><i class="fa fa-check"></i><b>10.7</b> Variable de expuestos (offset)</a></li>
<li class="chapter" data-level="10.8" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplos-seguros"><i class="fa fa-check"></i><b>10.8</b> Ejemplos: seguros</a><ul>
<li class="chapter" data-level="" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#numero-de-expuestos-interpretacion"><i class="fa fa-check"></i>Número de expuestos (interpretación)</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-arboles"><i class="fa fa-check"></i><b>10.9</b> Ejemplo: árboles</a></li>
<li class="chapter" data-level="10.10" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#sobredispersion"><i class="fa fa-check"></i><b>10.10</b> Sobredispersión</a></li>
<li class="chapter" data-level="10.11" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-numero-de-publicaciones"><i class="fa fa-check"></i><b>10.11</b> Ejemplo: número de publicaciones</a></li>
<li class="chapter" data-level="10.12" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#tarea-8"><i class="fa fa-check"></i><b>10.12</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html"><i class="fa fa-check"></i><b>11</b> Análisis de Discriminante Lineal 1</a><ul>
<li class="chapter" data-level="11.1" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#problemas-de-clasificacion"><i class="fa fa-check"></i><b>11.1</b> Problemas de clasificación</a></li>
<li class="chapter" data-level="11.2" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#funciones-de-discriminante"><i class="fa fa-check"></i><b>11.2</b> Funciones de discriminante</a></li>
<li class="chapter" data-level="11.3" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#regresion-lineal-en-una-matriz-indicadora"><i class="fa fa-check"></i><b>11.3</b> Regresión lineal en una matriz indicadora</a></li>
<li class="chapter" data-level="11.4" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#discriminante-lineal-de-fisher"><i class="fa fa-check"></i><b>11.4</b> Discriminante lineal de Fisher</a><ul>
<li class="chapter" data-level="11.4.1" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#ejemplo-separacion-entre-clases"><i class="fa fa-check"></i><b>11.4.1</b> Ejemplo: separación entre clases</a></li>
<li class="chapter" data-level="11.4.2" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#ejemplo-iris-de-fisher"><i class="fa fa-check"></i><b>11.4.2</b> Ejemplo: iris de Fisher</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="analisis-de-discriminante-lineal-1.html"><a href="analisis-de-discriminante-lineal-1.html#tarea-9"><i class="fa fa-check"></i><b>11.5</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html"><i class="fa fa-check"></i><b>12</b> Análisis de Discriminante Lineal 2</a><ul>
<li class="chapter" data-level="12.1" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#aplicaciones"><i class="fa fa-check"></i><b>12.1</b> Aplicaciones</a></li>
<li class="chapter" data-level="12.2" data-path="analisis-de-discriminante-lineal-2.html"><a href="analisis-de-discriminante-lineal-2.html#ejemplo-vinos"><i class="fa fa-check"></i><b>12.2</b> Ejemplo: vinos</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ejemplo-admisiones-al-mba.html"><a href="ejemplo-admisiones-al-mba.html"><i class="fa fa-check"></i><b>13</b> Ejemplo: admisiones al MBA</a><ul>
<li class="chapter" data-level="13.1" data-path="ejemplo-admisiones-al-mba.html"><a href="ejemplo-admisiones-al-mba.html#tarea-10"><i class="fa fa-check"></i><b>13.1</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html"><i class="fa fa-check"></i><b>14</b> Componentes Principales 1</a><ul>
<li class="chapter" data-level="14.1" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#motivacion"><i class="fa fa-check"></i><b>14.1</b> Motivación</a></li>
<li class="chapter" data-level="14.2" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#formulacion-de-maxima-varianza"><i class="fa fa-check"></i><b>14.2</b> Formulación de máxima varianza</a></li>
<li class="chapter" data-level="14.3" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#formulacion-de-error-minimo"><i class="fa fa-check"></i><b>14.3</b> Formulación de error mínimo</a></li>
<li class="chapter" data-level="14.4" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#aplicaciones-de-pca"><i class="fa fa-check"></i><b>14.4</b> Aplicaciones de PCA</a><ul>
<li class="chapter" data-level="14.4.1" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#compresion-de-datos"><i class="fa fa-check"></i><b>14.4.1</b> Compresión de datos</a></li>
<li class="chapter" data-level="14.4.2" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#ejemplo-compresion-de-una-imagen"><i class="fa fa-check"></i><b>14.4.2</b> Ejemplo: compresión de una imagen</a></li>
<li class="chapter" data-level="14.4.3" data-path="componentes-principales-1.html"><a href="componentes-principales-1.html#ejemplo-grado-de-marginacion"><i class="fa fa-check"></i><b>14.4.3</b> Ejemplo: grado de marginación</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="componentes-principales-2.html"><a href="componentes-principales-2.html"><i class="fa fa-check"></i><b>15</b> Componentes Principales 2</a></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Estadística Aplicada III</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresion-logistica-3" class="section level1">
<h1><span class="header-section-number">Clase 8</span> Regresión logística 3</h1>
<style>
  .espacio {
     margin-bottom: 1cm;
  }
</style>
<style>
  .espacio3 {
     margin-bottom: 3cm;
  }
</style>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)</code></pre></div>
<div id="ejemplo-oscares" class="section level2">
<h2><span class="header-section-number">8.1</span> Ejemplo óscares</h2>
<p>Algunas de los factores citados usualmente por las personas como importantes para que una película gane un Óscar son:</p>
<ol style="list-style-type: decimal">
<li><p>Estar nominada a Mejor Director.</p></li>
<li><p>Haber ganado un premio en los Director’s Guild Awards.</p></li>
<li><p>Tener más nominaciones a la Academia.</p></li>
<li><p>Ganó mejor película en Golden Globe Awards.</p></li>
<li><p>La calificación en IMdb.</p></li>
<li><p>El score de Metacritic</p></li>
<li><p>El gusto de las personas por la película.</p></li>
<li><p>El score en RT de los críticos más destacados.</p></li>
<li><p>Las recaudaciones en taquilla dométicas.</p></li>
<li><p>Las recaudaciones en taquilla generales.</p></li>
<li><p>El presupuesto con el que se realizó la película.</p></li>
<li><p>La duración de la película.</p></li>
<li><p>El número de estrellas “conocidas”</p></li>
</ol>
<p>Contamos con datos que provienen de varias fuentes y se tienen las siguientes variables disponibles:</p>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th>Descripción</th>
<th>Fuente</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>film</td>
<td>Nombre de la película nominada</td>
<td></td>
</tr>
<tr class="even">
<td>year</td>
<td>Año de nominación de la película</td>
<td></td>
</tr>
<tr class="odd">
<td>release_date</td>
<td>Fecha de lanzamiento</td>
<td>IMdb</td>
</tr>
<tr class="even">
<td>mpaa</td>
<td>Clasificación</td>
<td>IMdb</td>
</tr>
<tr class="odd">
<td>imdb_score</td>
<td>Rating de IMdb</td>
<td>IMdb</td>
</tr>
<tr class="even">
<td>metacritic_score</td>
<td>Score de Metacritic</td>
<td>IMdb</td>
</tr>
<tr class="odd">
<td>rt_audience_score</td>
<td>% de personas que la favorecen</td>
<td>Rotten Tomatoes</td>
</tr>
<tr class="even">
<td>rt_critic_score</td>
<td>Score de “Top critics”</td>
<td>Rotten Tomatoes</td>
</tr>
<tr class="odd">
<td>bo</td>
<td>Recaudado en taquilla</td>
<td>Box Office Mojo</td>
</tr>
<tr class="even">
<td>budget</td>
<td>Estimated budget</td>
<td>Wikipedia</td>
</tr>
<tr class="odd">
<td>running_time</td>
<td>Duración (en minutos)</td>
<td>Wikipedia</td>
</tr>
<tr class="even">
<td>stars_count</td>
<td># de actores mostrados en el recuadro</td>
<td>Wikipedia</td>
</tr>
<tr class="odd">
<td>aabd</td>
<td>Nominación a mejor director Óscar</td>
<td>Wikipedia</td>
</tr>
<tr class="even">
<td>dga</td>
<td>Ganadora del Director’s Guild</td>
<td>Wikipedia</td>
</tr>
<tr class="odd">
<td>noms</td>
<td>Número de nominaciones al Óscar</td>
<td>Wikipedia</td>
</tr>
<tr class="even">
<td>ggbp</td>
<td>Ganadora en los Globos</td>
<td>Wikipedia</td>
</tr>
<tr class="odd">
<td>winner</td>
<td>Ganó Óscar</td>
<td>Wikipedia</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">oscars &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;datos/oscars.csv&quot;</span>)
<span class="kw">glimpse</span>(oscars)
<span class="co">#&gt; Observations: 156</span>
<span class="co">#&gt; Variables: 17</span>
<span class="co">#&gt; $ film              &lt;chr&gt; &quot;Forrest Gump&quot;, &quot;Four Weddings and a Funeral...</span>
<span class="co">#&gt; $ year              &lt;int&gt; 1994, 1994, 1994, 1994, 1994, 1995, 1995, 19...</span>
<span class="co">#&gt; $ release_date      &lt;date&gt; 1994-07-06, 1994-04-15, 1994-10-14, 1994-10...</span>
<span class="co">#&gt; $ mpaa              &lt;chr&gt; &quot;PG-13&quot;, &quot;R&quot;, &quot;R&quot;, &quot;PG-13&quot;, &quot;R&quot;, &quot;PG&quot;, &quot;G&quot;, ...</span>
<span class="co">#&gt; $ imdb_score        &lt;dbl&gt; 8.8, 7.1, 8.9, 7.5, 9.3, 7.6, 6.8, 8.4, 7.7,...</span>
<span class="co">#&gt; $ metacritic_score  &lt;int&gt; 82, 81, 94, 88, 80, 77, 83, 68, 84, 81, 85, ...</span>
<span class="co">#&gt; $ rt_audience_score &lt;int&gt; 95, 74, 96, 87, 98, 87, 67, 85, 90, 94, 92, ...</span>
<span class="co">#&gt; $ rt_critic_score   &lt;int&gt; 72, 95, 94, 96, 91, 95, 97, 77, 98, 93, 93, ...</span>
<span class="co">#&gt; $ bo                &lt;dbl&gt; 677.90, 245.70, 213.90, 24.80, 58.30, 355.20...</span>
<span class="co">#&gt; $ budget            &lt;dbl&gt; 55.0, 2.8, 88.5, 31.0, 25.0, 52.0, 30.0, 65....</span>
<span class="co">#&gt; $ running_time      &lt;int&gt; 142, 117, 154, 133, 142, 140, 92, 178, 136, ...</span>
<span class="co">#&gt; $ stars_count       &lt;int&gt; 5, 10, 12, 5, 7, 6, 2, 4, 4, 3, 5, 7, 12, 5,...</span>
<span class="co">#&gt; $ aabd              &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,...</span>
<span class="co">#&gt; $ dga               &lt;int&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,...</span>
<span class="co">#&gt; $ noms              &lt;int&gt; 19, 12, 17, 11, 7, 7, 6, 6, 17, 1, 12, 7, 12...</span>
<span class="co">#&gt; $ ggbp              &lt;int&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,...</span>
<span class="co">#&gt; $ winner            &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,...</span></code></pre></div>
<p>Ajustamos un modelo de regresión logística para “winner” utilizando como predictores “imdb_score + metacritic_score + rt_audience_score + rt_critic_score + bo + budget + running_time + stars_count + aabd + dga + noms + ggbp”:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">oscars_<span class="dv">2</span> &lt;-<span class="st"> </span>oscars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">imdb_score =</span> imdb_score<span class="op">/</span><span class="dv">10</span>,
         <span class="dt">metacritic_score =</span> metacritic_score<span class="op">/</span><span class="dv">100</span>,
         <span class="dt">rt_audience_score =</span> rt_audience_score<span class="op">/</span><span class="dv">100</span>,
         <span class="dt">bo =</span> <span class="kw">log</span>(bo),
         <span class="dt">budget =</span> <span class="kw">log</span>(budget),
         <span class="dt">running_time =</span> <span class="kw">log</span>(running_time),
         <span class="dt">stars_count =</span> <span class="kw">log</span>(stars_count)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(year <span class="op">&lt;</span><span class="st"> </span><span class="dv">2017</span>)
fit.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> winner <span class="op">~</span><span class="st"> </span>imdb_score <span class="op">+</span><span class="st"> </span>rt_critic_score <span class="op">+</span><span class="st"> </span>bo <span class="op">+</span><span class="st"> </span>budget <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>running_time <span class="op">+</span><span class="st"> </span>stars_count <span class="op">+</span><span class="st"> </span>aabd <span class="op">+</span><span class="st"> </span>dga <span class="op">+</span><span class="st"> </span>noms <span class="op">+</span><span class="st"> </span>ggbp, 
             <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), 
             <span class="dt">data =</span> oscars_<span class="dv">2</span>)
fit.<span class="dv">1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  glm(formula = winner ~ imdb_score + rt_critic_score + bo + budget + </span>
<span class="co">#&gt;     running_time + stars_count + aabd + dga + noms + ggbp, family = binomial(link = &quot;logit&quot;), </span>
<span class="co">#&gt;     data = oscars_2)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;     (Intercept)       imdb_score  rt_critic_score               bo  </span>
<span class="co">#&gt;        -14.7473          11.0596          -0.0426           0.1991  </span>
<span class="co">#&gt;          budget     running_time      stars_count             aabd  </span>
<span class="co">#&gt;         -0.7994           0.9864           0.6808           0.6482  </span>
<span class="co">#&gt;             dga             noms             ggbp  </span>
<span class="co">#&gt;          3.6727           0.1642           0.4169  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Degrees of Freedom: 146 Total (i.e. Null);  136 Residual</span>
<span class="co">#&gt; Null Deviance:       128 </span>
<span class="co">#&gt; Residual Deviance: 65.7  AIC: 87.7</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(oscars<span class="op">$</span>dga)
<span class="co">#&gt; [1] 0.154</span></code></pre></div>
<p>Podemos ver las probabilidades <span class="math inline">\(p_1({x^{(i)}})\)</span>, <span class="math inline">\(i=1,\ldots,N\)</span>, que <em>predice</em> el modelo utilizando la función <code>predict</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit.<span class="dv">1</span>, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
<span class="co">#&gt;        1        2        3        4        5        6        7        8 </span>
<span class="co">#&gt; 0.979292 0.080864 0.210608 0.013778 0.060864 0.188686 0.002499 0.034669 </span>
<span class="co">#&gt;        9       10       11       12       13       14       15       16 </span>
<span class="co">#&gt; 0.042967 0.004579 0.095418 0.007524 0.193038 0.116403 0.930428 0.025499 </span>
<span class="co">#&gt;       17       18       19       20       21       22       23       24 </span>
<span class="co">#&gt; 0.105491 0.023082 0.042503 0.744668 0.011385 0.029657 0.773747 0.119831 </span>
<span class="co">#&gt;       25       26       27       28       29       30       31       32 </span>
<span class="co">#&gt; 0.016132 0.978409 0.062259 0.040791 0.014552 0.032914 0.044156 0.478123 </span>
<span class="co">#&gt;       33       34       35       36       37       38       39       40 </span>
<span class="co">#&gt; 0.018495 0.150354 0.029615 0.938574 0.066907 0.063561 0.021840 0.128031 </span>
<span class="co">#&gt;       41       42       43       44       45       46       47       48 </span>
<span class="co">#&gt; 0.837398 0.026380 0.324370 0.028328 0.053700 0.186569 0.005487 0.102118 </span>
<span class="co">#&gt;       49       50       51       52       53       54       55       56 </span>
<span class="co">#&gt; 0.004324 0.912908 0.032282 0.454924 0.034419 0.060990 0.059391 0.944622 </span>
<span class="co">#&gt;       57       58       59       60       61       62       63       64 </span>
<span class="co">#&gt; 0.071828 0.335235 0.054461 0.014381 0.181415 0.011564 0.034738 0.850732 </span>
<span class="co">#&gt;       65       66       67       68       69       70       71       72 </span>
<span class="co">#&gt; 0.037506 0.054856 0.046618 0.023317 0.757340 0.055775 0.042111 0.028372 </span>
<span class="co">#&gt;       73       74       75       76       77       78       79       80 </span>
<span class="co">#&gt; 0.873698 0.057019 0.087092 0.002829 0.015224 0.011532 0.014084 0.071483 </span>
<span class="co">#&gt;       81       82       83       84       85       86       87       88 </span>
<span class="co">#&gt; 0.010316 0.017203 0.598860 0.000934 0.025656 0.006078 0.118978 0.021457 </span>
<span class="co">#&gt;       89       90       91       92       93       94       95       96 </span>
<span class="co">#&gt; 0.055964 0.031121 0.923980 0.055082 0.002808 0.012944 0.016335 0.007388 </span>
<span class="co">#&gt;       97       98       99      100      101      102      103      104 </span>
<span class="co">#&gt; 0.003131 0.024420 0.006746 0.725232 0.061440 0.114100 0.002055 0.002480 </span>
<span class="co">#&gt;      105      106      107      108      109      110      111      112 </span>
<span class="co">#&gt; 0.017222 0.406895 0.019276 0.025923 0.033696 0.011340 0.060555 0.099039 </span>
<span class="co">#&gt;      113      114      115      116      117      118      119      120 </span>
<span class="co">#&gt; 0.006466 0.356416 0.075537 0.007128 0.030705 0.135007 0.007054 0.039183 </span>
<span class="co">#&gt;      121      122      123      124      125      126      127      128 </span>
<span class="co">#&gt; 0.009095 0.051619 0.002576 0.240445 0.272211 0.004463 0.142060 0.142537 </span>
<span class="co">#&gt;      129      130      131      132      133      134      135      136 </span>
<span class="co">#&gt; 0.065616 0.055508 0.005639 0.008353 0.004578 0.059580 0.042772 0.046167 </span>
<span class="co">#&gt;      137      138      139      140      141      142      143      144 </span>
<span class="co">#&gt; 0.013173 0.500113 0.015982 0.006753 0.036636 0.007289 0.010139 0.825918 </span>
<span class="co">#&gt;      145      146      147 </span>
<span class="co">#&gt; 0.047052 0.138713 0.345245</span></code></pre></div>
<p><br></p>
<p><strong>Responde las siguientes preguntas:</strong></p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>La variable que tiene más impacto en la probabilidad de ganar un Óscar…</p>
<ol style="list-style-type: lower-alpha">
<li><p>Ganó un premio en el Director’s Guild.</p></li>
<li><p>Número de nominaciones a los Óscares.</p></li>
<li><p>Crítica de Rotten Tomatoes (Top critics).</p></li>
<li><p>Presupuesto de la película.</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>Si la película ganó un premio en los Director’s Guild, entonces</p>
<ol style="list-style-type: lower-alpha">
<li><p>la probabilidad logit de ganar un Óscar aumenta en 3.67.</p></li>
<li><p>los momios de ganar un Óscar son <span class="math inline">\(0.92\)</span>.</p></li>
<li><p>la probabilidad de ganar un Óscar aumenta en <span class="math inline">\(0.92\)</span>.</p></li>
<li><p>los momios de ganar un Óscar aumentan en <span class="math inline">\(e^{3.67}\)</span>.</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<p>Correspondientes al 2017 estas películas fueron nominadas al premio de la academia:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">oscars <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2017</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(film,year,release_date, mpaa, imdb_score,rt_critic_score) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">film</th>
<th align="right">year</th>
<th align="left">release_date</th>
<th align="left">mpaa</th>
<th align="right">imdb_score</th>
<th align="right">rt_critic_score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Call Me by Your Name</td>
<td align="right">2017</td>
<td align="left">2018-01-19</td>
<td align="left">R</td>
<td align="right">8.1</td>
<td align="right">96</td>
</tr>
<tr class="even">
<td align="left">Darkest Hour</td>
<td align="right">2017</td>
<td align="left">2017-12-22</td>
<td align="left">PG-13</td>
<td align="right">7.4</td>
<td align="right">86</td>
</tr>
<tr class="odd">
<td align="left">Dunkirk</td>
<td align="right">2017</td>
<td align="left">2017-07-21</td>
<td align="left">PG-13</td>
<td align="right">8.0</td>
<td align="right">93</td>
</tr>
<tr class="even">
<td align="left">Get Out</td>
<td align="right">2017</td>
<td align="left">2017-02-24</td>
<td align="left">R</td>
<td align="right">7.7</td>
<td align="right">99</td>
</tr>
<tr class="odd">
<td align="left">Lady Bird</td>
<td align="right">2017</td>
<td align="left">2017-12-01</td>
<td align="left">R</td>
<td align="right">7.6</td>
<td align="right">99</td>
</tr>
<tr class="even">
<td align="left">Phantom Thread</td>
<td align="right">2017</td>
<td align="left">2018-01-19</td>
<td align="left">R</td>
<td align="right">7.8</td>
<td align="right">91</td>
</tr>
<tr class="odd">
<td align="left">The Post</td>
<td align="right">2017</td>
<td align="left">2018-01-12</td>
<td align="left">PG-13</td>
<td align="right">7.3</td>
<td align="right">88</td>
</tr>
<tr class="even">
<td align="left">The Shape of Water</td>
<td align="right">2017</td>
<td align="left">2017-12-22</td>
<td align="left">R</td>
<td align="right">7.7</td>
<td align="right">92</td>
</tr>
<tr class="odd">
<td align="left">Three Billboards Outside Ebbing, Missouri</td>
<td align="right">2017</td>
<td align="left">2017-12-01</td>
<td align="left">R</td>
<td align="right">8.3</td>
<td align="right">92</td>
</tr>
</tbody>
</table>
<p>Veamos qué predicciones obtenemos para el 2017:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">oscars_<span class="dv">3</span> &lt;-<span class="st"> </span>oscars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">imdb_score =</span> imdb_score<span class="op">/</span><span class="dv">10</span>,
         <span class="dt">metacritic_score =</span> metacritic_score<span class="op">/</span><span class="dv">100</span>,
         <span class="dt">rt_audience_score =</span> rt_audience_score<span class="op">/</span><span class="dv">100</span>,
         <span class="dt">bo =</span> <span class="kw">log</span>(bo),
         <span class="dt">budget =</span> <span class="kw">log</span>(budget),
         <span class="dt">running_time =</span> <span class="kw">log</span>(running_time),
         <span class="dt">stars_count =</span> <span class="kw">log</span>(stars_count)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2017</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(imdb_score,rt_critic_score,bo,budget,running_time,stars_count,aabd,dga,noms,ggbp)
<span class="kw">predict</span>(fit.<span class="dv">1</span>, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>, <span class="dt">newdata =</span> oscars_<span class="dv">3</span>)
<span class="co">#&gt;       1       2       3       4       5       6       7       8       9 </span>
<span class="co">#&gt; 0.07409 0.00672 0.01005 0.06379 0.07457 0.00597 0.00422 0.76347 0.17262</span></code></pre></div>
<p>La probabilidad de predicción más alta es 0.76347 que corresponde a <em>The Shape Of Water</em>.</p>
<hr />
<p><br></p>
</div>
<div id="repaso-de-regresion-logistica" class="section level2">
<h2><span class="header-section-number">8.2</span> Repaso de regresión logística</h2>
<p>En un problema de clasificación donde las <span class="math inline">\(y_i\)</span>’s son binarias, desearíamos tener un modelo de la forma: <span class="math display">\[
\pi_i = x_i^\prime \beta
\]</span> donde <span class="math inline">\(\beta\)</span> es un vector de coeficientes.</p>
<p class="espacio">
</p>
<p>El problema es que el componente lineal puede tomar cualquier valor <em>real</em>, mientras que <span class="math inline">\(\pi_i\)</span> sólo puede tomar valores entre <span class="math inline">\(0\)</span> y <span class="math inline">\(1\)</span>.</p>
<p>Una alternativa es utilizar los momios:</p>
<p><span class="math display">\[
\Omega_i = \dfrac{\pi_i}{1-\pi_i},
\]</span> el cociente de la probabilidad y su complemento, la razón de exitosos por fracasados.</p>
<p class="espacio">
</p>

<div class="information">
<p><strong>Nota:</strong></p>
<p class="espacio">
</p>
<ul>
<li><p>Si la probabilidad de un evento es <span class="math inline">\(1/2\)</span>, entonces los momios son <em>uno a uno</em> o <em>justos</em>.</p></li>
<li>Si la probabilidad es <span class="math inline">\(1/3\)</span>, entonces los momios son uno a dos.
</div>
</li>
</ul>
<p><br></p>
<p>Los momios toman valores entre <span class="math inline">\(0\)</span> e <span class="math inline">\(\infty\)</span>, lo cual no los hace del todo útiles para especificar nuestro modelo. Por lo tanto lo planteamos de la forma:</p>
<p><span class="math display">\[
\mbox{logit}(\pi_i) = x_i^\prime\beta
\]</span></p>
<p>La trasformación logit es <em>uno a uno</em>. La función logit inversa <span class="math inline">\(\mbox{logit}^{-1}\)</span> nos permite regresar de probabilidades logits a probabilidades usuales.</p>
<p>Podemos ver gráficamente la transformación logit:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit &lt;-<span class="st"> </span><span class="cf">function</span>(x){<span class="kw">log</span>((x<span class="op">/</span>((<span class="dv">1</span><span class="op">-</span>x))))}
graf_data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">100</span>), <span class="dt">logit =</span> <span class="kw">logit</span>(x))
<span class="kw">ggplot</span>(graf_data, <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> logit), <span class="dt">colour =</span> <span class="st">&#39;lightpink&#39;</span>, <span class="dt">size=</span><span class="fl">1.2</span>)</code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-10-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Despejando para <span class="math inline">\(\pi_i\)</span> obtenemos</p>
<p><span class="math display">\[
\pi_i = \mbox{logit}^{-1}(\eta_i)=\mbox{logit}^{-1}(x_i^\prime\beta)=\dfrac{e^{x_i^\prime\beta}}{1+e^{x_i^\prime\beta}}
\]</span></p>
<p>donde <span class="math inline">\(\eta_i=x_i^\prime\beta\)</span>.</p>
<p>Cada variable aleatoria <span class="math inline">\(y_i\)</span> puede tomar valor de <span class="math inline">\(0\)</span> o <span class="math inline">\(1\)</span>, fracaso o éxito, respectivamente. Por lo tanto, <span class="math inline">\(y_i\)</span> tiene una distribución Bernoulli con probabilidad de éxito <span class="math inline">\(\pi_i\)</span>. Y se tiene que <span class="math display">\[
p_1(x_i) = \pi_i = \mbox{logit}^{-1}(x_i^\prime\beta).
\]</span></p>
<p><strong>Recordemos las interpretaciones de los coeficientes:</strong></p>
<ol style="list-style-type: decimal">
<li><p>Evaluar en o alrededor de la media: <span class="math display">\[\mbox{logit}^{-1}(\beta_0+\beta_j\cdot \bar{x}_j).\]</span></p></li>
<li><p>Interpretar como un cambio en la probabilidad ante un cambio unitario en <span class="math inline">\(x\)</span> alrededor de la media: <span class="math display">\[\mbox{logit}^{-1}(\beta_0+\beta_j\cdot \bar{x}_j) - \mbox{logit}^{-1}(\beta_0+\beta_j\cdot (\bar{x}_j-1)).\]</span></p></li>
<li><p>Calcular la derivada de la curva logística en la media: <span class="math display">\[\dfrac{\beta_j\, e^{\beta_0+\beta_j \bar{x}_j}}{(1 + e^{\beta_0 +\beta_j \bar{x}_j})^2}.\]</span></p></li>
<li><p>Dividir entre 4: <span class="math display">\[\dfrac{\beta_j}{4}.\]</span> Se interpreta como la diferencia en la probabilidad ante un cambio unitario en <span class="math inline">\(x_j\)</span> alrededor de la media (aproximadamente).</p></li>
<li><p>Calcular el cociente de momios: <span class="math display">\[\mbox{log}\left[\dfrac{P(y_i=1|x)}{P(y_i=0|x)} \right] = \alpha + \beta x.\]</span> Sumar 1 a la variable <span class="math inline">\(x\)</span> es equivalente a sumar <span class="math inline">\(\beta\)</span> en ambos lados de la ecuación. Exponenciando nuevamente ambos lados, el cociente de momios se multiplica por <span class="math inline">\(e^\beta\)</span>.</p></li>
</ol>
<p><br></p>
<p><strong>Estimación de los parámetros <span class="math inline">\(\beta\)</span>:</strong></p>
<p>Se utiliza descenso en gradiente para estimar los coeficientes <span class="math inline">\(\beta_j\)</span> minimizando la devianza: <span class="math display">\[
D(\beta) = -2\sum_{i=1}^N \log(p_{y^{(i)}} (x^{(i)})).
\]</span></p>
<p><strong>Responde las siguientes preguntas:</strong></p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>Supongamos que se ajustan los coeficientes de un modelo lineal logístico y una <em>nueva observación</em> <span class="math inline">\(x\)</span> es tal que su predicción es <span class="math inline">\(h(x^\prime \beta)=0.7\)</span>. Esto significa que (selecciona una o más):</p>
<ol style="list-style-type: lower-alpha">
<li><p>Nuestra estimación de <span class="math inline">\(P(y=0|x;\beta)\)</span> es 0.7.</p></li>
<li><p>Nuestra estimación de <span class="math inline">\(P(y=1|x;\beta)\)</span> es 0.7.</p></li>
<li><p>Nuestra estimación de <span class="math inline">\(P(y=0|x;\beta)\)</span> es 0.3.</p></li>
<li><p>Nuestra estimación de <span class="math inline">\(P(y=1|x;\beta)\)</span> es 0.3.</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>Supongamos que se ajusta un modelo logístico <span class="math inline">\(p_1(x_i)=h(\beta_0+\beta_1x_1^{(i)}+\beta_2 x_2^{(i)})\)</span>. Supongamos que <span class="math inline">\(\beta_0=6, \;\beta_1=-1,\; \beta_2=0\)</span>. ¿Cuál de las siguientes figuras puede servir como una regla de decisión para este modelo?</p>
<ol style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>  <img src="figuras/a.png" style="width:20.0%" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>  <img src="figuras/b.png" style="width:20.0%" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>  <img src="figuras/c.png" style="width:20.0%" /></p>
<ol start="4" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>  <img src="figuras/d.png" style="width:20.0%" /></p>
<p class="espacio3">
</p>
</div>
<p><br></p>
<hr />
<p><br></p>
</div>
<div id="regresion-logistica-con-interacciones" class="section level2">
<h2><span class="header-section-number">8.3</span> Regresión logística con interacciones</h2>
<p>Recordemos el modelo de pozos en Bangladesh:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wells &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;datos/wells.csv&quot;</span>)
wells &lt;-<span class="st"> </span>wells <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">dist_100 =</span> dist<span class="op">/</span><span class="dv">100</span>)
fit.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_<span class="dv">100</span>, <span class="dt">data =</span> wells, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
fit.<span class="dv">2</span></code></pre></div>
<p>Posteriormente añadimos una segunda variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>arsenic, <span class="dt">data =</span> wells, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
fit.<span class="dv">3</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  glm(formula = switch ~ dist_100 + arsenic, family = binomial(link = &quot;logit&quot;), </span>
<span class="co">#&gt;     data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)     dist_100      arsenic  </span>
<span class="co">#&gt;     0.00275     -0.89664      0.46077  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Degrees of Freedom: 3019 Total (i.e. Null);  3017 Residual</span>
<span class="co">#&gt; Null Deviance:       4120 </span>
<span class="co">#&gt; Residual Deviance: 3930  AIC: 3940</span></code></pre></div>
<p>Añadimos una interacción entre estos dos términos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>arsenic <span class="op">+</span><span class="st"> </span>dist_<span class="dv">100</span><span class="op">:</span>arsenic, 
             <span class="dt">data =</span> wells,
             <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
fit.<span class="dv">4</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  glm(formula = switch ~ dist_100 + arsenic + dist_100:arsenic, </span>
<span class="co">#&gt;     family = binomial(link = &quot;logit&quot;), data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;      (Intercept)          dist_100           arsenic  dist_100:arsenic  </span>
<span class="co">#&gt;           -0.148            -0.577             0.556            -0.179  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Degrees of Freedom: 3019 Total (i.e. Null);  3016 Residual</span>
<span class="co">#&gt; Null Deviance:       4120 </span>
<span class="co">#&gt; Residual Deviance: 3930  AIC: 3940</span></code></pre></div>
<p>Para entender los números en la tabla, usamos los siguientes trucos:</p>
<ul>
<li><p>Evaluar predicciones e interacciones en la media de los datos, que tienen valores promedio de 0.48 para distancia y 1.66 para arsénico (es decir, una distancia media de 48 metros al pozo seguro más cercano, y un nivel promedio de arsénico de 1.66 entre los pozos inseguros).</p></li>
<li><p>Dividir entre 4 para obtener diferencias predictivas aproximadas en la escala de probabilidad.</p></li>
</ul>
<p>Intrepretamos los coeficientes:</p>
<ol style="list-style-type: decimal">
<li><p>El término constante no tiene interpretación: <span class="math inline">\(\mbox{logit}^{-1}(-0.15) = 0.47\)</span> es la probabilidad estimada de cambio, si la distancia al pozo seguro más cercano es <span class="math inline">\(0\)</span> y el nivel de arsénico del pozo actual es 0. Esto es imposible porque la distribución de arsénico en pozos inseguros comienza en <span class="math inline">\(0.5\)</span>.</p>
<p>En cambio, podemos evaluar la predicción en los valores promedio de <span class="math inline">\(\mbox{dist_100} = 0.48\)</span> y <span class="math inline">\(\mbox{arsénico} = 1.66\)</span>, la probabilidad de cambiar de pozo es <span class="math inline">\(\mbox{logit}^1(-0.15 - 0.58 \cdot 0.48 + 0.56 \cdot 1.66 - 0.18 \cdot 0.48 \cdot 1.66) = 0.59\)</span>.</p></li>
<li><p>Coeficiente de distancia: esto corresponde a la comparación de dos pozos que difieren en 1 en dist_100, si el nivel de arsénico es 0 para ambos pozos. Una vez más, no debemos tratar de interpretarlo.</p>
<p>En cambio, podemos ver el valor promedio, arsénico = 1.66, donde la distancia tiene un coeficiente de <span class="math inline">\(-0.58 - 0.18 · 1.66 = -0.88\)</span> en la escala logit. Para interpretar esto rápidamente en la escala de probabilidad, lo dividimos por 4: <span class="math inline">\(-0.88 / 4 = -0.22\)</span>. Por lo tanto, al nivel medio de arsénico en los datos, a cada 100 metros de distancia le corresponden a una diferencia negativa aproximada del 22% en la probabilidad de cambio.</p></li>
<li><p>Coeficiente para arsénico: esto equivale a comparar dos pozos que difieren en 1 en arsénico, si la distancia al pozo seguro más cercano es 0 para ambos.</p>
<p>Evaluamos la comparación en el valor promedio para la distancia, <span class="math inline">\(\mbox{dist_100} = 0.48\)</span>, donde el arsénico tiene un coeficiente de <span class="math inline">\(0.56 - 0.18 · 0.48 = 0.47\)</span> en la escala logit. Para interpretar esto rápidamente en la escala de probabilidad, <em>lo dividimos entre <span class="math inline">\(4\)</span></em>: <span class="math inline">\(0.47 / 4 = 0.12\)</span>. Por lo tanto, en el nivel medio de distancia en los datos, cada unidad adicional de arsénico corresponde a una diferencia positiva aproximada del 12% en la probabilidad de cambio.</p></li>
<li><em>Coeficiente para el término de interacción</em>: se puede interpretar de dos maneras:
<ul>
<li>por cada unidad adicional de arsénico, el valor <span class="math inline">\(-0.18\)</span> se agrega al coeficiente de distancia. Ya hemos visto que el coeficiente de distancia es <span class="math inline">\(-0.88\)</span> en el nivel promedio de arsénico, por lo que podemos entender la interacción diciendo que la importancia de la distancia como predictor aumenta para los hogares con niveles más altos de arsénico.</li>
<li>por cada <span class="math inline">\(100\)</span> metros adicionales de distancia al pozo más cercano, se agrega el valor <span class="math inline">\(-0.18\)</span> al coeficiente de arsénico. Ya hemos visto que el coeficiente de distancia es <span class="math inline">\(0.47\)</span> a la distancia promedio al pozo seguro más cercano, y así podemos entender la interacción diciendo que la importancia del arsénico como predictor disminuye para los hogares que están más lejos de los pozos seguros existentes.</li>
</ul></li>
</ol>
<div id="centrando-las-variables" class="section level4 unnumbered">
<h4>Centrando las variables</h4>
<p>Como se discutió anteriormente en el contexto de la regresión lineal, antes de ajustar las interacciones tiene sentido centrar las variables de entrada para que podamos interpretar los coeficientes más fácilmente. Las entradas centradas son:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wells &lt;-<span class="st"> </span>wells <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dist_100_c =</span> dist_<span class="dv">100</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(dist_<span class="dv">100</span>),
         <span class="dt">arsenic_c =</span> arsenic <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(arsenic))</code></pre></div>
<p>Podemos reajustar el modelo usando las variables de entrada centradas, lo que hará que los coeficientes sean mucho más fáciles de interpretar:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">5</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_100_c <span class="op">+</span><span class="st"> </span>arsenic_c <span class="op">+</span><span class="st"> </span>dist_100_c<span class="op">:</span>arsenic_c, <span class="dt">data =</span> wells,
  <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
fit.<span class="dv">5</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c, </span>
<span class="co">#&gt;     family = binomial(link = &quot;logit&quot;), data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;          (Intercept)            dist_100_c             arsenic_c  </span>
<span class="co">#&gt;                0.351                -0.874                 0.470  </span>
<span class="co">#&gt; dist_100_c:arsenic_c  </span>
<span class="co">#&gt;               -0.179  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Degrees of Freedom: 3019 Total (i.e. Null);  3016 Residual</span>
<span class="co">#&gt; Null Deviance:       4120 </span>
<span class="co">#&gt; Residual Deviance: 3930  AIC: 3940</span></code></pre></div>
<p>Centramos las entradas, no los predictores. Por lo tanto, no centramos la interacción (<span class="math inline">\(\mbox{dist_100} * \mbox{arsénico}\)</span>); más bien, incluimos la interacción de las dos variables de entrada centradas.</p>
<p>Interpretamos los coeficientes en esta nueva escala:</p>
<ol style="list-style-type: decimal">
<li><p>Término constante: <span class="math inline">\(\mbox{logit}^{-1}(0.35) = 0.59\)</span> es la probabilidad estimada cambiar de pozo, si <span class="math inline">\(\mbox{dist_100_c} = \mbox{arsenic_c} = 0\)</span>, es decir, en las medias de la distancia al pozo seguro más cercano y el nivel de arsénico. (Obtuvimos este mismo cálculo, pero con más esfuerzo, con nuestro modelo anterior con datos no centrados).</p></li>
<li><p>Coeficiente de distancia: éste es el coeficiente de distancia (en la escala logit) si el nivel de arsénico está en su valor promedio. Para interpretar esto rápidamente en la escala de probabilidad, lo dividimos por 4: <span class="math inline">\(-0.88 / 4 = -0.22\)</span>. Por lo tanto, al nivel medio de arsénico en los datos, cada 100 metros de distancia corresponde a una diferencia negativa aproximada del 22% en la probabilidad de cambio.</p></li>
<li><p>Coeficiente para arsénico: este es el coeficiente para el nivel de arsénico si la distancia al pozo seguro más cercano está en su valor promedio. Para interpretar esto rápidamente en la escala de probabilidad, lo dividimos por 4: <span class="math inline">\(0.47 / 4 = 0.12\)</span>. Por lo tanto, en el nivel medio de distancia en los datos, cada unidad adicional de arsénico corresponde a una diferencia positiva aproximada del <span class="math inline">\(12\%\)</span> en la probabilidad de cambio.</p></li>
<li><p>Coeficiente para el término de interacción: esto no se modifica al centrarse y tiene la misma interpretación que antes.</p></li>
</ol>
<p>Las predicciones para nuevas observaciones no se modifican. Centrar los predictores cambia las interpretaciones de los coeficientes pero no cambia el modelo subyacente.</p>
<p><img src="figuras/manicule.jpg" /> Estima el error estándar del coeficiente de interacción usando la técnica de <em>bootsrap</em>. ¿Es significativo dicho coeficiente?</p>
<p class="espacio">
</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit.<span class="dv">5</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c, </span>
<span class="co">#&gt;     family = binomial(link = &quot;logit&quot;), data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Deviance Residuals: </span>
<span class="co">#&gt;    Min      1Q  Median      3Q     Max  </span>
<span class="co">#&gt;  -2.78   -1.20    0.77    1.08    1.85  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;                      Estimate Std. Error z value Pr(&gt;|z|)    </span>
<span class="co">#&gt; (Intercept)            0.3511     0.0399    8.81   &lt;2e-16 ***</span>
<span class="co">#&gt; dist_100_c            -0.8737     0.1048   -8.34   &lt;2e-16 ***</span>
<span class="co">#&gt; arsenic_c              0.4695     0.0421   11.16   &lt;2e-16 ***</span>
<span class="co">#&gt; dist_100_c:arsenic_c  -0.1789     0.1023   -1.75     0.08 .  </span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     Null deviance: 4118.1  on 3019  degrees of freedom</span>
<span class="co">#&gt; Residual deviance: 3927.6  on 3016  degrees of freedom</span>
<span class="co">#&gt; AIC: 3936</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></code></pre></div>
</div>
</div>
<div id="graficas-del-modelo-con-interacciones" class="section level2">
<h2><span class="header-section-number">8.4</span> Gráficas del modelo con interacciones</h2>
<p>La forma más clara de visualizar el modelo de interacción es graficar la función de la curva de regresión para cada posible escenario.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">invlogit &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))
}
<span class="kw">ggplot</span>(wells, <span class="kw">aes</span>(<span class="dt">x =</span> dist_<span class="dv">100</span>, <span class="dt">y =</span> <span class="cf">switch</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="fl">0.308</span>, <span class="dt">height =</span> <span class="fl">0.1</span>, <span class="dt">size =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x){
    <span class="kw">invlogit</span>(fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">4</span>]<span class="op">*</span><span class="fl">0.5</span><span class="op">*</span>x)}, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">3.5</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x){
    <span class="kw">invlogit</span>(fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">4</span>]<span class="op">*</span><span class="dv">1</span><span class="op">*</span>x)}, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">3.5</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">0.50</span>, <span class="dt">y =</span> <span class="fl">0.45</span>, <span class="dt">label =</span> <span class="st">&quot;As=0.5&quot;</span>, <span class="dt">size =</span> <span class="dv">4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">0.75</span>, <span class="dt">y =</span> <span class="fl">0.65</span>, <span class="dt">label =</span> <span class="st">&quot;As=1.0&quot;</span>, <span class="dt">size =</span> <span class="dv">4</span>)</code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-17-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(wells, <span class="kw">aes</span>(<span class="dt">x =</span> arsenic, <span class="dt">y =</span> <span class="cf">switch</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="fl">0.308</span>, <span class="dt">height =</span> <span class="fl">0.1</span>, <span class="dt">size =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x){
    <span class="kw">invlogit</span>(fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">3</span>]<span class="op">*</span>x <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">4</span>]<span class="op">*</span><span class="dv">0</span><span class="op">*</span>x)}, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="dv">10</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x){
    <span class="kw">invlogit</span>(fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">3</span>]<span class="op">*</span>x <span class="op">+</span><span class="st"> </span>fit.<span class="dv">5</span><span class="op">$</span>coef[<span class="dv">4</span>]<span class="op">*</span><span class="fl">0.5</span><span class="op">*</span>x)}, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="dv">10</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">0.7</span>, <span class="dt">y =</span> <span class="fl">0.80</span>, <span class="dt">label =</span> <span class="st">&quot;dist=0&quot;</span>, <span class="dt">size =</span> <span class="dv">4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">2.0</span>, <span class="dt">y =</span> <span class="fl">0.65</span>, <span class="dt">label =</span> <span class="st">&quot;dist=50&quot;</span>, <span class="dt">size =</span> <span class="dv">4</span>)</code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-18-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>La interacción no es grande en el rango de la mayoría de los datos. En la gráfica de arriba vemos que las líneas se empiezan a juntar a los <span class="math inline">\(300\)</span> metros de distancia.</p>
<p>Las diferencias en el cambio asociadas con las diferencias en el nivel de arsénico son grandes si se está cerca de un pozo seguro, pero el efecto disminuye si se está lejos de un pozo seguro. Esta interacción tiene algún sentido; sin embargo, hay cierta incertidumbre en el tamaño de la interacción (de la tabla de regresión anterior, una estimación de <span class="math inline">\(-0.18\)</span> con un error estándar de <span class="math inline">\(0.10\)</span>). Solo hay unos pocos datos en el área donde la interacción hace alguna diferencia.</p>
</div>
<div id="agregar-mas-predictores" class="section level2">
<h2><span class="header-section-number">8.5</span> Agregar más predictores</h2>
<p>¿Son más propensos los usuarios a cambiar de pozo si pertenecen a alguna asociación en su comunidad o si tienen mayor educación? Para ver, agregamos dos entradas:</p>
<ul>
<li><p>assoc = 1 si un miembro del hogar pertenece a alguna organización comunitaria</p></li>
<li><p>educ = años de educación del usuario del pozo.</p></li>
</ul>
<p>En realidad, trabajamos con <span class="math inline">\(\mbox{educ4} = \mbox{educ} / 4\)</span>, por las razones habituales de hacer que su coeficiente de regresión sea más interpretable: ahora representa la diferencia predictiva de agregar cuatro años de educación.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wells &lt;-<span class="st"> </span>wells <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">educ4 =</span> educ <span class="op">/</span><span class="st"> </span><span class="dv">4</span>)
fit.<span class="dv">6</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_100_c <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>arsenic_c <span class="op">+</span><span class="st"> </span>dist_100_c<span class="op">:</span>arsenic_c <span class="op">+</span>
<span class="st">               </span>assoc <span class="op">+</span><span class="st"> </span>educ4, 
             <span class="dt">data =</span> wells,
             <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
fit.<span class="dv">6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c + </span>
<span class="co">#&gt;     assoc + educ4, family = binomial(link = &quot;logit&quot;), data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;          (Intercept)            dist_100_c             arsenic_c  </span>
<span class="co">#&gt;                0.203                -0.875                 0.475  </span>
<span class="co">#&gt;                assoc                 educ4  dist_100_c:arsenic_c  </span>
<span class="co">#&gt;               -0.123                 0.168                -0.161  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Degrees of Freedom: 3019 Total (i.e. Null);  3014 Residual</span>
<span class="co">#&gt; Null Deviance:       4120 </span>
<span class="co">#&gt; Residual Deviance: 3910  AIC: 3920</span></code></pre></div>
<p>Nota:</p>
<ul>
<li><p>Para los hogares con pozos inseguros, pertenecer a una asociación comunitaria sorprendentemente no es predictivo de cambio de pozo, después de controlar los otros factores en el modelo.</p></li>
<li><p>Sin embargo, las personas con educación superior tienen más probabilidades de cambiar: la diferencia estimada bruta es <span class="math inline">\(0.17 / 4 = 0.04\)</span>, o una diferencia positiva de <span class="math inline">\(4\%\)</span> en la probabilidad de cambio cuando se comparan hogares que difieren en 4 años de educación.</p></li>
</ul>
<p>El coeficiente para la educación tiene sentido y es estadísticamente significativo, por lo que lo mantenemos en el modelo. El coeficiente de asociación comunitaria no tiene sentido y no es estadísticamente significativo, por lo que lo eliminamos.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">7</span> &lt;-<span class="st"> </span><span class="kw">glm</span> (<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_100_c <span class="op">+</span><span class="st"> </span>arsenic_c <span class="op">+</span><span class="st"> </span>dist_100_c<span class="op">:</span>arsenic_c <span class="op">+</span><span class="st"> </span>educ4,
              <span class="dt">data =</span> wells,
              <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
fit.<span class="dv">7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  glm(formula = switch ~ dist_100_c + arsenic_c + dist_100_c:arsenic_c + </span>
<span class="co">#&gt;     educ4, family = binomial(link = &quot;logit&quot;), data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;          (Intercept)            dist_100_c             arsenic_c  </span>
<span class="co">#&gt;                0.148                -0.875                 0.477  </span>
<span class="co">#&gt;                educ4  dist_100_c:arsenic_c  </span>
<span class="co">#&gt;                0.169                -0.163  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Degrees of Freedom: 3019 Total (i.e. Null);  3015 Residual</span>
<span class="co">#&gt; Null Deviance:       4120 </span>
<span class="co">#&gt; Residual Deviance: 3910  AIC: 3920</span></code></pre></div>
<p>Añadimos otras interacciones (centrando la variable de educación):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wells &lt;-<span class="st"> </span>wells <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">educ4_c =</span> educ4 <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(educ4))
fit.<span class="dv">8</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_100_c <span class="op">+</span><span class="st"> </span>arsenic_c <span class="op">+</span><span class="st"> </span>educ4_c <span class="op">+</span><span class="st"> </span>dist_100_c<span class="op">:</span>arsenic_c <span class="op">+</span>
<span class="st">               </span>dist_100_c<span class="op">:</span>educ4_c <span class="op">+</span><span class="st"> </span>arsenic_c<span class="op">:</span>educ4_c, 
             <span class="dt">data =</span> wells,
             <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
<span class="kw">summary</span>(fit.<span class="dv">8</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; glm(formula = switch ~ dist_100_c + arsenic_c + educ4_c + dist_100_c:arsenic_c + </span>
<span class="co">#&gt;     dist_100_c:educ4_c + arsenic_c:educ4_c, family = binomial(link = &quot;logit&quot;), </span>
<span class="co">#&gt;     data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Deviance Residuals: </span>
<span class="co">#&gt;    Min      1Q  Median      3Q     Max  </span>
<span class="co">#&gt; -2.571  -1.196   0.731   1.072   1.871  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;                      Estimate Std. Error z value Pr(&gt;|z|)    </span>
<span class="co">#&gt; (Intercept)            0.3563     0.0403    8.84  &lt; 2e-16 ***</span>
<span class="co">#&gt; dist_100_c            -0.9029     0.1073   -8.41  &lt; 2e-16 ***</span>
<span class="co">#&gt; arsenic_c              0.4950     0.0431   11.50  &lt; 2e-16 ***</span>
<span class="co">#&gt; educ4_c                0.1850     0.0392    4.72  2.4e-06 ***</span>
<span class="co">#&gt; dist_100_c:arsenic_c  -0.1177     0.1035   -1.14   0.2557    </span>
<span class="co">#&gt; dist_100_c:educ4_c     0.3227     0.1066    3.03   0.0025 ** </span>
<span class="co">#&gt; arsenic_c:educ4_c      0.0722     0.0439    1.65   0.0996 .  </span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     Null deviance: 4118.1  on 3019  degrees of freedom</span>
<span class="co">#&gt; Residual deviance: 3891.7  on 3013  degrees of freedom</span>
<span class="co">#&gt; AIC: 3906</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></code></pre></div>
<p>Podemos interpretar estas nuevas interacciones entendiendo cómo la educación modifica la diferencia predictiva correspondiente a la distancia y el arsénico.</p>
<ul>
<li><p><em>Interacción de distancia y educación</em>: una diferencia de 4 años de educación corresponde a una diferencia de <span class="math inline">\(0.32\)</span> en el coeficiente para <span class="math inline">\(\mbox{dist_100}\)</span>. Como ya hemos visto, <span class="math inline">\(\mbox{dist_100}\)</span> tiene un coeficiente negativo en promedio; por lo tanto, los cambios positivos en la educación reducen la asociación negativa de la distancia. Esto tiene sentido: las personas con más educación probablemente tengan otros recursos, por lo que andar una distancia extra para obtener agua no es una carga tan pesada.</p></li>
<li><p><em>Interacción de arsénico y educación</em>: una diferencia de 4 años de educación corresponde a una diferencia de <span class="math inline">\(0.07\)</span> en el coeficiente de arsénico. Como ya hemos visto, el arsénico tiene un coeficiente positivo en promedio; por lo tanto, aumentar la educación aumenta la asociación positiva del arsénico. Esto tiene sentido: las personas con más educación podrían estar más informadas sobre los riesgos del arsénico y, por lo tanto, ser más sensibles al aumento de los niveles de arsénico (o, a la inversa, tener menos prisa para cambiar de pozos con niveles de arsénico relativamente bajos).</p></li>
</ul>
<p><strong>Estandarizar los predictores</strong></p>
<p>Deberíamos considerar seriamente la posibilidad de estandarizar todos los predictores como una opción predeterminada para ajustar modelos con interacciones. Las dificultades con <span class="math inline">\(\mbox{dist_100}\)</span> y <span class="math inline">\(\mbox{educ4}\)</span> en este ejemplo sugieren que la estandarización, restar la media de cada una de las variables de entrada y dividir entre 2 desviaciones estándar.</p>
</div>
<div id="evaluacion-de-modelos-de-regresion-logistica" class="section level2">
<h2><span class="header-section-number">8.6</span> Evaluación de modelos de regresión logística</h2>
<p>Podemos definir residuales en regresión logística como <span class="math display">\[
\mbox{residual}_i = y_i − E(y_i|X_i) = y_i − \mbox{logit}^{-1}(X_i\beta).
\]</span> Los datos <span class="math inline">\(y_i\)</span> son discretos y también los residuales. Por ejemplo, si <span class="math inline">\(\mbox{logit}^{-1} (X_i\beta) = 0.7\)</span>, entonces <span class="math inline">\(\mbox{residual}_i = -0.7\)</span> o <span class="math inline">\(+0.3\)</span>, dependiendo de si <span class="math inline">\(y_i = 0\)</span> o <span class="math inline">\(1\)</span>.</p>
<p>Graficamos los residuales de la regresión logística:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">8</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_100_c <span class="op">+</span><span class="st"> </span>arsenic_c <span class="op">+</span><span class="st"> </span>educ4_c <span class="op">+</span><span class="st"> </span>dist_100_c<span class="op">:</span>arsenic_c <span class="op">+</span>
<span class="st">               </span>dist_100_c<span class="op">:</span>educ4_c <span class="op">+</span><span class="st"> </span>arsenic_c<span class="op">:</span>educ4_c,
             <span class="dt">data =</span> wells,
             <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))

<span class="co"># Probabilidades de predicción</span>
wells<span class="op">$</span>pred.<span class="dv">8</span> &lt;-<span class="st"> </span>fit.<span class="dv">8</span><span class="op">$</span>fitted.values

<span class="kw">ggplot</span>(wells, <span class="kw">aes</span>(<span class="dt">x=</span>pred.<span class="dv">8</span>, <span class="dt">y=</span><span class="cf">switch</span><span class="op">-</span>pred.<span class="dv">8</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">0</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;P(switch) de predicción&quot;) +</span>
<span class="st">  ylab(&quot;</span>Observado <span class="op">-</span><span class="st"> </span>estimado<span class="st">&quot;)</span></code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-22-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que esto no es útil. En la gráfica se ve un patrón fuerte en los residuales debido a que las observaciones de <span class="math inline">\(y_i\)</span> son <em>discretas</em>. Esto nos sugiere hacer una gráfica de residuales agrupados.</p>
<p>Para calcular los residuales agrupados dividimos los datos en clases (cubetas) en función de sus valores ajustados. Luego graficamos el residual promedio contra el valor promedio ajustado para cada cubeta.</p>
<p>Calculamos la agrupación de los residuales con la siguiente función:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">binned_residuals &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, <span class="dt">nclass=</span><span class="kw">sqrt</span>(<span class="kw">length</span>(x))){
  breaks.index &lt;-<span class="st"> </span><span class="kw">floor</span>(<span class="kw">length</span>(x)<span class="op">*</span>(<span class="dv">1</span><span class="op">:</span>(nclass<span class="op">-</span><span class="dv">1</span>))<span class="op">/</span>nclass)
  breaks &lt;-<span class="st"> </span><span class="kw">c</span> (<span class="op">-</span><span class="ot">Inf</span>, <span class="kw">sort</span>(x)[breaks.index], <span class="ot">Inf</span>)
  output &lt;-<span class="st"> </span><span class="ot">NULL</span>
  xbreaks &lt;-<span class="st"> </span><span class="ot">NULL</span>
  x.binned &lt;-<span class="st"> </span><span class="kw">as.numeric</span> (<span class="kw">cut</span> (x, breaks))
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nclass){
    items &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x))[x.binned<span class="op">==</span>i]
    x.range &lt;-<span class="st"> </span><span class="kw">range</span>(x[items])
    xbar &lt;-<span class="st"> </span><span class="kw">mean</span>(x[items])
    ybar &lt;-<span class="st"> </span><span class="kw">mean</span>(y[items])
    n &lt;-<span class="st"> </span><span class="kw">length</span>(items)
    sdev &lt;-<span class="st"> </span><span class="kw">sd</span>(y[items])
    output &lt;-<span class="st"> </span><span class="kw">rbind</span>(output, <span class="kw">c</span>(xbar, ybar, n, x.range, <span class="dv">2</span><span class="op">*</span>sdev<span class="op">/</span><span class="kw">sqrt</span>(n)))
  }
  <span class="kw">colnames</span>(output) &lt;-<span class="st"> </span><span class="kw">c</span> (<span class="st">&quot;xbar&quot;</span>, <span class="st">&quot;ybar&quot;</span>, <span class="st">&quot;n&quot;</span>, <span class="st">&quot;x.lo&quot;</span>, <span class="st">&quot;x.hi&quot;</span>, <span class="st">&quot;2se&quot;</span>)
  <span class="kw">return</span> (<span class="kw">list</span>(<span class="dt">binned=</span>output, <span class="dt">xbreaks=</span>xbreaks))
}</code></pre></div>
<p><img src="figuras/manicule.jpg" /> La función <em>binned_residuals</em> recibe como entrada un vector <span class="math inline">\(x\)</span>. ¿Es significativo dicho coeficiente?</p>
<p class="espacio">
</p>
<p><br></p>
<p>Veamos la gráfica:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">br.<span class="dv">8</span> &lt;-<span class="st"> </span><span class="kw">binned_residuals</span>(wells<span class="op">$</span>pred.<span class="dv">8</span>, wells<span class="op">$</span><span class="cf">switch</span><span class="op">-</span>wells<span class="op">$</span>pred.<span class="dv">8</span>, <span class="dt">nclass=</span><span class="dv">40</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.<span class="op">$</span>binned <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.data.frame</span>()

<span class="kw">ggplot</span>(br.<span class="dv">8</span>, <span class="kw">aes</span>(xbar, ybar)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="op">-</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;P(switch) de predicción&quot;) +</span>
<span class="st">  ylab(&quot;</span>Residual promedio<span class="st">&quot;)</span></code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-24-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p class="espacio3">
</p>
<p>Lo que observamos es los datos divididos en 40 cubetas de igual tamaño. Las líneas de color gris se calculan como <span class="math display">\[
\displaystyle{2\sqrt{\frac{p(1 - p)}{n}}},
\]</span> donde <span class="math inline">\(n\)</span> es el número de puntos por cubeta. En este caso, <span class="math inline">\(n = 3020/40 = 75\)</span> en este caso) indican</p>
<p>Si consideramos <span class="math inline">\(\pm 2\)</span> errores estándar esperamos que caigan adentro de estas bandas aproximadamente el 95% de los residuales agrupados (si el modelo fuera realmente verdadero).</p>
<hr />
<p><br></p>
<div id="graficas-de-residuales-agrupados-vs-predictores" class="section level3">
<h3><span class="header-section-number">8.6.1</span> Gráficas de residuales agrupados vs predictores</h3>
<p>Podemos estudiar los residuales graficándolos contra algunos predictores.</p>
<p>Graficamos los residuales contra la distancia al pozo seguro más cercano:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">br.<span class="fl">8.</span>dist &lt;-<span class="st"> </span><span class="kw">binned_residuals</span>(wells<span class="op">$</span>dist_<span class="dv">100</span>, wells<span class="op">$</span><span class="cf">switch</span><span class="op">-</span>wells<span class="op">$</span>pred.<span class="dv">8</span>, <span class="dt">nclass=</span><span class="dv">40</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.<span class="op">$</span>binned <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.data.frame</span>()
<span class="kw">ggplot</span>(br.<span class="fl">8.</span>dist, <span class="kw">aes</span>(xbar, ybar)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="op">-</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;P(switch) de predicción&quot;) +</span>
<span class="st">  ylab(&quot;</span>Distancia al pozo seguro más cercano<span class="st">&quot;)</span></code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-25-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>No observamos algún patrón en los residuales de la gráfica anterior, lo cual es consistente con el modelo.</p>
<p>Vemos ahora la gráfica con arsénico:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">br.<span class="fl">8.</span>ars &lt;-<span class="st"> </span><span class="kw">binned_residuals</span>(wells<span class="op">$</span>arsenic, wells<span class="op">$</span><span class="cf">switch</span><span class="op">-</span>wells<span class="op">$</span>pred.<span class="dv">8</span>, <span class="dt">nclass=</span><span class="dv">40</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.<span class="op">$</span>binned <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.data.frame</span>()
<span class="kw">ggplot</span>(br.<span class="fl">8.</span>ars, <span class="kw">aes</span>(xbar, ybar)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="op">-</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&#39;loess&#39;</span>, <span class="dt">se=</span>F, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;P(switch) de predicción&quot;) +</span>
<span class="st">  ylab(&quot;</span>Arsénico<span class="st">&quot;)</span></code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-26-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Esta gráfica sí muestra un patrón en los residuales. Tiene un residual negativo extremo. Además podemos decir que:</p>
<ul>
<li>las personas en los pozos para las primeras 3 cubetas tienen probabilidad de cambio promedio de:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">personas_cub3 &lt;-<span class="st"> </span>wells <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(arsenic <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.59</span>)
<span class="kw">sum</span>(personas_cub3<span class="op">$</span><span class="cf">switch</span>)<span class="op">/</span><span class="kw">nrow</span>(personas_cub3)
<span class="co">#&gt; [1] 0.308</span></code></pre></div>
<ul>
<li>el modelo predice que la probabilidad de cambio de estas personas en las primras tres cubetas es en promedio:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(personas_cub3<span class="op">$</span>pred.<span class="dv">8</span>)
<span class="co">#&gt; [1] 0.485</span></code></pre></div>
<p>Esto quiere decir que la probabilidad de que realmente cambien de pozo es aproximadamente 20% menos que la que predice el modelo.</p>
<ul>
<li>Observamos un patrón en los residuales: los residuales positivos (ej promedio) están en la mitad del rango de arsénico, y los residuales están en los extremos.</li>
</ul>
<p class="espacio3">
</p>
</div>
<div id="transformaciones" class="section level3">
<h3><span class="header-section-number">8.6.2</span> Transformaciones</h3>
<p>Ahora consideramos transformar la variable de arsénico: vemos un patrón en los residuales en el cual estos primero aumentan y luego disminuyen.</p>
<p>Para solucionar esto hay algunas opciones:</p>
<ul>
<li><p>usar una <strong>transformación logarítmica</strong>.</p></li>
<li><p>agregar un término cuadrático al término lineal.</p></li>
</ul>
<p>En este caso como la variable de arsénico es no negativa es un poco más práctico utilizar la transformación de logaritmo.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wells &lt;-<span class="st"> </span>wells <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">arsenic_log =</span> <span class="kw">log</span>(arsenic),
                          <span class="dt">arsenic_log_c =</span> arsenic_log <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(arsenic_log))
fit.<span class="dv">9</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_100_c <span class="op">+</span><span class="st"> </span>arsenic_log_c <span class="op">+</span><span class="st"> </span>educ4_c <span class="op">+</span>
<span class="st">               </span>dist_100_c<span class="op">:</span>arsenic_log_c <span class="op">+</span><span class="st"> </span>dist_100_c<span class="op">:</span>educ4_c <span class="op">+</span><span class="st"> </span>arsenic_log_c<span class="op">:</span>educ4_c,
             <span class="dt">data =</span> wells,
             <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
<span class="kw">summary</span>(fit.<span class="dv">9</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; glm(formula = switch ~ dist_100_c + arsenic_log_c + educ4_c + </span>
<span class="co">#&gt;     dist_100_c:arsenic_log_c + dist_100_c:educ4_c + arsenic_log_c:educ4_c, </span>
<span class="co">#&gt;     family = binomial(link = &quot;logit&quot;), data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Deviance Residuals: </span>
<span class="co">#&gt;    Min      1Q  Median      3Q     Max  </span>
<span class="co">#&gt; -2.103  -1.162   0.718   1.040   1.923  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;                          Estimate Std. Error z value Pr(&gt;|z|)    </span>
<span class="co">#&gt; (Intercept)                0.3452     0.0405    8.53  &lt; 2e-16 ***</span>
<span class="co">#&gt; dist_100_c                -0.9796     0.1112   -8.81  &lt; 2e-16 ***</span>
<span class="co">#&gt; arsenic_log_c              0.9036     0.0695   13.00  &lt; 2e-16 ***</span>
<span class="co">#&gt; educ4_c                    0.1785     0.0390    4.58  4.7e-06 ***</span>
<span class="co">#&gt; dist_100_c:arsenic_log_c  -0.1567     0.1851   -0.85   0.3974    </span>
<span class="co">#&gt; dist_100_c:educ4_c         0.3384     0.1078    3.14   0.0017 ** </span>
<span class="co">#&gt; arsenic_log_c:educ4_c      0.0601     0.0703    0.85   0.3926    </span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     Null deviance: 4118.1  on 3019  degrees of freedom</span>
<span class="co">#&gt; Residual deviance: 3863.1  on 3013  degrees of freedom</span>
<span class="co">#&gt; AIC: 3877</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></code></pre></div>
<p>Esto da resultados similares al modelo anterior, los signos de las interacciones son los mismos y los signos de los efectos principales también son los mismos.</p>
<p>Volvemos a hacer la gráfica de niveles de arsénico vs residuales:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wells<span class="op">$</span>pred.<span class="dv">9</span> &lt;-<span class="st"> </span>fit.<span class="dv">9</span><span class="op">$</span>fitted.values
br.fit.<span class="dv">9</span> &lt;-<span class="st"> </span><span class="kw">binned_residuals</span>(wells<span class="op">$</span>arsenic, wells<span class="op">$</span><span class="cf">switch</span><span class="op">-</span>wells<span class="op">$</span>pred.<span class="dv">9</span>, <span class="dt">nclass=</span><span class="dv">40</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.<span class="op">$</span>binned <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.data.frame</span>()
<span class="kw">ggplot</span>(br.fit.<span class="dv">9</span>, <span class="kw">aes</span>(xbar, ybar)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span><span class="op">-</span><span class="st">`</span><span class="dt">2se</span><span class="st">`</span>), <span class="dt">color=</span><span class="st">&quot;grey60&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&#39;loess&#39;</span>, <span class="dt">se=</span>F, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Residual promedio P(switch)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Arsénico&quot;</span>)</code></pre></div>
<p><img src="08-logit-3_files/figure-html/unnamed-chunk-30-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Los residuales se ven mucho mejor, aunque para niveles bajos de arsénico aún hay muchos residuales negativos: los usuarios de pozos con niveles de arsénico justo por encima de <span class="math inline">\(0.50\)</span> tienen menos probabilidad de cambiarse que la que predice el modelo.</p>
<p>Posibles explicaciones:</p>
<ul>
<li><p>psicológicamente, las mediciones justo por encima de <span class="math inline">\(0.50\)</span> podrían parecer no muy graves</p></li>
<li><p>por error de medición, tal vez algunos de los pozos con <span class="math inline">\(0.51\)</span> o <span class="math inline">\(0.52\)</span> se midieron antes o después y de encontrar niveles de arsénico por debajo de <span class="math inline">\(0.5\)</span></p></li>
</ul>
</div>
<div id="tasa-de-error-y-comparacion-contra-el-modelo-nulo" class="section level3">
<h3><span class="header-section-number">8.6.3</span> Tasa de error y comparación contra el modelo nulo</h3>
<p>La <strong>tasa de error</strong> se define como la proporción de casos para los cuáles la predicción de <span class="math inline">\(y_i\)</span> a partir de la probabilidad estimada <span class="math display">\[
\pi_i=p_1(x_i)=\mbox{logit}^{-1}(X_i\beta)
\]</span> es <em>incorrecta</em>.</p>
<p>Para hacer la predicción para cada <span class="math inline">\(y_i\)</span> es necesario definir un <strong>punto de corte</strong>, en principio se utiliza el <span class="math inline">\(0.5\)</span> de la forma que la predicción de <span class="math inline">\(y_i\)</span>, denotada por <span class="math inline">\(\hat{y}_i\)</span>, es <span class="math display">\[
\hat{y}_{i} = \left\{ \begin{array}{cl}
1 &amp; \text{si }\;\mbox{logit}^{-1}(X_i\beta) &gt; 0.5,\\
0 &amp; \text{en otro caso.}
\end{array}\right.
\]</span></p>
<p>Por lo tanto, calculamos la tasa de error para el ejemplo de los pozos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(error_rate &lt;-<span class="st"> </span><span class="kw">mean</span>((wells<span class="op">$</span>pred.<span class="dv">9</span><span class="op">&gt;</span><span class="fl">0.5</span> <span class="op">&amp;</span><span class="st"> </span>wells<span class="op">$</span><span class="cf">switch</span><span class="op">==</span><span class="dv">0</span>) <span class="op">|</span><span class="st"> </span>(wells<span class="op">$</span>pred.<span class="dv">9</span><span class="op">&lt;=</span><span class="fl">0.5</span> <span class="op">&amp;</span><span class="st"> </span>wells<span class="op">$</span><span class="cf">switch</span><span class="op">==</span><span class="dv">1</span>)))
<span class="co">#&gt; [1] 0.365</span></code></pre></div>

<div class="nota">
<p>Observaciones:</p>
<ul>
<li><p>La tasa de error siempre debe ser menor que <span class="math inline">\(1/2\)</span>.</p></li>
<li><p>Si la tasa de error fuera mayor o igual a <span class="math inline">\(1/2\)</span>, entonces poniendo todas las <span class="math inline">\(\beta^\prime\mbox{s}\)</span> igual a cero obtendríamos un mejor modelo.</p></li>
<li>El <strong>modelo nulo</strong> se define como el modelo que asigna la misma probabilidad <span class="math inline">\(p\)</span> a toda <span class="math inline">\(y_i\)</span>, y dicha probabilidad es <span class="math display">\[p = \dfrac{1}{n}\sum_{i=1}^N{y_i}.\]</span>
</div>
</li>
</ul>
<p class="espacio3">
</p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>¿Cuál o cuáles de las siguientes afirmaciones son ciertas?</p>
<ol style="list-style-type: lower-alpha">
<li><p>La tasa de error del modelo nulo es igual a <span class="math inline">\(p\)</span>.</p></li>
<li><p>El modelo nulo es aquel en el cual todas las observaciones son equiprobables.</p></li>
<li><p>El modelo nulo es simplemente regresión logística con sólo un término constante.</p></li>
<li><p>La tasa de error del modelo nulo es igual a <span class="math inline">\(1-p\)</span>.</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<p>Por ejemplo, en el modelo de los pozos la tasa de error del modelo nulo es:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="dv">1</span><span class="op">-</span>wells<span class="op">$</span><span class="cf">switch</span>)
<span class="co">#&gt; [1] 0.425</span></code></pre></div>
<p>Es decir, <span class="math inline">\(58\%\)</span> de los encuestados cambiaron de pozo, pero <span class="math inline">\(42\%\)</span> no cambiaron de pozo.</p>
<p class="espacio3">
</p>

<div class="information">
<p><strong>Interpretación del modelo nulo:</strong></p>
<ul>
<li><p>el modelo sin predictores le da a cada persona un <span class="math inline">\(58\%\)</span> de posibilidad de cambio (que corresponde a una predicción puntual de cambiar para cada persona), y</p></li>
<li>esta predicción será incorrecta el <span class="math inline">\(42\%\)</span> de las veces.
</div>
</li>
</ul>
<p class="espacio3">
</p>

<div class="nota">
<p>Nota:</p>
<p class="espacio">
</p>
<ul>
<li><p>Utilizamos la <strong>devianza</strong> para evaluar el ajuste de un modelo porque la tasa de error no es un resumen perfecto del <em>desajuste</em> del modelo. La razón de esto es que la tasa de error no puede distinguir entre predicciones de <span class="math inline">\(0.6\)</span> y <span class="math inline">\(0.9\)</span>, por ejemplo, porque únicamente toma en cuenta predicciones de <span class="math inline">\(y_i\)</span>’s.</p></li>
<li><p>Es importante poder predecir <span class="math inline">\(y_i\)</span>’s pero generalmente es más útil trabajar con las probabilidades porque en estas subyace la incertidumbre de la predicción.</p></li>
<li><p>La tasa de error se utiliza a menudo porque es fácil de interpretar y en la práctica puede llegar a ser muy útil.</p></li>
<li><p>Una tasa de error igual a la tasa de error del modelo nulo es terrible, y la mejor tasa de error posible es cero.</p></li>
<li>Una tasa de error alta (cercana a la del modelo nulo pero menor) no significa que el modelo no sea útil. La interpretación del modelo puede aportar a explicar el fenómeno de estudio.
</div>
</li>
</ul>
<p class="espacio3">
</p>
<p>La razón por la cual la tasa de error del modelo de pozos es alta se resume en los siguientes puntos:</p>
<ul>
<li><p>La mayoría de los datos están alrededor de las medias de los predictores (la distancia menor a 100 m y arsénico en 0.5 y 1).</p></li>
<li><p>Por el punto anterior, para la mayoría de los datos la probabilidad de cambio <span class="math inline">\(P(\mbox{switch})=0.58\)</span> funciona bien, que es simplementa la media en los datos.</p></li>
<li><p>El modelo da información sobre lo que está pasando en los extremos, pero relativamente hay muy pocos datos, por lo que la precisión predictiva general del modelo no es muy alta.</p></li>
</ul>
<hr />
<p><br></p>
</div>
</div>
<div id="diferencias-predictivas-promedio-en-la-escala-de-probabilidad" class="section level2">
<h2><span class="header-section-number">8.7</span> Diferencias predictivas promedio en la escala de probabilidad</h2>
<p>Recordemos:</p>
<ol style="list-style-type: decimal">
<li><p>Las regresiones logísticas son inherentemente más difíciles que las regresiones lineales para interpretar.</p></li>
<li><p>La regresión logística es no lineal en la escala de probabilidad, es decir, a una diferencia unitaria en <span class="math inline">\(x\)</span> no le corresponde una diferencia constante en <span class="math inline">\(P(y_i=1)\)</span>.</p></li>
<li><p>Como resultado de esto, los coeficientes de regresión logística no se pueden interpretar directamente en la escala de probabilidad.</p></li>
</ol>
<p class="espacio3">
</p>
<p>Se puede calcular la <em>diferencia predictiva promedio</em> como función de los valores de entrada de los predictores para luego dar una diferencia <em>promedio</em> en <span class="math inline">\(P(y=1)\)</span> correspondiente a una diferencia en cada una de las variables de entrada.</p>
<p>Comenzamos utilizando el siguiente modelo sin interacciones para hacer más simple la explicación:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">10</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>arsenic <span class="op">+</span><span class="st"> </span>educ4,
               <span class="dt">data =</span> wells,
               <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
<span class="kw">summary</span>(fit.<span class="dv">10</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; glm(formula = switch ~ dist_100 + arsenic + educ4, family = binomial(link = &quot;logit&quot;), </span>
<span class="co">#&gt;     data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Deviance Residuals: </span>
<span class="co">#&gt;    Min      1Q  Median      3Q     Max  </span>
<span class="co">#&gt; -2.577  -1.197   0.755   1.063   1.699  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span>
<span class="co">#&gt; (Intercept)  -0.2139     0.0931   -2.30    0.022 *  </span>
<span class="co">#&gt; dist_100     -0.8956     0.1046   -8.56  &lt; 2e-16 ***</span>
<span class="co">#&gt; arsenic       0.4684     0.0416   11.26  &lt; 2e-16 ***</span>
<span class="co">#&gt; educ4         0.1713     0.0383    4.47  7.7e-06 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     Null deviance: 4118.1  on 3019  degrees of freedom</span>
<span class="co">#&gt; Residual deviance: 3910.4  on 3016  degrees of freedom</span>
<span class="co">#&gt; AIC: 3918</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></code></pre></div>
<p class="espacio3">
</p>
<p><strong>Ejemplo: Diferencia predictiva promedio en probabilidad de cambio, comparando hogares que están a 100 metros o menos de un pozo seguro más cercano.</strong></p>
<p class="espacio3">
</p>
<p>Comparemos dos hogares:</p>
<ul>
<li><p>uno con <span class="math inline">\(\mbox{dist_100} = 0\)</span>, y</p></li>
<li><p>uno con <span class="math inline">\(\mbox{dist_100} = 1\)</span>,</p></li>
</ul>
<p>pero ambos con los mismos valores en las otras variables de entrada: arsénico y educ4.</p>
<p>La <em>diferencia predictiva</em> en probabilidad de cambiar de pozo entre los dos hogares: <span class="math display">\[
\begin{eqnarray*}
\delta(\mbox{arsenic}, \mbox{educ4}) = \mbox{logit}^{−1}(−0.21 − 0.90 \cdot 1 + 0.47 \cdot \mbox{arsenic} + 0.17 \cdot \mbox{educ4}) &amp;−&amp;\\ \mbox{logit}^{−1}(−0.21 − 0.90 \cdot 0 + 0.47 \cdot \mbox{arsenic} + 0.17 \cdot \mbox{educ4}).
\end{eqnarray*}
\]</span> Escribimos <span class="math inline">\(\delta\)</span> como una función de arsénico y educ4 para enfatizar que depende de los niveles de estas otras variables.</p>
<p>Promeiamos las diferencias predictivas sobre los <span class="math inline">\(n\)</span> hogares en los datos para obtener: <span class="math display">\[
\mbox{diferencia predictiva promedio} = \dfrac{1}{n}\sum_{i=1}^n{\delta(\mbox{arsenic}_i, \mbox{educ4}_i)}
\]</span></p>
<p>Hacemos el cálculo:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b &lt;-<span class="st"> </span><span class="kw">coef</span>(fit.<span class="dv">10</span>)
hi &lt;-<span class="st"> </span><span class="dv">1</span>
lo &lt;-<span class="st"> </span><span class="dv">0</span>
delta &lt;-<span class="st"> </span><span class="kw">invlogit</span> (b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>hi <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>wells<span class="op">$</span>educ4) <span class="op">-</span>
<span class="st">         </span><span class="kw">invlogit</span> (b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>lo <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>wells<span class="op">$</span>educ4)
<span class="kw">print</span>(<span class="kw">mean</span>(delta))
<span class="co">#&gt; [1] -0.204</span></code></pre></div>
<p>El resultado es <span class="math inline">\(-0.20\)</span>, lo que implica que, en promedio en los datos, los hogares que están a 100 metros del pozo seguro más cercano tienen un <span class="math inline">\(20\%\)</span> menos de probabilidades de cambiar, en comparación con los hogares que están justo al lado del pozo seguro más cercano, con mismos niveles de arsénico y mismos niveles de educación.</p>
<hr />
<p><br></p>
<p><strong>Ejemplo: Diferencia predictiva promedio en probabilidad de cambio, comparando hogares con niveles de arsénico existentes de 0.5 y 1.0.</strong></p>
<p>Calculamos la diferencia predictiva y la diferencia predictiva promedio, comparando los hogares en dos niveles diferentes de arsénico, suponiendo que la distancia al pozo seguro más cercano y los niveles de educación son iguales. Elegimos <span class="math inline">\(\mbox{arsenic} = 0.5\)</span> y <span class="math inline">\(1.0\)</span> como puntos de comparación porque <span class="math inline">\(0.5\)</span> es el nivel más bajo inseguro, <span class="math inline">\(1.0\)</span> es el doble, y esta comparación captura gran parte del rango de los datos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hi &lt;-<span class="st"> </span><span class="fl">1.0</span>
lo &lt;-<span class="st"> </span><span class="fl">0.5</span>
delta &lt;-<span class="st"> </span><span class="kw">invlogit</span> (b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>hi <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>wells<span class="op">$</span>educ4) <span class="op">-</span>
<span class="st">         </span><span class="kw">invlogit</span> (b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>lo <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>wells<span class="op">$</span>educ4)
<span class="kw">print</span> (<span class="kw">mean</span>(delta))
<span class="co">#&gt; [1] 0.0564</span></code></pre></div>
<p>El resultado es <span class="math inline">\(0.06\)</span>, por lo que esta comparación corresponde a una diferencia del <span class="math inline">\(6\%\)</span> en la probabilidad de cambiar de pozo.</p>
<p><strong>Ejemplo: Diferencia predictiva promedio en probabilidad de cambio, comparando hogares con 0 y 12 años de educación.</strong></p>
<p>Calcular nuevamente la diferencia predictiva promedio de la probabilidad de cambio de pozo para hogares con 0 vs 12 años de educación:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hi &lt;-<span class="st"> </span><span class="dv">3</span>
lo &lt;-<span class="st"> </span><span class="dv">0</span>
delta &lt;-<span class="st"> </span><span class="kw">invlogit</span> (b[<span class="dv">1</span>]<span class="op">+</span>b[<span class="dv">2</span>]<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span><span class="op">+</span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic<span class="op">+</span>b[<span class="dv">4</span>]<span class="op">*</span>hi) <span class="op">-</span>
<span class="st">         </span><span class="kw">invlogit</span> (b[<span class="dv">1</span>]<span class="op">+</span>b[<span class="dv">2</span>]<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span><span class="op">+</span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic<span class="op">+</span>b[<span class="dv">4</span>]<span class="op">*</span>lo)
<span class="kw">print</span> (<span class="kw">mean</span>(delta))
<span class="co">#&gt; [1] 0.117</span></code></pre></div>
<hr />
<p><br></p>
<div id="diferencias-predictivas-promedio-en-presencia-de-interacciones" class="section level3 unnumbered">
<h3>Diferencias predictivas promedio en presencia de interacciones</h3>
<p>Por ejemplo, consideremos la diferencia predictiva promedio, comparando <span class="math inline">\(\mbox{dist} = 0\)</span> con <span class="math inline">\(\mbox{dist} = 100\)</span>, para el modelo que incluye una interacción distancia <span class="math inline">\(\times\)</span> arsénico:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">11</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>arsenic <span class="op">+</span><span class="st"> </span>educ4 <span class="op">+</span><span class="st"> </span>dist_<span class="dv">100</span><span class="op">:</span>arsenic,
              <span class="dt">data =</span> wells,
              <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))
<span class="kw">summary</span>(fit.<span class="dv">11</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; glm(formula = switch ~ dist_100 + arsenic + educ4 + dist_100:arsenic, </span>
<span class="co">#&gt;     family = binomial(link = &quot;logit&quot;), data = wells)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Deviance Residuals: </span>
<span class="co">#&gt;    Min      1Q  Median      3Q     Max  </span>
<span class="co">#&gt; -2.715  -1.189   0.748   1.069   1.722  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    </span>
<span class="co">#&gt; (Intercept)       -0.3490     0.1264   -2.76   0.0057 ** </span>
<span class="co">#&gt; dist_100          -0.6047     0.2095   -2.89   0.0039 ** </span>
<span class="co">#&gt; arsenic            0.5554     0.0695    7.99  1.4e-15 ***</span>
<span class="co">#&gt; educ4              0.1692     0.0383    4.42  1.0e-05 ***</span>
<span class="co">#&gt; dist_100:arsenic  -0.1629     0.1023   -1.59   0.1115    </span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     Null deviance: 4118.1  on 3019  degrees of freedom</span>
<span class="co">#&gt; Residual deviance: 3907.9  on 3015  degrees of freedom</span>
<span class="co">#&gt; AIC: 3918</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></code></pre></div>
<p>Calculamos la diferencia predictiva promedio entre hogares a 0 y 100 metros de distancia del pozo seguro como:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b &lt;-<span class="st"> </span><span class="kw">coef</span>(fit.<span class="dv">11</span>)
hi &lt;-<span class="st"> </span><span class="dv">1</span>
lo &lt;-<span class="st"> </span><span class="dv">0</span>
delta &lt;-<span class="st"> </span><span class="kw">invlogit</span>(b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>hi <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>wells<span class="op">$</span>educ4 <span class="op">+</span><span class="st"> </span>b[<span class="dv">5</span>]<span class="op">*</span>hi<span class="op">*</span>wells<span class="op">$</span>arsenic) <span class="op">-</span>
<span class="st">         </span><span class="kw">invlogit</span>(b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>lo <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>wells<span class="op">$</span>educ4 <span class="op">+</span><span class="st"> </span>b[<span class="dv">5</span>]<span class="op">*</span>lo<span class="op">*</span>wells<span class="op">$</span>arsenic)
<span class="kw">print</span>(<span class="kw">mean</span>(delta))
<span class="co">#&gt; [1] -0.194</span></code></pre></div>
</div>
<div id="notacion-general-para-diferencias-predictivas" class="section level3 unnumbered">
<h3>Notación general para diferencias predictivas</h3>
<p>Consideramos que deseamos evaluar las diferencias predictivas una a la vez y usamos la notación de:</p>
<ul>
<li><p><span class="math inline">\(u\)</span> para la entrada de interés, y</p></li>
<li><p><span class="math inline">\(v\)</span> para el vector de todas las otras entradas.</p></li>
</ul>
<p>Supongamos que estamos considerando comparaciones entre <span class="math inline">\(u=u^{(1)}\)</span> y <span class="math inline">\(u=u^{(0)}\)</span> y todas las demás variables permanecen constantes.</p>
<p>La diferencia predictiva en probabilidades entre los dos casos que difiere solo en <span class="math inline">\(u\)</span> es: <span class="math display">\[
\delta(u^{(\mbox{hi})},u^{(\mbox{lo})},v,\beta) = P(y=1|u^{(\mbox{hi})},v,\beta)−P(y=1|u^{(\mbox{lo})},v,\beta).
\]</span></p>
<p>La diferencia predictiva media es la media de las <span class="math inline">\(n\)</span> diferencias predictivas correspondientes a cada observación en los datos: <span class="math display">\[
\Delta(u^{(\mbox{hi})}, u^{(\mbox{lo})}) = \dfrac{1}{n} \sum_{i=1}^n{\delta(u^{(\mbox{hi})}, u^{(\mbox{lo})}, v_i, β),}
\]</span> donde <span class="math inline">\(v_i\)</span> representa el vector de otras variables de entrada para la <span class="math inline">\(i\)</span>-ésima observación.</p>
<p class="espacio3">
</p>
<p>Consideremos el modelo de regresión logística con la interacción de educación y distancia:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.<span class="dv">12</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="cf">switch</span> <span class="op">~</span><span class="st"> </span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>arsenic <span class="op">+</span><span class="st"> </span>educ4 <span class="op">+</span><span class="st"> </span>dist_<span class="dv">100</span><span class="op">:</span>educ4,
              <span class="dt">data =</span> wells,
              <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))</code></pre></div>
<p>Los coeficientes del modelo son:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fit.<span class="dv">12</span>)
<span class="co">#&gt;    (Intercept)       dist_100        arsenic          educ4 dist_100:educ4 </span>
<span class="co">#&gt;       0.000496      -1.389852       0.480599      -0.008308       0.382545</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b &lt;-<span class="st"> </span><span class="kw">coef</span>(fit.<span class="dv">12</span>)
hi &lt;-<span class="st"> </span><span class="dv">3</span>
lo &lt;-<span class="st"> </span><span class="dv">0</span>
delta &lt;-<span class="st"> </span><span class="kw">invlogit</span>(b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>hi <span class="op">+</span><span class="st"> </span>b[<span class="dv">5</span>]<span class="op">*</span>wells<span class="op">$</span>educ<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span>) <span class="op">-</span>
<span class="st">         </span><span class="kw">invlogit</span>(b[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b[<span class="dv">2</span>]<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>b[<span class="dv">3</span>]<span class="op">*</span>wells<span class="op">$</span>arsenic <span class="op">+</span><span class="st"> </span>b[<span class="dv">4</span>]<span class="op">*</span>lo <span class="op">+</span><span class="st"> </span>b[<span class="dv">5</span>]<span class="op">*</span>wells<span class="op">$</span>educ<span class="op">*</span>wells<span class="op">$</span>dist_<span class="dv">100</span>)
<span class="kw">print</span>(<span class="kw">mean</span>(delta))
<span class="co">#&gt; [1] -0.00468</span></code></pre></div>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>La diferencia predictiva promedio entre hogares con 0 años y 12 años de educación es:</p>
<ol style="list-style-type: lower-alpha">
<li><p>-0.05</p></li>
<li><p>0.30</p></li>
<li><p>0.12</p></li>
<li><p>0.05</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
En regresión logística, el gradiente de <span class="math inline">\(D(\beta)\)</span> está dado por <span class="math display">\[
p_1(x) = h\left(\beta_0+\beta_1 x_1 + \beta_2x_2\right).
\]</span>
<p class="espacio3">
</p>
<p>Se cuenta con los datos de la siguiente tabla y que aparecen en la gráfica de la derecha:</p>
<p class="espacio3">
</p>
<center>
<img src="figuras/ejemplo.png" style="width:60.0%" />
</center>
<p class="espacio3">
</p>
<p class="espacio3">
</p>
<p>¿Cuál de las siguientes afirmaciones son ciertas? Selecciona una o más.</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(D(\beta)\)</span> será una función convexa, por lo que descenso en gradiente debería converger al mínimo global.</p></li>
<li><p>Para incrementar el ajuste del modelo a los datos podríamos agregar términos polinomiales o de interacción, por ejemplo, <span class="math inline">\(p_1(x_i)=h\left(\beta_0+\beta_1 x_1 + \beta_2x_2 + \beta_3x_1^2 + \beta_4x_1x_2 +\beta_5x_2^2\right)\)</span>.</p></li>
<li><p>Los datos no se pueden separar utilizando una recta de modo que las observaciones de éxito estén de un lado de la recta y las observaciones de fracaso del otro. Por lo tanto, descenso en gradiente no podría converger.</p></li>
<li><p>Como lo datos no se pueden separar mediante una recta, entonces el método de regresión logística produciría los mismos resultados que aplicar regresión lineal a estos datos.</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>En regresión logística el gradiente de la devianza está dado por <span class="math display">\[
\dfrac{\partial D(\beta)}{\partial \beta_j} = \sum_{i=1}^N(h_\beta(x^{(i)}) - y^{(i)})x_j^{(i)}.
\]</span> ¿Cuál de las siguientes expresiones es una actualizción correcta para descenso en gradiente con parámetro <span class="math inline">\(\alpha\)</span>? Selecciona una o más.</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\beta_j := \beta_j - \alpha\frac{1}{N}\sum_{i=1}^N\left(\beta^Tx-y^{(i)}\right)x_j^{(i)}.\)</span></p></li>
<li><p><span class="math inline">\(\beta := \beta - \alpha\frac{1}{N}\sum_{i=1}^N\left(\dfrac{1}{1+e^{-\beta^Tx^{(i)}}}-y^{(i)}\right)x^{(i)}.\)</span></p></li>
<li><p><span class="math inline">\(\beta := \beta - \alpha\frac{1}{N}\sum_{i=1}^N\left(\beta^Tx-y^{(i)}\right)x^{(i)}.\)</span></p></li>
<li><p><span class="math inline">\(\beta_j := \beta_j - \alpha\frac{1}{N}\sum_{i=1}^N\left(\dfrac{1}{1+e^{-\beta^Tx^{(i)}}}-y^{(i)}\right)x_j^{(i)}\)</span>   (actualiza simultáneamente para toda <span class="math inline">\(j\)</span>).</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<img src="figuras/manicule2.jpg" />
<div class="centered">
<p class="espacio">
</p>
<p>¿Cuáles de las siguientes afirmaciones son ciertas?</p>
<ol style="list-style-type: lower-alpha">
<li><p>La devianza <span class="math inline">\(D(\beta)\)</span> en regresión logística con <span class="math inline">\(N\geq 1\)</span> observaciones siempre es no negativa.</p></li>
<li><p>En regresión logística, descenso en gradiente algunas veces converge a un mínimo global (no logra encontrar el mínimo global). Esta es la razón por la cual preferimos algoritmos más avanzados de optimización, como BFGS y Región de confianza.</p></li>
<li><p>La regresión lineal siempre funciona bien para hacer clasificación si se clasifica utilizando un umbral en la predicción hecha por regresión lineal.</p></li>
<li><p>La función logística <span class="math inline">\(h(x)=\frac{1}{1+e^{-x}}\)</span> nunca es mayor que uno (<span class="math inline">\(&gt;1\)</span>).</p></li>
</ol>
<p class="espacio3">
</p>
</div>
<p><br></p>
<hr />
<p><br></p>
</div>
</div>
<div id="tarea-6" class="section level2">
<h2><span class="header-section-number">8.8</span> Tarea</h2>
<ol style="list-style-type: decimal">
<li>Utiliza los datos de cambio de pozos de arsénico del archivo wells.csv.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Ajusta un modelo de regresión logística para la probabilidad de cambio de pozo utilizando una transformación logarítmica para la variable de distancia.</p></li>
<li><p>Haz una gráfica que muestre la probabilidad de cambio de pozo como función de la distancia al pozo, y que en la gráfica se muestren también los datos.</p></li>
<li><p>Haz una gráfica de residuales contra ajustados utilizando residuales agrupados.</p></li>
<li><p>Calcula la tasa de error del modelo ajustado y compárala con la tasa de error del modelo nulo.</p></li>
<li><p>Crea variables indicadores correspondientes a:</p>
<ul>
<li><p><span class="math inline">\(\mbox{dist} &lt; 100\)</span>,</p></li>
<li><p><span class="math inline">\(100 \leq \mbox{dist} &lt; 200\)</span>, y</p></li>
<li><p><span class="math inline">\(\mbox{dist} \geq 200\)</span>.</p></li>
</ul></li>
</ol>
<p>Ajusta una regresión logística para <span class="math inline">\(P(\mbox{switch})\)</span> utilizando estas tres variables indicadoras. Con este nuevo modelo repite las gráficas de los incisos b y c, y los cálculos del inciso d.</p>
<ol start="2" style="list-style-type: decimal">
<li>Continuamos con los datos de los pozos.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Ajusta una regresión logística utilizando como predictores distancia, el logaritmo del arsénico y su interacción. Interpreta los coeficientes estimados y sus errores estándar.</p></li>
<li><p>Haz gráficas que muestren la relación entre la probabilidad de cambio de pozo, la distancia y el nivel de arsénico.</p></li>
<li><p>Calcula las diferencias predictivas promedio correspondientes a:</p>
<ul>
<li><p>la diferencia entre <span class="math inline">\(\mbox{dist}=0\)</span> y <span class="math inline">\(\mbox{dist=100}\)</span>, cuando arsénico permanece constante.</p></li>
<li><p>la diferencia entre <span class="math inline">\(\mbox{dist=100}\)</span> y <span class="math inline">\(\mbox{dist=200}\)</span>, cuando arsénico permanece constante.</p></li>
<li><p>la diferencia entre <span class="math inline">\(\mbox{arsenic=0.5}\)</span> y <span class="math inline">\(\mbox{arsenic=1.0}\)</span>, cuando distancia permanece constante.</p></li>
<li><p>la diferencia entre <span class="math inline">\(\mbox{arsenic=1.0}\)</span> y <span class="math inline">\(\mbox{arsenic=2.0}\)</span>, cuando distancia permanece constante.</p></li>
</ul></li>
</ol>
<p>Discute los resultados.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regresion-logistica-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularizacion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
